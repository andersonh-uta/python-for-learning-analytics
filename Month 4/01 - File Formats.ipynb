{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "177c999f-8d12-4daa-8f85-6341ca924bce",
   "metadata": {},
   "source": [
    "# Common File Formats\n",
    "\n",
    "Your data will come to you in many different formats, and your outputs will come in at least as many.  This notebook shows examples of some common formats and how to deal with them.\n",
    "\n",
    "*Terminology:* the verb *serialize* will come up a lot.  \"To serialize some data\" just means to convert it to a format that can be stored permanently in a file.\n",
    "\n",
    "All cells in this notebook are designed to be be runnable without depending on any other cells.  This means you can copy-paste them into your editor to see how they work.  It also means I'm going to repeat a lot of code (like imports) in every cell, which you usually won't see with Notebooks.\n",
    "\n",
    "## A note about file extensions\n",
    "\n",
    "File extensions are a lie.  Sort of.\n",
    "\n",
    "The extension in a file--.txt, .docx, .exe, .py--is just part of the file's name.  You can change--or even remove--the file extension, and *nothing about the file's contents changes.*  The extensions *are* used by your operating system, though, to determine what the default program is for opening the file.  Notepad for .txt files, LibreOffice for .docx, PyCharm for .py, etc; but you *can* open any file in any program.\n",
    "\n",
    "A file extension is just a way to convey \"this file contains data formatted according to the .txt/.docx/.py specifications.\"  It's like changing the name of a file from \"Paper - Draft.docx\" to \"Paper - Final.docx\"; just changing the name changes nothing about the contents of the file.  It only changes the message you're conveying to the person who's looking at the file: \"this is the final, complete version of the paper, not a draft.\"  That person can, of course, ignore that information and still treat it like a draft!  In this same way, you can force your computer to open a .csv file in Notepad, or a .txt file in Word, or a .py file in Excel.  It might look and act weird when you do, but you can absolutely do it.\n",
    "\n",
    "# General note: text files\n",
    "\n",
    "By and large, text files (sometimes called \"plain\" text files, or \"plain text\") are the most common ways you will receive your data.  A text file is just a file that contains text: it does *not* contain any fancy binary data to specify layouts and formats.  A huge number of files that you interact with probbly every day are actually text files (some of these will be discussed in more detail later):\n",
    "\n",
    "- CSV and TSV files\n",
    "- JSON files\n",
    "- .txt files\n",
    "- HTML files\n",
    "- Code--in Python, R, Julia, etc\n",
    "- Config files (.cfg, .ini)\n",
    "\n",
    "A narrow definition of \"text file\" would be something like: any file where, when you open it, the first bit of binary data directly correspond to some text character (e.g. a letter, or space, or accent mark), as does the next, etc etc up until (and including) the very last one.\n",
    "\n",
    "By this token, some common files that are *not* text files:\n",
    "\n",
    "- Images: the binary data corresponds to pixel colors/brightness, *not* text characters.\n",
    "- Compiled code (.dmg, .exe, .elf, .o, etc): the binary data corresponds to *machine instructions,* not text characters.\n",
    "- Sound files (.mp3, .wav, .ogg, .flac, etc): the binary data corresponds to *audio information,* not text characters.\n",
    "\n",
    "## Encoding: the bane of everyone who works with text files\n",
    "\n",
    "All files are, ultimately, binary data (1s and 0s).  To map these digits to text, an *encoding* is needed.  An encoding is just a list of text characters and their corresponding binary values.  So, you might define a capital 'A' as 0, 'B' as 1, 'C' as 10, etc.\n",
    "\n",
    "There are a few common encodings you'll run into. Each one of these differs in terms of 1) what characters they have mappings for in the first place; 2) what the specific mappings are.\n",
    "\n",
    "- ASCII: The simplest and most universal encoding, for anything using the Latin alphabet plus a few common accent marks.  Most other encodings are supersets of ASCII.\n",
    "- UTF-8: The one you should be using by default.  UTF-8 extends ASCII by adding mappings for text in (almost) every script ever used, including extinct ones like cuneiform and Egyptian hieroglyphs.  This is the default encoding on almost every modern Linux distribution, and on modern verions of macOS.\n",
    "- Windows-1252, often incorrectly identified as ISO-8859-1: Very similar to UTF-8, but uses slightly different mappings for some characters.  This is the default on modern Windows versions.\n",
    "\n",
    "By and large, most people have standardized on UTF-8 as a general default, with ISO-8859-1 sometimes coming into the mix.  Just by looking at a file, you can't usually tell what the encoding is.  Some text editing programs will try to infer the encoding, but they can make mistakes.  And if you try to open a file with an incompatible encoding, you'll get errors.  Debugging these can be a pain, but here's the good rule of thumb I've learned:\n",
    "\n",
    "1. UTF-8 should be your first guess.\n",
    "2. If that doesn't work, try Windows-1252.\n",
    "3. If that doesn't work, try your system's default encoding.\n",
    "4. If that doesn't work, go back to where you got the data and see if the encoding is documented, or ask the person who gave it to you.\n",
    "5. Worst case, you might have to find the specific bytes that can't be read, google them, and try to piece together what encoding is probably being used based on what encodings have a mapping for that string of bytes.\n",
    "\n",
    "When you open a file in Python, you can optionally specify the file encoding.  If you don't, Python will default to whatever your system's default encoding is (Windows-1252 for Windows, UTF-8 for basically everything else).  I strongly recommend always explicitly specifying an encoding when reading and writing files, in case someone need to run your code on a different computer that may have a different operating system, or the same operating system configured with a different default encoding.\n",
    "\n",
    "## File \"modes\"\n",
    "\n",
    "Files can be opened in several different \"modes.\"\n",
    "\n",
    "1. \"Read\" mode (`mode=\"r\"`).  The file's contents can only be read; no modifications can be made.\n",
    "2. \"Write\" mode (`mode=\"w\"`).  The file's contents are erased, and then re-written to.\n",
    "3. \"Append\" mode (`mode=\"a\"`).  The file's contents are preserved, and new data is written to the very end of the file.\n",
    "\n",
    "In addition, Python lets you specify whether a file's contents should be *decoded* from binary to text using a specified encoding, or whether the data should be read in as \"raw bytes\" that are *not* decoded.  The former is the default behavior.  The latter can be specified by adding a `\"b\"` to the end of the `mode` string.  E.g.:\n",
    "\n",
    "1. Read the file as raw bytes; don't decode anything: `mode=\"rb\"`\n",
    "2. Open the file for writing, and write raw bytes into it; don't encode data before writing: `mode=\"wb\"`\n",
    "3. Open the file for appending, and write raw bytes into it: `mode=\"ab\"`\n",
    "\n",
    "Usually you only use these \"binary modes\" in fairly specific circumstances.  It's rare that you'll use them for general-purpose file access.\n",
    "\n",
    "## Basic text files in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fff3aa3-37c4-4a5b-983b-a683249886c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two files to show different encodings.  Smart quote characters\n",
    "# are where I've seen the most differences and issues with encodings,\n",
    "# mostly because UTF-8 and ISO-8859-1 encode them differently.\n",
    "example_text = \"“Hello”, he said, “to you”\"\n",
    "\n",
    "# We'll come back to `with` in just a moment.\n",
    "with open(\"my file.txt\", \"w\", encoding=\"utf8\") as OUT:\n",
    "    OUT.write(example_text)\n",
    "    \n",
    "with open(\"my windows-1252 file.txt\", \"w\", encoding=\"windows-1252\") as OUT:\n",
    "    OUT.write(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e51d32c-36cf-489a-8464-b450886c6a73",
   "metadata": {},
   "source": [
    "*Digression: `with`*\n",
    "\n",
    "Sometimes when you open a file, your program might unexpectedly exit (e.g. crash) before the file gets closed.  This can cause some issues.  The `with` statement in Python helps you avoid this: if the program exits for any reason, the `with` statement makes sure that the file gets closed.\n",
    "\n",
    "You'll usually see `with` used for opening files, opening connections (e.g., connecting to a database), or sometimes, temporarily moving computations to a different device (e.g. from the CPU to the GPU; neural network libaries are the most common places to see this).\n",
    "\n",
    "You can think of `with` as taking code that looks like:\n",
    "```python\n",
    "my_file = open(\"some file\")\n",
    "# do stuff with my_file\n",
    "my_file.close()\n",
    "```\n",
    "\n",
    "And making sure that `my_file.close()` *always* gets called.  The above block, rewritten using `with`, looks like:\n",
    "\n",
    "```python\n",
    "with open(\"some file\") as my_file:\n",
    "    # do stuff with my_file\n",
    "```\n",
    "\n",
    "This will bind the result of `open(\"some file\")` to the variable `my_file`, and make that variable accessible to you within the scope of the `with` block.\n",
    "\n",
    "This paradigm of using `with` is often called *context management,* since it's kind of like saying \"in this context, `my_file` means `open(\"some file\")`.  Now, do some stuff with `my_file`...\"\n",
    "\n",
    "*End of digression.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dc22fda-a58d-4fb3-a9a8-b78cbd200cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Hello”, he said, “to you”\n",
      "b'\\xe2\\x80\\x9cHello\\xe2\\x80\\x9d, he said, \\xe2\\x80\\x9cto you\\xe2\\x80\\x9d'\n"
     ]
    }
   ],
   "source": [
    "# Open a file using the default system encoding.\n",
    "# Note: this is a file that *does not have a file extention.*\n",
    "# Python--and all programming languages I know of--generally\n",
    "# require you to include the file extension on a file, if present.\n",
    "#\n",
    "# Note that `encoding=\"utf8\"` is *required* on Windows computers--\n",
    "# it my not be on Mac and Linux, where UTF-8 is often the system-wide\n",
    "# default.\n",
    "contents = open(\n",
    "    \"my file.txt\",   # Open the file \"my file\"...\n",
    "    mode=\"r\"  ,       # ...in \"read\" mode...\n",
    "    encoding=\"utf8\", # ...using utf-8 encoding...\n",
    ").read()             # ...and read the resulting text into memory.\n",
    "print(contents)\n",
    "\n",
    "# Note: `mode=` is usually omitted, and the open mode passed positionally.\n",
    "# WHen reading in binary mode, you cannot specify an encoding--the encoding\n",
    "# tells how to map from binary to text, but reading a file in binary mode\n",
    "# says \"don't do that conversion.\"\n",
    "binary_contents = open(\n",
    "    \"my file.txt\", # Open the file \"my file\"...\n",
    "    \"rb\"           # ...in \"binary read\" mode...\n",
    ").read()           # ..and read the resulting bytes into memory.\n",
    "print(binary_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23a62cd8-58b7-4730-9379-d21b63127ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Hello”, he said, “to you”\n"
     ]
    }
   ],
   "source": [
    "# Specify an encoding.  The name of the encoding is usually case-insensitive;\n",
    "# this is eon of the only places in Python where things are case-insensitive.\n",
    "# The `encoding` argument is usually passed by name; it's the fourt or fifth\n",
    "# defined argument to `open()`.\n",
    "#\n",
    "# Note that the windows-1252 encoding will be used by default on Windows computers\n",
    "# if you don't specify encoding=.\n",
    "contents = open(\n",
    "    \"my windows-1252 file.txt\", # Open the file \"my file\"...\n",
    "    \"r\",                        # ...in \"read\" mode...\n",
    "    encoding=\"windows-1252\"     # ...decode the binary data using the UTF-8 codec...\n",
    ").read()                        # ...and read the resuting text into memory.\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba8e10d4-bc8a-4526-b825-e044d0d4d0ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x93 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Errors will happen if the file can't be decoded using the specified encoding.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy windows-1252 file.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\PythonForDataAnalysis\\lib\\codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x93 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "# Errors will happen if the file can't be decoded using the specified encoding.\n",
    "contents = open(\"my windows-1252 file.txt\", \"r\", encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a7a526-8f5a-4172-9d7c-ef8fbe26376a",
   "metadata": {},
   "source": [
    "Once you've read data in to your program, it's just a string.  It's up to you at that point to figure out how to manipulate that string into the format you need.\n",
    "\n",
    "Sometimes you have to do that manipulation by hand, e.g., if there's some strange custom data format that you need to parse.  But, for the most common kinds of data format--CSV, JSON, XML, and a few others--there are tools in Python to do that for you.\n",
    "\n",
    "# CSV: Tabular Data\n",
    "\n",
    "CSV files are basically universal for storing tabular data.  They are text files that store data using the following conventions:\n",
    "\n",
    "1. There is a one-to-one relationship between the rows in the tabular data and the lines in the text file.\n",
    "2. Columns are sparated by commas.\n",
    "3. If a comma appears inside a cell, e.g. you have a string of text, that string must be *escaped*, usually by just surrounding it with quotes.\n",
    "\n",
    "Every spreadsheet program and data-oriented language can work with CSV files quite easily.  They have some downsides when it comes to speed--they can be slower to read and write than some other formats--and when you have *huge* data files, this can be exacerbated.  But for 95% of all use cases, CSVs is a really good go-to, if only because of how universally used they are.\n",
    "\n",
    "(Sidebar: .xlsx files from Excel are not text files; they're actually compressed archives storing a mishmash of folders and files, with the actual file contents being stored as XML.  Excel files are pretty universal too, and will generally be smaller than the equivalent CSV, but can be a *lot* slower to read and write).\n",
    "\n",
    "Strictly speaking, CSV files are a subset of *delimited files.*  Any symbol (or sequence of symbols) could be used to separate columns.  Tabs are a common alternative--yielding Tab Separated Value (.tsv) files--as are *pipe* characters (i.e., \"|\").  All CSV parsers I know of allow you to specify the column delimiter character, though, so the same approaches that work with CSV can work very easily with other delimited formats.\n",
    "\n",
    "Python's standard library has a `csv` module that contains some basic, but useful, tools for reading and writing CSV files.  These work with Python `list`s and `dict`ionaries, and don't make it easy to do much beyond very basic analyses, but they do make it very easy to work with enormous files (they only read/write one row at a time, so you can process a huge file with a very small amount of RAM).\n",
    "\n",
    "Later on, we'll see the `pandas` third-party library, which offers a *much* richer set of ways to interact with tabular data--including CSV data--and is generally a better option if your data isn't enormous.  But, the standard library's `csv` tools are still important to know about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6deffe79-32df-434d-98ac-02aed12f8a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Language', 'Number of Speakers (millions)', 'Language Family']\n",
      "['English', '1000', 'Indo-European']\n",
      "['Zulu', '28', 'Niger-Congo']\n",
      "['Tamil', '83', 'Dravidian']\n",
      "['Zapotec', '0.5', 'Oto-Manguean']\n",
      "\n",
      "{'Language': 'English', 'Number of Speakers (millions)': '1000', 'Language Family': 'Indo-European'}\n",
      "{'Language': 'Zulu', 'Number of Speakers (millions)': '28', 'Language Family': 'Niger-Congo'}\n",
      "{'Language': 'Tamil', 'Number of Speakers (millions)': '83', 'Language Family': 'Dravidian'}\n",
      "{'Language': 'Zapotec', 'Number of Speakers (millions)': '0.5', 'Language Family': 'Oto-Manguean'}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Read a CSV file into a list of lists:\n",
    "# [\n",
    "#     [row1_col1, row1_col2, row1_col3, ...],\n",
    "#     [row2_col1, row2_col2, row2_col3, ...],\n",
    "#     ...\n",
    "# ]\n",
    "with open(\"simple_csv.csv\", \"r\", encoding=\"utf8\") as INFILE:\n",
    "    reader = csv.reader(INFILE)\n",
    "    for row in reader:\n",
    "        print(row)\n",
    "        \n",
    "# csv.DictReader --> returns a list of dictionaries, one dictionary\n",
    "# per row, containing {column_name: value} pairs.\n",
    "# [\n",
    "#     {\"col1\": row1_col1, \"col2\": row1_col2, ...},\n",
    "#     {\"col1\": row2_col1, \"col2\": row2_col2, ...},\n",
    "#     ...\n",
    "# ]\n",
    "# Unlike csv.reader(), this will default to parsing the first row as\n",
    "# column headers.  csv.reader() treats the first row like any other\n",
    "# row.\n",
    "print()\n",
    "with open(\"simple_csv.csv\", \"r\", encoding=\"utf8\") as INFILE:\n",
    "    reader = csv.DictReader(INFILE)\n",
    "    for row in reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc4846ab-1f12-4b7b-9a0b-f7732b7e4825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Write to files with the corresponding writer objects.\n",
    "# NOTE: newline=\"\" is needed here.  Normally multiple calls to open(...).write()\n",
    "# will automatically append either \"\\n\" or (on Windows) \"\\r\\n\".  The CSV writers\n",
    "# handle the newline writing to make sure everything complies with the CSV\n",
    "# specification; so setting `newline=\"\"` essentially says \"it is the job\n",
    "# of whatever is writing data into this file to manage when text moves down\n",
    "# to the next line\"--and in this case, that's good, because the csv writers\n",
    "# can handle it.\n",
    "with open(\"my new csv file.csv\", \"w\", encoding=\"utf8\", newline=\"\") as OUTFILE:\n",
    "    writer = csv.writer(OUTFILE)\n",
    "    # first row is not \"special\"--just write the column names.\n",
    "    # It's up to us to make sure the order of columns later is correct.\n",
    "    writer.writerow([\"Language\", \"Speakers\", \"Language Family\"])\n",
    "    \n",
    "    # Write data one row at a time, with columns in the same order\n",
    "    # as the first row we wrote.\n",
    "    writer.writerow([\"Djinang\", \"125\", \"Pama-Nyungan\"])\n",
    "    writer.writerow([\"Unangam Tunuu\", \"150\", \"Eskimo-Aleut\"])\n",
    "    writer.writerow([\"Hurrian\", \"0 (extinct)\", \"Hurro-Urartian\"])\n",
    "    \n",
    "# csv.DictWriter will manage column order for us.  We pass its .writerow()\n",
    "# method a dictionary of {column: value} pairs, and it puts them in the\n",
    "# right places.  So the order of the pairs doesn't matter.\n",
    "with open(\"my new csv file.csv\", \"w\", encoding=\"utf8\", newline=\"\") as OUTFILE:\n",
    "    writer = csv.DictWriter(OUTFILE, fieldnames=[\"Language\", \"Speakers\", \"Language Family\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerow({\"Language\": \"Djinang\", \"Speakers\": \"125\", \"Language Family\": \"Pama-Nyungan\"})\n",
    "    writer.writerow({\"Speakers\": \"150\", \"Language\": \"Unangam Tunuu\", \"Language Family\": \"Eskimo-Aleut\"})\n",
    "    writer.writerow({\"Language Family\": \"Hurro-Urartian\", \"Speakers\": \"0 (extinct)\", \"Language\": \"Hurrian\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00b15c47-34a8-4ab6-8627-06d3de74e1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language,Number of Speakers (millions),Language Family\n",
      "English,1000,Indo-European\n",
      "Zulu,28,Niger-Congo\n",
      "Tamil,83,Dravidian\n",
      "Zapotec,0.5,Oto-Manguean\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Of course, since CSV files are just text, we can open them like normal text files.\n",
    "csv_data = open(\"simple_csv.csv\", \"r\", encoding=\"utf8\").read()\n",
    "print(csv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e9c051-02bf-4e07-80f6-d5834fe04718",
   "metadata": {},
   "source": [
    "## JSON files: universal, flexible, and fast (enough)\n",
    "\n",
    "JavaScript Object Notation (JSON) is a more general way to store data than CSV.  It's used almost literally everywhere.  Usually, the term JSON is used as a noun to refer to \"a piece of data formatted using the JSON specification.\"\n",
    "\n",
    "JSONs look a *lot* like Python `dict`ionaries.  They're key-value pairs surrounded by curly brackets.  `dict`ionaries are actually just a more general form of JSONs, if you squint.  Here are the major differences:\n",
    "\n",
    "- JSON keys can only be strings.\n",
    "- JSON values can only be strings, numbers, arrays, booleans (`true` and `false`--all lowercase, unlike Python's `True` and `False`), missing (`null`, compare to Python's `None`), or other JSONs.  Arrays can contain strings, numbers, arrays, and JSONs in any combination.\n",
    "- Text *must* be surrounded by *single quotes.*  Python doesn't care about `'single'` versus `\"double\"` quotes for strings, but JSON does, and JSON only allows double quotes.\n",
    "- All values must be *literal values.*  JSON does not have a notion of variables.  (when you're creating a JSON in Python, Python will convert variables to their literal values--you don't have to worry about that step).\n",
    "\n",
    "Beyond that, JSONs look like Python dictionaries:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"name\": \"Henry\",\n",
    "    \"job title\": \"Data Scientist\",\n",
    "    \"favorite languages\": [\"Python\", \"Julia\", \"Haskell\", \"Lua\"],\n",
    "    \"favorite numbers\": [42, 5, 2.718],\n",
    "    \"some random junk\": [1, \"two\", [3, 4, 5], {\"six\": 6, \"7\": \"seven\"}]\n",
    "}\n",
    "```\n",
    "\n",
    "You'll often see JSONs written on a single line and without spaces between things, to save a bit of space in files:\n",
    "```json\n",
    "{\"1\":2,\"3\":4,\"5\":\"six\"}\n",
    "```\n",
    "\n",
    "You'll also see a lot of files in the not-quite-official-but-effectively-official *newline-delimited JSON* (ndjson) format, where each line in a file contains one JSON object.  Strictly speaking, this ndjson format is *not* part of the JSON specification; for a file to contain multiple JSONs and still be a completely valid JSON file, top to bottom, you'd need it to be an array of JSONs.  But, there is no benefit to doing this; to parse the JSON array, you'd need to parse the entire file all at once, which might take a while if the file is large.  With ndjson, you cn read one line at a time, parse it, do what you need, and then grab the next.  This *incremental* parsing is way easier to work with, so ndjson is *effectively* the way that JSON data is always stored in files.\n",
    "\n",
    "The simplest way to parse JSON data in Python is with the `json` module, which only has four functions you ever really need to care about (and really, only two of those are ever super useful):\n",
    "\n",
    "- `json.loads(\"some text\")`: parse a JSON string and return a corresponding Python dictionary.\n",
    "- `json.dumps(a dictionary)`: convert a Python dictionary to a corresponding JSON string, and throw an error if there's something that can't be JSON'd.  (e.g., a special Python object).\n",
    "- `json.load(opened file)`: read the specified file's contents, and parse it as a single JSON object. (`opened file` should be a readable file-like object, e.g. what you get from calling `open()`)\n",
    "- `json.dump(dictionary, opened file)`: convert the Python dictionary `dictionary` into a JSON object, and write it to `opened file` (`opened file` should be a writeable file-like object, e.g. what you get from calling `open()`).\n",
    "\n",
    "Realisically, you'll very rarely use `json.load()` or `json.dump()`.  Usually you'll be working with `json.loads()` and `json.dumps()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "483835f0-8755-4107-b6f0-f74d596f488c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Henry', 'job title': 'Data Scientist', 'favorite languages': ['Python', 'Julia', 'Haskell', 'Lua'], 'favorite numbers': [42, 5, 2.718], 'some random junk': [1, 'two', [3, 4, 5], {'six': 6, '7': 'seven'}]}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_data = \"\"\"{\n",
    "    \"name\": \"Henry\",\n",
    "    \"job title\": \"Data Scientist\",\n",
    "    \"favorite languages\": [\"Python\", \"Julia\", \"Haskell\", \"Lua\"],\n",
    "    \"favorite numbers\": [42, 5, 2.718],\n",
    "    \"some random junk\": [1, \"two\", [3, 4, 5], {\"six\": 6, \"7\": \"seven\"}]\n",
    "}\"\"\"\n",
    "parsed = json.loads(json_data)\n",
    "print(parsed)\n",
    "print(type(parsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cdd8278-75d8-402f-9326-e2c33d747076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'{\"1\": 2, \"3\": 4, \"5\": 6}'\n",
      "'[1, 2, 3]'\n",
      "'\"blah blah blah\"'\n",
      "'[true, {\"Nope\": false}]'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "my_dict = {1:2, 3:4, \"5\": 6}\n",
    "# All keys will be converted to strings!\n",
    "# repr() just makes sure that strings get printed with quotes around them;\n",
    "# normally the quotes we put around strings aren't part of printed output.\n",
    "print(repr(json.dumps(my_dict)))\n",
    "\n",
    "# Dictionaries, lists, strings, ints, bools, and None can all be serialized.\n",
    "print(repr(json.dumps([1,2,3])))\n",
    "print(repr(json.dumps(\"blah blah blah\")))\n",
    "print(repr(json.dumps([True, {\"Nope\": False}])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5318d3-31ae-4af2-9dc8-3bacf677d3ba",
   "metadata": {},
   "source": [
    "Sometimes, you may end up needing way more speed than you can get with the built-in `json` module.  Usually, it's good enough--but sometimes you have so much data that you need to squeeze some extra speed out of it.  If you want to see some libaries that are specialized for faster JSON parsing, check the `01a - Faster JSON processing.ipynb` notebook.\n",
    "\n",
    "# XML: JSON's bigger, badder, uglier cousin\n",
    "\n",
    "eXtended Markup Language (XML) is a data format that is almost as universal as JSON, but is more commonly reserved for marking up document structure.  HTML is a variant of XML, and all the Microsoft Office file formats--.docx, .xlsx, .pptx, etc--store their information as XML under the hood (that's actually what the \"x\" in the extension indicates).\n",
    "\n",
    "XML is based around a few ideas:\n",
    "- The *tag*.  A basic tag looks like: `<tagname>contents</tagname>`.  `tagname` can be basically anything.  `contents` can be text, or more XML tags.\n",
    "- Some tags have no contents: `<tagname />`\n",
    "- Some tags have *attributes*: `<person name=Henry job=DataScientist>`.\n",
    "\n",
    "With a few exceptions, you can just make tag names and attribute names on the fly as you see fit, kind of like keys in a JSON.  \n",
    "\n",
    "XML is far more flexible, but far more complex, than JSON.  A quick list of the tradeoffs:\n",
    "\n",
    "Pros of XML:\n",
    "- Allow you to store much more complex and detailed data.\n",
    "- Can easily be tweaked/extended to create new \"dialects\" with new features (HTML is the most common dialect of XML!).\n",
    "- A very natural way to express certain kinds of data.\n",
    "- XML supports variables (sort of), JSON does not.\n",
    "- XML supports comments inside the document.\n",
    "\n",
    "Cons of XML:\n",
    "- It is extremely \"noisey\"--the syntax has a lot of stuff that isn't the data itself.  This makes it very hard for a human to read, and means XML files are almost always larger than equivalent JSON files for the same data.\n",
    "- It is usually slower to read and write than JSON data, all else being equal.\n",
    "- XML doesn't have notions of arrays or data types; this can make it a bit unintuitive to store some kinds of data.\n",
    "- It's harder--not impossible, but harder--to parse an XML file incrementally.  You can have tags that open once and take forever to close, which means you have to keep something around in RAM until it closes.\n",
    "- It's *extremely* rare, but there are some vulnerabilities in XML.  E.g., the delightfully named \"billion laughs\" exploit.\n",
    "\n",
    "If we take the previous JSON example:\n",
    "```json\n",
    "{\n",
    "    \"name\": \"Henry\",\n",
    "    \"job title\": \"Data Scientist\",\n",
    "    \"favorite languages\": [\"Python\", \"Julia\", \"Haskell\", \"Lua\"],\n",
    "    \"favorite numbers\": [42, 5, 2.718],\n",
    "    \"some random junk\": [1, \"two\", [3, 4, 5], {\"six\": 6, \"7\": \"seven\"}]\n",
    "}\n",
    "```\n",
    "\n",
    "We can convert it to XML (there are many possible ways we could do this, but this is just one):\n",
    "```xml\n",
    "<person>\n",
    "    <name>Henry</name>\n",
    "    <job_title>Data Scientit</job_title>\n",
    "    <favorite_languages>\n",
    "        <item>Python</item>\n",
    "        <item>Julia</item>\n",
    "        <item>Haskell</item>\n",
    "        <item>Lua</item>\n",
    "    </favorite_languages>\n",
    "    <favorite_numbers>\n",
    "        <item>42</item>\n",
    "        <item>5</item>\n",
    "        <item>2.718</item>\n",
    "    </favorite_numbers>\n",
    "    <some_random_junk>\n",
    "        <item>1</item>\n",
    "        <item>two</item>\n",
    "        <item>\n",
    "            <item>3</item>\n",
    "            <item>4</item>\n",
    "            <item>5</item>\n",
    "        </item>\n",
    "        <item>\n",
    "            <six>6</six>\n",
    "            <seven>7</seven>\n",
    "        </item>\n",
    "    </some_random_junk>\n",
    "</person>\n",
    "```\n",
    "\n",
    "Or, we could maybe write the very last bit as an empty tag with some attributes:\n",
    "```xml\n",
    "<person>\n",
    "    <!-- some stuff -->\n",
    "    <item><map six=6 seven=7 /></item>\n",
    "</person>\n",
    "```\n",
    "\n",
    "It's a *lot* wordier and more complex than JSON, and typing it out by hand is a pain.  But, it allows a *lot* more flexiblity.  I still avoid it wherever I can, though; usually, I never need the extra flexibility, and can just add a field to a JSON and get whatever I need.\n",
    "\n",
    "Python has some tools for parsing XML.  I'm not going to cover them too deeply here, since XML data is relatively uncommon compare to CSV/tabular data and JSON.  That said, there are three libraries you should be aware of:\n",
    "\n",
    "- `xml`, in the standard library.\n",
    "- `lxml`, a third party library.  Faster and generally upgraded version of `xml`.\n",
    "- `BeautifulSoup`, a third party library.  Specialized for HTML parsing, but you can probably make it work on general XML.  Provides a very easy-to-use interface, if a slightly slow one, for navigating an XML document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f1bafe5-fe5c-46f1-97f9-25f8d4dfd1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country {'name': 'Liechtenstein'}\n",
      "country {'name': 'Singapore'}\n",
      "country {'name': 'Panama'}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as etree\n",
    "\n",
    "# Quick parsing demo--mostly borrowed from Python's `xml` documentation.\n",
    "xml_data = \"\"\"<?xml version=\"1.0\"?>\n",
    "<data>\n",
    "    <country name=\"Liechtenstein\">\n",
    "        <rank>1</rank>\n",
    "        <year>2008</year>\n",
    "        <gdppc>141100</gdppc>\n",
    "        <neighbor name=\"Austria\" direction=\"E\"/>\n",
    "        <neighbor name=\"Switzerland\" direction=\"W\"/>\n",
    "    </country>\n",
    "    <country name=\"Singapore\">\n",
    "        <rank>4</rank>\n",
    "        <year>2011</year>\n",
    "        <gdppc>59900</gdppc>\n",
    "        <neighbor name=\"Malaysia\" direction=\"N\"/>\n",
    "    </country>\n",
    "    <country name=\"Panama\">\n",
    "        <rank>68</rank>\n",
    "        <year>2011</year>\n",
    "        <gdppc>13600</gdppc>\n",
    "        <neighbor name=\"Costa Rica\" direction=\"W\"/>\n",
    "        <neighbor name=\"Colombia\" direction=\"E\"/>\n",
    "    </country>\n",
    "</data>\"\"\"\n",
    "\n",
    "# Load the XML data into a tree-like structure.\n",
    "root = etree.fromstring(xml_data)\n",
    "# if you have a .xml file:\n",
    "# tree = etree.parse(\"filename\")\n",
    "# root = tree.getroot()\n",
    "\n",
    "# `root` is now an iterable over XML `element`s.\n",
    "# Each element can be indexed to get tags \"inside\" of it,\n",
    "# and we can access different attributes on them.\n",
    "for child in root:\n",
    "    print(child.tag, child.attrib)\n",
    "    # print(\"\\t\", child[0].tag, child[0].attrib)\n",
    "    # print(\"\\t\", child[-1].tag, child[-1].attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48f4161b-2b23-4567-b7af-35c52210edba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country {'name': 'Liechtenstein'}\n",
      "\t rank {}\n",
      "\t neighbor {'name': 'Switzerland', 'direction': 'W'}\n",
      "country {'name': 'Singapore'}\n",
      "\t rank {}\n",
      "\t neighbor {'name': 'Malaysia', 'direction': 'N'}\n",
      "country {'name': 'Panama'}\n",
      "\t rank {}\n",
      "\t neighbor {'name': 'Colombia', 'direction': 'E'}\n"
     ]
    }
   ],
   "source": [
    "# using lxml\n",
    "from lxml import etree\n",
    "\n",
    "xml_data = \"\"\"<?xml version=\"1.0\"?>\n",
    "<data>\n",
    "    <country name=\"Liechtenstein\">\n",
    "        <rank>1</rank>\n",
    "        <year>2008</year>\n",
    "        <gdppc>141100</gdppc>\n",
    "        <neighbor name=\"Austria\" direction=\"E\"/>\n",
    "        <neighbor name=\"Switzerland\" direction=\"W\"/>\n",
    "    </country>\n",
    "    <country name=\"Singapore\">\n",
    "        <rank>4</rank>\n",
    "        <year>2011</year>\n",
    "        <gdppc>59900</gdppc>\n",
    "        <neighbor name=\"Malaysia\" direction=\"N\"/>\n",
    "    </country>\n",
    "    <country name=\"Panama\">\n",
    "        <rank>68</rank>\n",
    "        <year>2011</year>\n",
    "        <gdppc>13600</gdppc>\n",
    "        <neighbor name=\"Costa Rica\" direction=\"W\"/>\n",
    "        <neighbor name=\"Colombia\" direction=\"E\"/>\n",
    "    </country>\n",
    "</data>\"\"\"\n",
    "\n",
    "root = etree.fromstring(xml_data)\n",
    "# root = lxml.etree.ElementTree(root)\n",
    "\n",
    "# then, same as before\n",
    "for child in root:\n",
    "    print(child.tag, child.attrib)\n",
    "    print(\"\\t\", child[0].tag, child[0].attrib)\n",
    "    print(\"\\t\", child[-1].tag, child[-1].attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c1d0899-6515-4eaa-b506-caf8d9fe8f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<neighbor direction=\"E\" name=\"Austria\"/>\n",
      "<neighbor direction=\"N\" name=\"Malaysia\"/>\n",
      "<neighbor direction=\"W\" name=\"Costa Rica\"/>\n",
      "\n",
      "<neighbor direction=\"W\" name=\"Switzerland\"/>\n",
      "<neighbor direction=\"W\" name=\"Costa Rica\"/>\n"
     ]
    }
   ],
   "source": [
    "# using BeautifulSoup.  Install as `beautifulsoup4`,\n",
    "# import as `bs4`.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "xml_data = \"\"\"<?xml version=\"1.0\"?>\n",
    "<data>\n",
    "    <country name=\"Liechtenstein\">\n",
    "        <rank>1</rank>\n",
    "        <year>2008</year>\n",
    "        <gdppc>141100</gdppc>\n",
    "        <neighbor name=\"Austria\" direction=\"E\"/>\n",
    "        <neighbor name=\"Switzerland\" direction=\"W\"/>\n",
    "    </country>\n",
    "    <country name=\"Singapore\">\n",
    "        <rank>4</rank>\n",
    "        <year>2011</year>\n",
    "        <gdppc>59900</gdppc>\n",
    "        <neighbor name=\"Malaysia\" direction=\"N\"/>\n",
    "    </country>\n",
    "    <country name=\"Panama\">\n",
    "        <rank>68</rank>\n",
    "        <year>2011</year>\n",
    "        <gdppc>13600</gdppc>\n",
    "        <neighbor name=\"Costa Rica\" direction=\"W\"/>\n",
    "        <neighbor name=\"Colombia\" direction=\"E\"/>\n",
    "    </country>\n",
    "</data>\"\"\"\n",
    "\n",
    "# `features=\"xml\"` tells BeautifulSoup that this is plain\n",
    "# XML; BeautifulSoup normally assumes it is specifically HTML,\n",
    "# and will throw a warning if you don't specify this.  It will\n",
    "# also use an HTML parser, which won't be as reliable as an XML\n",
    "# parser if the data is in fact XML.\n",
    "soup = BeautifulSoup(xml_data, features=\"xml\")\n",
    "\n",
    "# bs4 sometimes adds surrounding <html></html> tags to documents\n",
    "# if they're not already present.\n",
    "# HTML uses .find() to find tags with certian attributes.\n",
    "for i in soup.find_all(\"country\"):\n",
    "    print(i.find(\"neighbor\"))\n",
    "    \n",
    "# find all neighbors with direction=\"W\" attributes\n",
    "print()\n",
    "for i in soup.find_all(\"neighbor\", direction=\"W\"):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c988b-c6c7-4d84-a44c-facffd5a5335",
   "metadata": {},
   "source": [
    "In general, I recommend *not* using XML for storing your own data, unless you have to.\n",
    "\n",
    "# Pickle, the Python-native format\n",
    "\n",
    "Plaintext, CSV, JSON, and XML all have their pros and cons, but the biggest con is this: *they can't store native Python object.*  Sure, if you're only using lists and dictionaries and simple data types, they'll all work. But what if you need to store some more complex object, like a set, or a custom class, or even a function?  This is where `pickle` comes in: Python's native, in-house solution to serializing data.\n",
    "\n",
    "Compared to the previous formats, `pickle` has some very important properties:\n",
    "- It is a binary format, *not* a text format.  The 1s and 0s are for Python's eyes, not yours.\n",
    "- It can store *any kind of thing that Python can work with.*\n",
    "- Like JSON, it stores on-thing-per-file.  But that thing can be a list, or dictionary, or set, or tuple that contains other thing you need.\n",
    "- Pickle is *only* meant for Python.  Other languages *can* read pickled data--everything about how it works is completely open-source--but they usually don't, because there's no reason to load a Python dictionary in R.  (or, there are usually better ways to do it).\n",
    "- ***Very importantly***: pickled files *can execute arbitrary Python code when you load them.* This can be a big security risk.  This is *not* an issue if it's a pickle file you created, but treat pickle files like executables: only use them if you trust where they came from.\n",
    "    - Usually, you should not be using pickle files to share data with people online, for this very reason.  It's better to convert your data to CSV/JSON/XML/etc and share any code that they might need to read the data into their program.\n",
    "    \n",
    "The `pickle` module in the standard library has basically the same interface as `json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cb9cb84-25bc-4781-8f38-4fe62d5686fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type set is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# This will throw an error; JSON can't serialize Python sets.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m my_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTexas\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHouston\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDallas\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFort Worth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArlington\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAustin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSan Antonio\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalifornia\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLos Angeles\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSan Francisco\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSacramento\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSan Bruno\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew York\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew York\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlbany\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSyracuse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRochester\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuffalo\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m      6\u001b[0m }\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_data\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\PythonForDataAnalysis\\lib\\json\\__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# cached encoder\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m skipkeys \u001b[38;5;129;01mand\u001b[39;00m ensure_ascii \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     check_circular \u001b[38;5;129;01mand\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m separators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort_keys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\PythonForDataAnalysis\\lib\\json\\encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\PythonForDataAnalysis\\lib\\json\\encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[0;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\PythonForDataAnalysis\\lib\\json\\encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type set is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# This will throw an error; JSON can't serialize Python sets.\n",
    "my_data = {\n",
    "    \"Texas\": {\"Houston\", \"Dallas\", \"Fort Worth\", \"Arlington\", \"Austin\", \"San Antonio\"},\n",
    "    \"California\": {\"Los Angeles\", \"San Francisco\", \"Sacramento\", \"San Bruno\"},\n",
    "    \"New York\": {\"New York\", \"Albany\", \"Syracuse\", \"Rochester\", \"Buffalo\"}\n",
    "}\n",
    "print(json.dumps(my_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "033aaf30-a9bd-47f3-b1df-c305d26978dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x80\\x04\\x95\\xd7\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x94(\\x8c\\x05Texas\\x94\\x8f\\x94(\\x8c\\tArlington\\x94\\x8c\\x06Dallas\\x94\\x8c\\nFort Worth\\x94\\x8c\\x0bSan Antonio\\x94\\x8c\\x06Austin\\x94\\x8c\\x07Houston\\x94\\x90\\x8c\\nCalifornia\\x94\\x8f\\x94(\\x8c\\nSacramento\\x94\\x8c\\tSan Bruno\\x94\\x8c\\x0bLos Angeles\\x94\\x8c\\rSan Francisco\\x94\\x90\\x8c\\x08New York\\x94\\x8f\\x94(\\x8c\\x06Albany\\x94\\x8c\\tRochester\\x94\\x8c\\x08Syracuse\\x94\\x8c\\x07Buffalo\\x94h\\x0f\\x90u.'\n",
      "\n",
      "{'Texas': {'Arlington', 'Fort Worth', 'Austin', 'Houston', 'San Antonio', 'Dallas'}, 'California': {'Los Angeles', 'San Bruno', 'Sacramento', 'San Francisco'}, 'New York': {'New York', 'Rochester', 'Syracuse', 'Buffalo', 'Albany'}}\n"
     ]
    }
   ],
   "source": [
    "# This wiill work just fine!\n",
    "# But you won't be able to read most of this.\n",
    "import pickle\n",
    "my_data = {\n",
    "    \"Texas\": {\"Houston\", \"Dallas\", \"Fort Worth\", \"Arlington\", \"Austin\", \"San Antonio\"},\n",
    "    \"California\": {\"Los Angeles\", \"San Francisco\", \"Sacramento\", \"San Bruno\"},\n",
    "    \"New York\": {\"New York\", \"Albany\", \"Syracuse\", \"Rochester\", \"Buffalo\"}\n",
    "}\n",
    "print(pickle.dumps(my_data))\n",
    "print()\n",
    "\n",
    "dumped = pickle.dumps(my_data)\n",
    "print(pickle.loads(dumped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4efd8f7e-5d8f-4df0-b4f3-658ee1e8fe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Texas': {'Arlington', 'Fort Worth', 'Austin', 'Houston', 'San Antonio', 'Dallas'}, 'California': {'Los Angeles', 'San Bruno', 'Sacramento', 'San Francisco'}, 'New York': {'New York', 'Rochester', 'Syracuse', 'Buffalo', 'Albany'}}\n"
     ]
    }
   ],
   "source": [
    "# when saving to and loading from files, the file needs to be opened in binary mode.\n",
    "# .p is the typical extension for pickled files.\n",
    "import pickle\n",
    "my_data = {\n",
    "    \"Texas\": {\"Houston\", \"Dallas\", \"Fort Worth\", \"Arlington\", \"Austin\", \"San Antonio\"},\n",
    "    \"California\": {\"Los Angeles\", \"San Francisco\", \"Sacramento\", \"San Bruno\"},\n",
    "    \"New York\": {\"New York\", \"Albany\", \"Syracuse\", \"Rochester\", \"Buffalo\"}\n",
    "}\n",
    "\n",
    "# Dump the data to a file\n",
    "pickle.dump(my_data, open(\"my_data.p\", \"wb\"))\n",
    "\n",
    "# Load the data from file\n",
    "print(pickle.load(open(\"my_data.p\", \"rb\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde8f09a-629b-4f08-acf0-59b8f838fe17",
   "metadata": {},
   "source": [
    "# A note on avoiding binary data formats\n",
    "\n",
    "Plain text formats for data storage are popular for a reason.  They're extremely easy to work with.  There are plenty of excellent *binary* data formats that are great for storing and sharing data--Parquet for tabular data, for instance--but they usually require more work to implement a parser for, and have more specialized use cases and goals.\n",
    "\n",
    "I recommend, in general, that you default to storing your data in plain text files, with a UTF-8 encoding.  They're easy to store, easy to share, and easy to inspect without having to load the whole thing into a program first.\n",
    "\n",
    "# Compressed files: Making files smaller without losing any data\n",
    "\n",
    "Data compression is extremely important.  A lot of data contains some amount of repetition; any time there's repetition, you can essentially \"get rid\" of some of it, replace it with a placeholder, and save some space in the file.  There are a lot of different algorithms for compressing data, each with their own pros and cons, but you'll see a few pop up all over the place:\n",
    "\n",
    "- GZIP, which uses the .gz extension.\n",
    "- BZIP2, which uses the .bz2 extension.\n",
    "- LZMA (sometimes erroneou called XZ; XZ is the name of a program that implements LZMA compression), which uses the .xz extension.\n",
    "- LZ4, which uses the lz4 extension.\n",
    "- Zstandard, which uses the .zst extention.  It's taking over as the general go-to compression algorithm, for good reason.\n",
    "\n",
    "GZIP, BZIP2, and LZMA/XZ are supported in Python's standard library.  Zstandard is supported in the `zstandard` third-party library, and LZ4 in the `lz4` third-party library.  (clever names, I know).\n",
    "\n",
    "The details of how compression works and how to pick the right algorithm is a very detailed discussion for another time, but each algorithm makes different tradeoffs between compression ratio (how small the final compressed data is), compression speed, decompression speed, and memory use for compression/decompression.  Here's my quick rule of thumb for how to pick a compression algorithm:\n",
    "- LZ4: bad compression ratios, but extremely fast.  Pick this when you need to be able to read and write to your file very quickly, and saving space is a \"nice to have\" feature.\n",
    "- GZIP/BZIP2 when you're willing to give up more speed in exchange for better compression ratios.\n",
    "- LZMA/XZ for when you are willing to give up a lot of speed for *really* good compression ratios, and pretty fast decompression later on.\n",
    "- Zstandard if you can use it, and you don't have any weird compatibility requirements.  (It can be as fast as LZ4 and have compression ratios as high as XZ).  Ust Zstandard when you need good compression ratios and *very* fast decompression--only LZ4 is faster, but Zstandard has way better compression ratios.\n",
    "\n",
    "Every compression algorithm lets you specify a *compression level*.  Higher compresion levels mean your computer will spend more time figuring out how to save as much space as possible when compressing the file, so it's a speed-versus-size tradeoff.\n",
    "\n",
    "An important note: once a file is compressed, it's a binary file, no longer a text file!\n",
    "\n",
    "Reading and writing compressed files looks the same regardless of the algorithm used, so the following examples will just use LZMA/XZ.  Every library has a `open()` function that behaves almost exactly like Python's built-in `open()`, but which will manage compression/decompression for the specified algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c91c8251-30cf-4654-9ee5-25b9c7540379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xfd7zXZ\\x00\\x00\\x04\\xe6\\xd6\\xb4F\\x02\\x00!\\x01\\x16\\x00\\x00\\x00t/\\xe5\\xa3\\xe0\\x00\\xc1\\x00\\x90]\\x00$\\x89\\xc9\\xa2\\x03\\xd1\\x14\\xfc0Y\\x02\\xf3\\xbd@j\\x0f\\xceK\\x93\\xce-\\xe9\\x08?\\xf8\\xf6\\x8f\\x9f\\r}\\x99g\\x02[\\xb4\\x80/w0(\\xb2\\xff\\xb4\\x82\\x84\\xc94\\xb5j\\xcf;\\x19\\x93\\xb0\\xfdCi3\\xf0;N&\\x07:yn\\xe4\\xa7\\xdb\\xc1&heO\\x0c\\xa9\\x13\\xde\\xa0\\x96\\n\\x95\\x02t\\xeb_\"\\xa9\\xfcw\\xceX6\\x16sp\\x9b\\x05X\\xe9C\\x02%\\xd5\\x04\\x8c\\x06OP\\x08\\xd6d\\x05\\xa1mO\\xef\\x1f\\xbbM\\x088h9\\xc0W\\xb2\\xec\\xd3!iF9\\x15\"r&\\x06C\\xbf\\xeb}*n\\x00@\\x1fU4ss\\xd26\\x00\\x01\\xac\\x01\\xc2\\x01\\x00\\x00B`mM\\xb1\\xc4g\\xfb\\x02\\x00\\x00\\x00\\x00\\x04YZ'\n",
      "\n",
      "b\"I'm some data!  This isn't enough data to get good compression with.  IN fact, the compressed file may actually get _larger_, since it needs to add some data about the compression into the file.\"\n",
      "\n",
      "I'm some data!  This isn't enough data to get good compression with.  IN fact, the compressed file may actually get _larger_, since it needs to add some data about the compression into the file.\n"
     ]
    }
   ],
   "source": [
    "import lzma\n",
    "\n",
    "# open in \"wt\" mode --> Python will handle compresing the text and writing the binary\n",
    "# data to the file.\n",
    "with lzma.open(\"compression_demo.xz\", \"wt\") as OUTFILE:\n",
    "    OUTFILE.write(\"I'm some data!  This isn't enough data to get good compression with.  \"\n",
    "                  \"IN fact, the compressed file may actually get _larger_, since it needs \"\n",
    "                  \"to add some data about the compression into the file.\")\n",
    "    \n",
    "# Read the data with Python's open() function\n",
    "print(open(\"compression_demo.xz\", \"rb\").read())\n",
    "\n",
    "# Read it with lzma.open(), which will handle decompression transparently for us.\n",
    "# \"rb\" mode will decompress the data and return it as bytes, which can be a bit faster\n",
    "# than \"rt\" mode, which performs the extra step of decoding this into a normal Python\n",
    "# string.  WHen in doubt, you generally want \"rt\".\n",
    "print()\n",
    "print(lzma.open(\"compression_demo.xz\", \"rb\").read())\n",
    "print()\n",
    "print(lzma.open(\"compression_demo.xz\", \"rt\").read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
