{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39b395bf-1797-4679-ac52-b222bc55ec26",
   "metadata": {},
   "source": [
    "# Faster JSON parsing\n",
    "\n",
    "For most purposes, the standard library's `json` module is all you need.  However, sometimes, you need to process a *lot* of JSON data, and you need to do it fast.  There are three libraries you should look at.  These are all third-party libraries, and need to be installed with `pip` or `conda`.\n",
    "\n",
    "First, and easiest: `ujson`.  `ujson` is a slimmed-down version of `json`, which is a drop-in replacement for probably 95% of all use cases.  It's got `ujson.loads()` and `ujson.dumps()`, which work exactly like their `json` counterparts, but are just faster.  Usually between 2x and 10x faster, depending on how big the JSONs are and how complex (i.e., how deeply nested--JSONs inside JSONs, arrays inside arrays, JSONs in arrays of JSONs containing arrays, etc) they are.\n",
    "\n",
    "I'll demonstrate this using the `timeit` module in the standard library, which will run a little snippet of code a bunch of times and tell us how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "117b22b3-d4f6-4c4c-bf06-63f4ce84a54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON library, loading json_data 1,000,000 times:  3.5366121000006387\n",
      "UJSON library, loading json_data 1,000,000 times:  1.635926400000244\n",
      "JSON library, serializing json_data 1,000,000 times:  4.752410699999018\n",
      "UJSON library, serializing json_data 1,000,000 times:  1.9669594999995752\n"
     ]
    }
   ],
   "source": [
    "from timeit import timeit\n",
    "\n",
    "import ujson\n",
    "# If you're converting code that already uses the built-in `json` module,\n",
    "# you can use this next line, and remove the `import json` from your code.\n",
    "# import ujson as json\n",
    "\n",
    "print(\n",
    "    \"JSON library, loading json_data 1,000,000 times: \",\n",
    "    timeit(\"json.loads(json_data)\", globals=globals(), number=1_000_000)\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"UJSON library, loading json_data 1,000,000 times: \",\n",
    "    timeit(\"ujson.loads(json_data)\", globals=globals(), number=1_000_000)\n",
    ")\n",
    "\n",
    "# Load that same JSON as a Python dictionary, then test how long it takes\n",
    "# to serialize it back to a JSON string.\n",
    "json_dict = json.loads(json_data)\n",
    "\n",
    "print(\n",
    "    \"JSON library, serializing json_data 1,000,000 times: \",\n",
    "    timeit(\"json.dumps(json_dict)\", globals=globals(), number=1_000_000)\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"UJSON library, serializing json_data 1,000,000 times: \",\n",
    "    timeit(\"ujson.dumps(json_dict)\", globals=globals(), number=1_000_000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d6efd6-3715-4f90-b94e-2bcded9505d2",
   "metadata": {},
   "source": [
    "Sometimes, you dont need the loading to be all that fast, but you need the serialization to be as fast as possible.  This is where `orjson` comes in.  Like `ujson`, it can work as a faster, drop-in replacement for `json`.  It's `loads()` speed is about on par with `ujson`.  But, it is absurdly fast for serializing a Python dictionary into a JSON string--this is because it serializes directly to a binary byte string, which is way faster than serializing to a Python text string.  (but, it does require to you be aware that it's a byte/binary string--you'll need to open files in \"wb\" mode to save it, for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74a58022-bff7-4e5e-807d-988bb4dcc429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORJSON library, loading json_data 1,000,000 times:  1.584552000000258\n",
      "ORJSON library, serializing json_data 1,000,000 times:  0.586960700000418\n"
     ]
    }
   ],
   "source": [
    "import orjson\n",
    "\n",
    "print(\n",
    "    \"ORJSON library, loading json_data 1,000,000 times: \",\n",
    "    timeit(\"orjson.loads(json_data)\", globals=globals(), number=1_000_000)\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"ORJSON library, serializing json_data 1,000,000 times: \",\n",
    "    timeit(\"orjson.dumps(json_dict)\", globals=globals(), number=1_000_000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe6f82-b2a4-4c08-9910-f474bef4efab",
   "metadata": {},
   "source": [
    "Lastly, there's `simdjson`, for when you need the parsing/loading step to be as fast as possible.  `simdjson` can be used as another drop-in replacement for `json` that will generally be faster, but if you want to get the most speed possible, you need to use its native/non-drop-in-replacement API.\n",
    "\n",
    "Note: `simdjson` gets installed as `pysimdjson` when you're using `pip` or `conda`, but it gets imported as `simdjson`.\n",
    "\n",
    "*Digression: why `smdjson` is so fast.*\n",
    "\n",
    "The vast, vast majority of the time spent parsing JSON data--when you're not using `json`--is not spent parsing the data itself.  That step is actually pretty fast.  The slow part is *constructing the Python dictionary.*  Every time you call a JSON library's `loads()` function, it converts the entire JSON into a Python dictionary.  This can be a problem when the JSON is very large, and you only need one or two things out of it (*especially* if those things are nested  few layers deep).  This might be the case if, for example, you have a huge amount of JSON data and you need to filter it down; maybe you're filtering tweet data (which comes in JSON format from the Twitter API) and only want to keep tweets that have geolocation tags.\n",
    "\n",
    "`simdjson` solves this problem by not contructing the Python dictionary.  It's actually not a Python library at all--it's a library for the C++ programming language, and it uses a *lot* of extremely cool (and sophisticated) tricks to parse JSON data at absurd speeds.  The `simdjson` library in Python is basically just a \"translation\" layer between Python and the C++ code.\n",
    "\n",
    "The Python library manages to get a lot of its speed by letting the much, much faster C++ code handle all the parsing and loading.  By default, *nothing* is handed over to Python until you specifically ask for it.  This is because parsing and loading the data in C++ is *way* faster than in Python, so `simdjson` wants to avoid converting things into Python objects unless it *absolutely* has to.\n",
    "\n",
    "The end result is that when you ask `simdjson` to parse some JSON data, it does--more or less--the following:\n",
    "\n",
    "1. Parse the JSON in C++, using every trick in the book to make it fast.\n",
    "2. Store the result as a C++ data structure.  Provide you a Python interface that behaves--on the surface--like a dictionary.\n",
    "3. Don't convert anything from C++ to Python until you ask for it (i.e., until you index the dictionary-like thing that you get back).\n",
    "\n",
    "Or, in other words: Python, as a language, is quite slow.  C++, as a language, is quite fast.  `simdjson` makes sure C++ does as much of the work as possible, and that Python does as little as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f95d3a5e-5191-42d5-977f-4d29410db4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMDJSON library, loading json_data 1,000,000 times:  2.2729243000012502\n",
      "SIMDJSON library, serializing json_data 1,000,000 times:  4.769098700000541\n"
     ]
    }
   ],
   "source": [
    "# simdjson's drop-in API is not actually all that fast.\n",
    "import simdjson\n",
    "\n",
    "print(\n",
    "    \"SIMDJSON library, loading json_data 1,000,000 times: \",\n",
    "    timeit(\"simdjson.loads(json_data)\", globals=globals(), number=1_000_000)\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"SIMDJSON library, serializing json_data 1,000,000 times: \",\n",
    "    timeit(\"simdjson.dumps(json_dict)\", globals=globals(), number=1_000_000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b285e6ae-3762-41f2-adab-d1dc56523fe7",
   "metadata": {},
   "source": [
    "To show off the native parsing API, we have to do a little trick to make `timeit` work properly.  Basically, `timeit.timeit()` only lets you execute one statement at a time; we need to explicitly loop over our JSON object and keep re-parsing it with `simdjson`, so we'll just do that in a function, and call the function with `timeit.timeit()`.\n",
    "\n",
    "Note: we have to completely delete all the parsed data *before we can parse another JSON.*  This has to do with some of the C++ optimization; it's related to re-using the same chunk of memory, rather than constantly getting a new chunk to store each parsed JSON in.  The details aren't super important, but you'll get weird error messages if you don't do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "39dcba63-4b9e-4b21-bddb-0bceb54cdeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMDJSON library, loading json_data 1,000,000 times with the native parsing API:  0.5380136999992828\n"
     ]
    }
   ],
   "source": [
    "# simdjson's native parsing API\n",
    "def simdjson_demo():\n",
    "    parser = simdjson.Parser()\n",
    "    for i in range(1_000_000):\n",
    "        parsed = parser.parse(json_data)\n",
    "        del parsed\n",
    "    return\n",
    "\n",
    "print(\n",
    "    \"SIMDJSON library, loading json_data 1,000,000 times with the native parsing API: \",\n",
    "    timeit(\"simdjson_demo()\", globals=globals(), number=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41edc1d-8219-46cd-aa02-e77fa615c528",
   "metadata": {},
   "source": [
    "The speed difference is even bigger for larger and more complex JSONs--I've seen the native simdjson API speed up some of my code by nearly 35x when I was working with really, really big JSONs that I only needed a few fields out of.\n",
    "\n",
    "So: what JSON libraries should you use, and when?  Here's the good rule of thum:\n",
    "1. Start with the standard library's `json`.  It's very robust and very flexible.\n",
    "2. Use `ujson` if you don't need any of the extra bells and whistles of `json`, or need faster parsing/serialization with minimal code changes.\n",
    "3. Use `orjson` if fast serialization is an absolute must-have.\n",
    "4. Use `pysimdjson`'s native API if ultra-fast parsing is an absolute must-have; you'l need to do some code re-writing, though, to use the native API.\n",
    "\n",
    "I will often stick with `json` for simplicity's sake in my projects, but I have definitely had projects where I used `pysimdjson` for parsing, `json` for debugging when `pysimdjson` ran into problems, and `orjson` for serialization.  So don't be afraid to mix and match."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
