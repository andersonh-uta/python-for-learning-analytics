{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b069b68-14dd-4fba-99b2-9fd5191bc7dd",
   "metadata": {},
   "source": [
    "# PyTorch: More control, but more work\n",
    "\n",
    "After Tensorflow+Keras, PyTorch is hands-down the most widely used neural network library in Python.  It's generally more popular in academia, and it competes pretty well with Tensorflow in the private/corporate space too.  But, it has a very different approach to working with models.\n",
    "\n",
    "Keras hides away a lot of the details of  neural network from you.  This is not necessarily a bad thing.  It strikes a very good balance between abstraction and usability.  Keras doesn't try too hard to anticipate what you want to do with it, and it can very effectively cover most use cases for most people.  Tensorflow--which Keras is built on top of--has an extremely bumpy and weird learning curve.  Tensorflow is basically a different programming language, with a fundamentally different model of how code works, embedded into a Python library.  It can be very clunky to work with.  Writing Tensorflow code does not feel like writing Python.\n",
    "\n",
    "PyTorch is designed to be closer to Tensorflow than to Keras, but to do as much using native Python constructs as possible.  As a result, writing code with PyTorch feels like writing Python code, and you can use everything you've learned about Python so far.  The end result is that you're dealing with more of the nitty-gritty details about creating and training networks, but it's much easier than if you were using Tensorflow directly.  It's not always as easy as Keras, but it's not too far from Keras for simple models.  (And for more complex models, PyTorch is _way_ more flexible).\n",
    "\n",
    "There are a lot of other differences between the libraries, but honestly, most of them don't matter when you're getting started.  (E.g.: the difference between dynamic and static graphs; different tools for parallelism and deployment; integration with monitoring and logging; ease of running on embedded devices; and so on).  \n",
    "\n",
    "But maybe most importantly: PyTorch is _way_ less of a pain to install.  To install PyTorch, go to the PyTorch website and follow [the installation instructions.](https://pytorch.org/get-started/locally/)  Note: if you're on Linux, and have an AMD GPU, PyTorch has support for ROCm!  Windows/Mac users, you're not so lucky; you still need to have an NVidia GPU, or just run the code on your CPU.  If you have an NVidia GPU and don't know what version of CUDA you have/should use, just use the default selection.  The installation can take a while, so be warned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f658e24-7d3f-4444-a879-c4d7cfac3800",
   "metadata": {},
   "source": [
    "# PyTorch Quickstart\n",
    "\n",
    "One very important thing: PyTorch has a `Tensor` class that's core to the entire library.  The `Tensor` is basically a Numpy `array`, but it can be easily moved to a GPU for faster math operations.  So instead of using Numpy `arrays`, we'll use PyTorch `Tensor`s.  (Keras/Tensorflow also have `Tensor`s, but we didn't use them in the previous notebooks; the code would look almost identical if we had, though).\n",
    "\n",
    "Let's load our Diamonds data again and convert it into PyTorch Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41fadb4d-3875-4b86-8898-39ce7464b03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 27)\n",
      "(43941, 27)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "if os.path.isfile(\"diamonds.csv\"):\n",
    "    diamonds = pd.read_csv(\"diamonds.csv\")\n",
    "else:\n",
    "    diamonds = pd.read_csv(\"https://raw.githubusercontent.com/tidyverse/ggplot2/main/data-raw/diamonds.csv\")\n",
    "    diamonds.to_csv(\"diamonds.csv\", index=False)\n",
    "    \n",
    "diamonds = pd.get_dummies(\n",
    "    diamonds,\n",
    "    [\"color\", \"clarity\", \"cut\"],\n",
    ")\n",
    "\n",
    "# Train-test split; ~20% data for testing.\n",
    "diamonds = diamonds.sample(frac=1, replace=False).reset_index(drop=True)\n",
    "test = diamonds.loc[:9999]\n",
    "train = diamonds.loc[9999:]\n",
    "\n",
    "print(test.shape)\n",
    "print(train.shape)\n",
    "\n",
    "train_x = train.drop(columns=[\"price\"]).values\n",
    "test_x = test.drop(columns=[\"price\"]).values\n",
    "\n",
    "# .reshape(-1,1) --> this will avoid some pytorch warnings\n",
    "# later.\n",
    "train_y = train[\"price\"].values.reshape(-1,1)\n",
    "test_y = test[\"price\"].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92491a74-8336-454b-af49-bb1c301e63a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andersonh\\Miniconda3\\envs\\PFDA_Month_8\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7000, 61.1000, 56.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
      "        [ 0.3400, 63.7000, 57.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.4000, 63.4000, 59.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 1.5200, 62.6000, 60.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.5200, 62.3000, 55.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.5100, 61.5000, 57.0000,  ...,  1.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Convert to tensors\n",
    "import torch # imports as torch, not pytorch!\n",
    "\n",
    "train_x = torch.Tensor(train_x)\n",
    "train_y = torch.Tensor(train_y)\n",
    "test_x = torch.Tensor(test_x)\n",
    "test_y = torch.Tensor(test_y)\n",
    "\n",
    "print(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a4d07-db9f-4d91-bd57-ec0fd8bf735f",
   "metadata": {},
   "source": [
    "`torch.Tensor` objects default to float32 for better performance. but Pandas/Numpy default to float64.  There is some loss of numeric precision, but it really doesn't matter in almost any scenario.  PyTorch has its own datatypes which have all the same names as the ones in Numpy: `float32`, `float64`, `int16`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f843f4d-e9ec-49f4-96e8-4035c283aa5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(train_x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1eb552-46ca-4cf3-a8f9-1f3d7d0844bd",
   "metadata": {},
   "source": [
    "Now, let's build the same simple neural network as in the last two notebooks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f3196e7-5c43-462c-af22-18b537a07803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=26, out_features=128, bias=True)\n",
      "  (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=train_x.shape[1], out_features=128),\n",
    "    torch.nn.Linear(in_features=128, out_features=128),\n",
    "    torch.nn.Linear(in_features=128, out_features=128),\n",
    "    torch.nn.Linear(in_features=128, out_features=1)\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3259d5-66fb-4f55-8255-12bbe86ffa70",
   "metadata": {},
   "source": [
    "Note a few big differences between this and the Keras `Sequential` model:\n",
    "1. We pass multiple positional arguments to the `Sequential` contructor, not a list of layers\n",
    "2. We're not specifying the activation--PyTorch requires activation to be specified like layers (Keras allowsus to specify activation in the layer's constructor, or as a separate layer--we did it in the layer constructors).  Not specifying an activation = linear/identity activation.  If we wanted to use something like ReLU activation, we would add `torch.nn.ReLU()` after the layer we want to apply it to.\n",
    "3. We specify the number of features coming in, and going out, of the layers.  This is a bit of extra work but it does end up allowing us to be more flexible with our model (but only in fairly exotic use cases).\n",
    "\n",
    "Now let's train this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a710ee-01c6-4946-acbb-9e2a0a415714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loss function and optimizer object\n",
    "# These objects track a lot of state internally between\n",
    "# calls, so they're objects and not plain functions.\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(\n",
    "    # Parameters to track and update\n",
    "    model.parameters(), \n",
    "    \n",
    "    # Learning rate\n",
    "    lr=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195e6bd8-7c23-4b47-b476-09cdbeaa0e21",
   "metadata": {},
   "source": [
    "We can write the training loop ourselves pretty easily.  Remember that neural networks are trained by:\n",
    "1. Grab a few observations, at random, from the training dataset.\n",
    "2. Run them through the model to generate predictions.\n",
    "3. Check how good the predictions are.\n",
    "4. Update the model's parameters.\n",
    "\n",
    "We have to do a bit of work to to the batching and shuffling, but it's not too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eefd8ba-b7cb-4723-9328-c37a3074020b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - MSE Loss=3,384,875 - R2=0.7865\n",
      "Epoch 1 - MSE Loss=2,363,933 - R2=0.8509\n",
      "Epoch 2 - MSE Loss=2,232,256 - R2=0.8592\n",
      "Epoch 3 - MSE Loss=2,151,837 - R2=0.8643\n",
      "Epoch 4 - MSE Loss=2,031,320 - R2=0.8719\n",
      "Epoch 5 - MSE Loss=1,971,408 - R2=0.8757\n",
      "Epoch 6 - MSE Loss=1,842,296 - R2=0.8838\n",
      "Epoch 7 - MSE Loss=1,772,493 - R2=0.8882\n",
      "Epoch 8 - MSE Loss=1,645,327 - R2=0.8962\n",
      "Epoch 9 - MSE Loss=1,539,982 - R2=0.9029\n",
      "Epoch 10 - MSE Loss=1,477,258 - R2=0.9068\n",
      "Epoch 11 - MSE Loss=1,834,693 - R2=0.8843\n",
      "Epoch 12 - MSE Loss=1,348,638 - R2=0.9149\n",
      "Epoch 13 - MSE Loss=1,339,713 - R2=0.9155\n",
      "Epoch 14 - MSE Loss=1,313,609 - R2=0.9172\n",
      "Epoch 15 - MSE Loss=1,314,494 - R2=0.9171\n",
      "Epoch 16 - MSE Loss=1,298,895 - R2=0.9181\n",
      "Epoch 17 - MSE Loss=1,307,711 - R2=0.9175\n",
      "Epoch 18 - MSE Loss=1,601,847 - R2=0.8990\n",
      "Epoch 19 - MSE Loss=1,352,297 - R2=0.9147\n",
      "Epoch 20 - MSE Loss=1,323,062 - R2=0.9166\n",
      "Epoch 21 - MSE Loss=1,342,844 - R2=0.9153\n",
      "Epoch 22 - MSE Loss=1,378,596 - R2=0.9131\n",
      "Epoch 23 - MSE Loss=1,313,490 - R2=0.9172\n",
      "Epoch 24 - MSE Loss=1,300,179 - R2=0.9180\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def batches(x, y, batchsize=128):\n",
    "    # Generator that yields (x, y) pairs\n",
    "    indices = torch.randperm(x.shape[0])\n",
    "    for i in range(0, len(indices), batchsize):\n",
    "        idx = indices[i:i+batchsize]\n",
    "        yield x[idx], y[idx]\n",
    "        \n",
    "# Training loop\n",
    "for epoch in range(25):\n",
    "    for (x, y) in batches(\n",
    "        train_x,\n",
    "        train_y.reshape(-1,1),\n",
    "        128,\n",
    "    ):\n",
    "        \n",
    "        # Note that we use the model like a callable/function;\n",
    "        # we don't call a .predict() method on it.\n",
    "        preds = model(x)\n",
    "        \n",
    "        # Calculate the loss.\n",
    "        loss = loss_fn(preds, y)\n",
    "        \n",
    "        # The next three lines do the backpropagation step.\n",
    "        # Reset the optimizer's gradient information.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate the gradient of the compute graph\n",
    "        # based on the loss function.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the optimizers to take a step down the gradient\n",
    "        # of all free parameters in the compute graph, i.e.,\n",
    "        # to update the graph's paramters.\n",
    "        optimizer.step()\n",
    "        \n",
    "    # print the loss after each epoch.\n",
    "    # torch.no_grad() --> context manager that prevents\n",
    "    # pytorch from calculating gradients.  We don't need\n",
    "    # them here anyways.\n",
    "    with torch.no_grad():\n",
    "        test_preds = model(test_x)\n",
    "        loss = loss_fn(test_preds, test_y)\n",
    "        r2 = r2_score(test_y, test_preds)\n",
    "        print(f\"Epoch {epoch:5<} - MSE Loss={int(loss):,} - R2={r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de93f03-1cb9-4dd1-80ef-9b2a33ca3960",
   "metadata": {},
   "source": [
    "There's a lot of \"magic\" happening in the above code.  PyTorch, like Keras, is constructing a _compute graph_ by adding connections every time we do anything that logically connects two objects.  So:\n",
    "- Specifying `model.parameters()` when creating our optimizer creates a link between the model's parameters and the optimizer.  PyTorch will remember this link.\n",
    "- Calcuating the loss function creates a connection between the loss function, the true values, and the model's predictions.\n",
    "- The model creates connections between each of its layers and their parameters.\n",
    "\n",
    "So, when we call `optimizer.step()`, PyTorch is looking at all the things the optimizer has ben connected to (directly and indirectly), and doing the update step for all of them.\n",
    "\n",
    "Some people find all this implicit \"magic\" to be a bad thing; I tend to agree more than I disagree.  It's not too bad when you have a good handle on the underlying steps and what they're actually doing to the network, but one of the criticisms of PyTorch is that it strikes a kind of werid balance between how much you have to do by hand, and how much PyTorch does for you, behind the scenes.  Fortunately, the code is usually pretty simple, and very well documented, so this ends up being mostly a philosophical point rather than a practical one.\n",
    "\n",
    "Note that we aren't doing any sort of early stopping.  We could do that, but we would have to implement it ourselves.  It's not actually that hard: just store the last few values of whatever we're tracking, then check if the next value we get is less than any of them.  Something like:\n",
    "\n",
    "```python\n",
    "losses = []\n",
    "for epoch in range(25):\n",
    "    for (x, y) in batches(...):\n",
    "        # do model stuff\n",
    "    loss = loss_fn(...)\n",
    "    \n",
    "    # stop if no improvement for 10 epochs\n",
    "    if any(loss < i for i in losses[-10:]):\n",
    "        losses.append(loss)\n",
    "    else:\n",
    "        break\n",
    "```\n",
    "\n",
    "One dirty secret here: we've been running this model on the CPU.  To move it--and our data--to the GPU, we can do two things:\n",
    "\n",
    "1. Use the `.to()` method to move tensors and models from the CPU to the GPU (or vice versa).\n",
    "2. Set the `device=` keyword argument when creating tensors/models/layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6db43847-0bb7-410f-b3ca-dd3481df1370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "cuda\n",
      "Created on the GPU: tensor([1, 2, 3, 4, 5], device='cuda:0')\n",
      "Device: cuda:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the list of device\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available :(\")\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Create the tensor directly on the device\n",
    "gpu_tensor = torch.tensor([1,2,3,4,5], device=device)\n",
    "print(\"Created on the GPU:\", gpu_tensor)\n",
    "print(\"Device:\", gpu_tensor.device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83934a2f-18e6-4f45-a48b-8bc1ce6ee038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created on the CPU: tensor([1, 2, 3, 4, 5])\n",
      "Device: cpu\n",
      "Moved to GPU: tensor([1, 2, 3, 4, 5], device='cuda:0')\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Move a tensor to the GPU after creating it on the CPU\n",
    "gpu_tensor = torch.tensor([1,2,3,4,5])\n",
    "print(\"Created on the CPU:\", gpu_tensor)\n",
    "print(\"Device:\", gpu_tensor.device)\n",
    "gpu_tensor = gpu_tensor.to(\"cuda\")\n",
    "print(\"Moved to GPU:\", gpu_tensor)\n",
    "print(\"Device:\", gpu_tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a534d28-218c-4653-9567-0d93b250117c",
   "metadata": {},
   "source": [
    "PyTorch doesn't have a way to default to constructing tensors/models on the GPU, which according to them is because this can be a pretty expensive and slow operation; it's better to move thing over to the GPU when they need to be moved, not sooner.  (Also, your GPU probably has less RAM than your main system, so you can store more stuff in the CPU/main system memory anyways).\n",
    "\n",
    "Let's re-run the training loop, but put everything on the GPU.  One issue we'll run into: when `torch.Tensor`s are on the GPU, we can't plug them in to scikit-learn's metric functions, because PyTorch doesn't automatically convert GPU tensor to Numpy `array`s (which can only live in CPU memory).  This is annoying, but it's done for a reason--moving data between the GPU and CPU can cause all sorts of slowdowns and other issues, so PyTorch wants you to do that explicitly, so it doesn't have to worry about it.\n",
    "\n",
    "Fortunately, it's very easy to define our own function to calculate the $R^2$ metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e902c002-df1c-4cbe-8d6a-660500bf9cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - MSE Loss=3,902,924 - R2=0.6935\n",
      "Epoch 1 - MSE Loss=2,431,993 - R2=0.8153\n",
      "Epoch 2 - MSE Loss=2,469,485 - R2=0.8243\n",
      "Epoch 3 - MSE Loss=2,127,600 - R2=0.8433\n",
      "Epoch 4 - MSE Loss=2,106,534 - R2=0.8429\n",
      "Epoch 5 - MSE Loss=1,980,015 - R2=0.8559\n",
      "Epoch 6 - MSE Loss=2,270,354 - R2=0.8138\n",
      "Epoch 7 - MSE Loss=1,854,507 - R2=0.8624\n",
      "Epoch 8 - MSE Loss=1,791,943 - R2=0.8624\n",
      "Epoch 9 - MSE Loss=1,714,606 - R2=0.8674\n",
      "Epoch 10 - MSE Loss=1,611,747 - R2=0.8817\n",
      "Epoch 11 - MSE Loss=1,593,832 - R2=0.8848\n",
      "Epoch 12 - MSE Loss=1,510,548 - R2=0.8927\n",
      "Epoch 13 - MSE Loss=1,446,722 - R2=0.9044\n",
      "Epoch 14 - MSE Loss=1,396,417 - R2=0.8997\n",
      "Epoch 15 - MSE Loss=1,421,135 - R2=0.8983\n",
      "Epoch 16 - MSE Loss=1,331,716 - R2=0.9150\n",
      "Epoch 17 - MSE Loss=1,321,524 - R2=0.9084\n",
      "Epoch 18 - MSE Loss=1,630,032 - R2=0.8870\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def pytorch_r2(pred, true):\n",
    "    \"\"\"Compute the r-squared metric.\"\"\"\n",
    "    ss_res = ((true - pred) ** 2).sum()\n",
    "    ss_tot = ((true - true.mean()) ** 2).sum()\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Everything needs to be on the GPU!\n",
    "# We can also use .cuda() and .cpu() in place of .to();\n",
    "# this is just a shortcut for .to(\"cuda\") and .to(\"cpu\"),\n",
    "# respectively.\n",
    "# NOTE: using any of these methods on a Sequential model\n",
    "# will update it *in-place*, but for individual Tensors,\n",
    "# it *returns a copy* of the moved Tensor.\n",
    "gpu_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=train_x.shape[1], out_features=128),\n",
    "    torch.nn.Linear(in_features=128, out_features=128),\n",
    "    torch.nn.Linear(in_features=128, out_features=128),\n",
    "    torch.nn.Linear(in_features=128, out_features=1)\n",
    ").cuda()\n",
    "\n",
    "# Re-create the optimizer and make sure it points at the GPU model's\n",
    "# parameters.\n",
    "gpu_optimizer = torch.optim.Adam(\n",
    "    gpu_model.parameters(), \n",
    "    lr=1e-3,\n",
    ")\n",
    "\n",
    "# Move our data to the GPU\n",
    "train_x_gpu = train_x.cuda()\n",
    "train_y_gpu = train_y.cuda()\n",
    "test_x_gpu = test_x.cuda()\n",
    "test_y_gpu = test_y.cuda()\n",
    "\n",
    "# Training loop--this time with early stopping\n",
    "losses = []\n",
    "for epoch in range(500):\n",
    "    for (x, y) in batches(train_x_gpu, train_y_gpu, 128):\n",
    "        preds = gpu_model(x)\n",
    "        loss = loss_fn(preds, y)\n",
    "        gpu_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        gpu_optimizer.step()\n",
    "        \n",
    "    # print the loss after each epoch.\n",
    "    # Notice the hoops we're jumping through with device management.\n",
    "    # There might be a better way to d this, but I've not spent enough\n",
    "    # time with PyTorch to know how.\n",
    "    with torch.no_grad():\n",
    "        test_preds = gpu_model(test_x_gpu)\n",
    "        loss = loss_fn(test_preds, test_y_gpu)\n",
    "        r2 = pytorch_r2(test_y_gpu, test_preds)\n",
    "        print(f\"Epoch {epoch:5<} - MSE Loss={int(loss):7<,} - R2={r2:.4f}\")\n",
    "        \n",
    "        # Early stopping: validation loss must improve, by any margin,\n",
    "        # within 5 epochs.\n",
    "        if (\n",
    "            len(losses) < 5\n",
    "            or any(loss < i for i in losses[-5:])\n",
    "        ):\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e786bc0d-42a2-417f-973f-df14862aed95",
   "metadata": {},
   "source": [
    "That's a basic sketch of a PyTorch model.  Building the model and the loss function and optimizer looks pretty similar to Keras, but note the following major differences:\n",
    "\n",
    "1. Keras uses a scikit-learn-style API with `.fit()` and `.predict()`.  PyTorch asks you do do manually manage the training loop--it's not much code for a basic training loop, and it gives you a lot more control, but you have to do more work.\n",
    "2. Keras transparently handles devices (CPU vs GPU).  PyTorch asks you to do this yourself.\n",
    "3. PyTorch does a lot of stuff implicitly: calling `loss.backward()` calculates gradients for _every tensor that was used at any point in the calculation of `loss`_, and sets the gradients as attributes on those tensors. Similarly, `optimizer.step()` steps through every tensor that you gave the optimizer when you initialized it, checks the gradients, and updates them in-place.  There's a lot of back magic going on here.  Keras doesn't expose any of this; you just call `.fit()` and Keras handles all the other details.\n",
    "4. In general, Keras ia _a neural network library._  PyTorch, though, is a GPU math library with automatic gradients/differentiation. You have to implement the neural network bits yourself, but it's very easy to see what needs to be done.\n",
    "\n",
    "Both libaries are excellent.  Which one you want to use is really just a matter of personal preference.  I personally prefer PyTorch--it's where most people are flocking these days, since it's much easier to create complex, exotic networks using it rather than Keras/Tensorflow.  PyTorch is also a lot easier to install.  But Keras has a lot more quality-of-life features that are immediately and easily accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d032a5b-d6a1-4c22-85c4-cfdaa7778462",
   "metadata": {},
   "source": [
    "# Stuff not covered\n",
    "\n",
    "I was originally planning to cover how to build more complex models using PyTorch's built-in classes and subclassing them.  That ended up getting a bit too messy, so I've cut it for time.  But, see the notebook on Python `class`es for a quickstart on the Python language tools for doing that.\n",
    "\n",
    "PyTorch also integrates with TensorBoard, as well as its own in-house tools for monitoring and visualizing models, but I haven't shown how to do that (because I haven't looked at how to do it).\n",
    "\n",
    "There are also a _lot_ of other neural network libraries.  JAX (coupled with FLAX or Haiku) is currently the biggest up-and-comer.  JAX is pretty interesting from a technical perspective, but I haven't spent enough time with it to say much more than that.  Picking a neural network library can be hard, and requires comparing a lot of apples to a lot of oranges--and almost all of that stuff only matters if you're thinking of deploying models in a production environment, rather than using them for research projects or experimentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
