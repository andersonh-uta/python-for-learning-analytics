{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cebd29ac-be34-4cb4-94a3-31936785986f",
   "metadata": {},
   "source": [
    "# Keras: the Sequential API\n",
    "\n",
    "[Keras](https://keras.io/) is the easiest library to use for building and using neural networks.  It has two major ways to build models:\n",
    "1. The Sequential API (in this notebook), which looks and feels a lot like a `sklearn.pipeline.Pipeline()`.\n",
    "2. The functional API (in the next notebook), which looks and feels very different, but which allows a lot more flexibility.\n",
    "\n",
    "Keras once existed as a standalone library, but it's been incorporated into [Tensorflow](https://www.tensorflow.org/).  We will not be covering Tensorflow, but it's one of the biggest libaries out there for doing neural networks and general GPU-accelerated matrix operations.  (But it's also a pretty clunky library).  A few years back, Tensorflow absorbed Keras, and the Keras API is now available as a Tensorflow module.\n",
    "\n",
    "## A note on GPUs\n",
    "\n",
    "Welcome to a world of confusion, frustration, and borderline broken installers.\n",
    "\n",
    "Graphics Provessing Units (GPUs; also called \"video cards\") are a perfect tool for speeding up matrix multiplication--and neural networks are mostly matrix multiplication.  I'm not going to address the details here, but if you can make a GPU (rather than CPU) run matrix multiplication, it can often run it _much_ faster.  At least for large matrices.  But, not everyone has a GPU, and not all GPU manufacturers use the same tools to run stuff on their GPUs.  E.g.: NVidia uses CUDA to write GPU code; AMD uses ROCm; I forget what Intel calls theirs, but they have their own too.  Almost every library for neural networks supports CUDA, but more are supporting ROCm, and Intel's support is still pretty far behind (as are their dedicated GPUs).\n",
    "\n",
    "So if you want to use your GPU to run code faster--and for neural networks, we're talking 10x faster or more--you need to go through the hassle of installing the right driver and software toolchains and such for your card.  That can be very non-trivial to do.  It's getting better, but it's still a bit of a mess.  (Mostly because some libaries pin their dependency requirements to old versions of e.g. CUDA, which causes all sorts of conflicts when trying to do an actual project).\n",
    "\n",
    "Installing Tensorflow specifically is an exercize in pain, at least on Windows.  It can't be installed with conda if you want GPU support, because Google seemingly hasn't bothered.  It has to be installed through `pip` (the built-in package manager for Python).  It doesn't install all the dependencies it needs.  It doesn't always see your GPU.  It just generally doesn't always work.  So why are we using it?  Simple: if you can just get it to install, Keras is the easiest way to build neural networks.\n",
    "\n",
    "All the code in this notebook can be run without a GPU.  But it will be slower.  Not so slow it can't be run--we're not using very large networks--but for any non-trivial neural network, a GPU can literally be the difference between minutes and days of runtime.\n",
    "\n",
    "# Installation\n",
    "\n",
    "If you have an NVidia GPU, get ready for a few more commands than normal.  At least on Windows (whish I'm running this notebook from), we have to install `cudatoolkit` and `cudnn` to use GPU acceleration, then we have to use `pip`--not `conda`--to install tensorflow with GPU support. For whatever reason, the Tensorflow devs just don't bother to maintain the `conda` version of tensorflow very well (if at all).\n",
    "\n",
    "```bash\n",
    "conda install cudatoolkit cudnn\n",
    "python -m pip install -U pip\n",
    "python -m pip install tensorflow-gpu\n",
    "```\n",
    "\n",
    "This should install the correct versions of the CUDA tools.\n",
    "\n",
    "If you have no GPU, or you run into issues with the GPU installation:\n",
    "```bash\n",
    "conda install tensorflow\n",
    "```\n",
    "\n",
    "If you have an AMD GPU: Tensorflow does seem to support AMD's ROCm (which is their competitor to NVidia's CUDA), but I don't know how stable it is or how to install it.\n",
    "\n",
    "If you don't have a GPU at all, you're going to be stuck running code on CPU, which also means you'll be stuck running very simple models.  (fortunately the models in these notebooks are simple enough that you should have no real problems, other than the models running a bit slower).\n",
    "\n",
    "Then test your installation.  If you see a non-empty list when you run the command below, Tensorflow sees your GPU and will use it.  Otherwise, if you see an empty list (`[]`), it'll run on CPU.  The networks we're going to use in this notebook are small enough that they can run on CPU, but it'll just take a bit longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cce81165-0063-4763-bac2-54ab38e9e56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15322844-e944-4d5d-aa47-66441ff45ffb",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "Most of the sequential API looks pretty similar no matter what kind of model you're building; just swap out some layers or parameters here and there.  The below cell shows a minimal working example of a model doing some simple regression on the [`diamonds`](https://www.tensorflow.org/datasets/catalog/diamonds) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af342600-2066-4240-9911-e244de0d9949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "\n",
    "if os.path.isfile(\"diamonds.csv\"):\n",
    "    diamonds = pd.read_csv(\"diamonds.csv\")\n",
    "else:\n",
    "    diamonds = pd.read_csv(\"https://raw.githubusercontent.com/tidyverse/ggplot2/main/data-raw/diamonds.csv\")\n",
    "    diamonds.to_csv(\"diamonds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0588970-81fb-49a2-8e19-aef8e98cbc99",
   "metadata": {},
   "source": [
    "We want to predict the `price` value for each diamond.  Let's do a bit of quick re-shaping--mostly, we want to one-hot encode the `cut`, `color`, and `clarity` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "516a5962-c89b-4f5d-93bb-c51abc3e4e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 27)\n",
      "(43941, 27)\n"
     ]
    }
   ],
   "source": [
    "diamonds = pd.get_dummies(\n",
    "    diamonds,\n",
    "    [\"color\", \"clarity\", \"cut\"],\n",
    ")\n",
    "\n",
    "# Train-test split; ~20% data for testing.\n",
    "diamonds = diamonds.sample(frac=1, replace=False).reset_index(drop=True)\n",
    "test = diamonds.loc[:9999]\n",
    "train = diamonds.loc[9999:]\n",
    "\n",
    "print(test.shape)\n",
    "print(train.shape)\n",
    "\n",
    "train_x = train.drop(columns=[\"price\"]).values\n",
    "test_x = test.drop(columns=[\"price\"]).values\n",
    "train_y = train[\"price\"].values\n",
    "test_y = test[\"price\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc340e6-8e9a-4fa5-a09b-d41afc287e93",
   "metadata": {},
   "source": [
    "Now, let's build a basic neural network.  We'll do three hidden layers of a moderate size, and configure it to use the squared error loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1eaf365-666d-47c9-82d2-2a8a3e4870b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "155/155 [==============================] - 2s 6ms/step - loss: 21580640.0000 - val_loss: 15207453.0000\n",
      "Epoch 2/25\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 14955958.0000 - val_loss: 14621845.0000\n",
      "Epoch 3/25\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 14038016.0000 - val_loss: 13143774.0000\n",
      "Epoch 4/25\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 10088071.0000 - val_loss: 5330798.0000\n",
      "Epoch 5/25\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 3851811.7500 - val_loss: 3167043.5000\n",
      "Epoch 6/25\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 3026217.2500 - val_loss: 2722680.7500\n",
      "Epoch 7/25\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 2713167.7500 - val_loss: 2488597.0000\n",
      "Epoch 8/25\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 2516031.5000 - val_loss: 2342604.2500\n",
      "Epoch 9/25\n",
      "155/155 [==============================] - 1s 7ms/step - loss: 2409730.5000 - val_loss: 2260182.7500\n",
      "Epoch 10/25\n",
      "155/155 [==============================] - 1s 7ms/step - loss: 2319204.2500 - val_loss: 2187137.2500\n",
      "Epoch 11/25\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 2267360.7500 - val_loss: 2142234.0000\n",
      "Epoch 12/25\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 2230701.7500 - val_loss: 2114693.0000\n",
      "Epoch 13/25\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 2197402.7500 - val_loss: 2088230.7500\n",
      "Epoch 14/25\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 2167428.5000 - val_loss: 2054046.6250\n",
      "Epoch 15/25\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 2142355.7500 - val_loss: 2031361.1250\n",
      "Epoch 16/25\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 2119057.5000 - val_loss: 2025184.5000\n",
      "Epoch 17/25\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 2085979.5000 - val_loss: 2013362.7500\n",
      "Epoch 18/25\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 2064093.1250 - val_loss: 1973284.6250\n",
      "Epoch 19/25\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 2036193.8750 - val_loss: 1946107.3750\n",
      "Epoch 20/25\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 2012281.7500 - val_loss: 1927766.2500\n",
      "Epoch 21/25\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 1985872.0000 - val_loss: 1938582.6250\n",
      "Epoch 22/25\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 1961271.8750 - val_loss: 1890130.1250\n",
      "Epoch 23/25\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 1933771.0000 - val_loss: 1858001.1250\n",
      "Epoch 24/25\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 1904371.7500 - val_loss: 1839158.2500\n",
      "Epoch 25/25\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 1882794.0000 - val_loss: 1819870.0000\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "CPU times: total: 33.3 s\n",
      "Wall time: 25.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Define a simple model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(32),\n",
    "    keras.layers.Dense(32),\n",
    "    keras.layers.Dense(32),\n",
    "    keras.layers.Dense(1), # linear = no activation = identity\n",
    "])\n",
    "\n",
    "# Compile the model--this gets it ready to run and does some behind-the-scenes\n",
    "# optimizations.\n",
    "model.compile(\n",
    "    # ADAM is a very standard optimizer; it's a solid go-to for mos problems.\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    \n",
    "    # squared error loss --> this network is analogous to least squared regression\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    \n",
    "    # Metrics to monitor during training--these will be printed out as the model\n",
    "    # trains.  The loss always gets printed out, so we're not going to specify\n",
    "    # this right now.\n",
    "    # metrics=[keras.metrics.MeanSquaredError()],\n",
    "    \n",
    "    # install the XLA library with conda and uncomment this line for extra speed,\n",
    "    # both on CPU and on GPU.\n",
    "    # jit_compile=True,\n",
    ")\n",
    "\n",
    "# Fit the model.\n",
    "# model.fit() updates the model in-place and returns some data\n",
    "# about the fit history, which can sometimes be useful.\n",
    "fit_history = model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    \n",
    "    # how many samples to use at once.  Higher --> more GPU VRAM used, model\n",
    "    # iterates faster, but the model might actually converge slower.\n",
    "    batch_size=256,\n",
    "    \n",
    "    # how many passes to do over the data.  This is a pretty low value just\n",
    "    # for demonstration purposes.\n",
    "    epochs=25,\n",
    "    \n",
    "    # set aside this fraction of the training data for \"validation\"--used to\n",
    "    # monitor the progress of the model against a held-out dataset, kind of like\n",
    "    # cross-validation.\n",
    "    validation_split=0.1,\n",
    "    \n",
    "    # Callbacks = things to run after each batch.  There are a lot of options\n",
    "    # for these.\n",
    "    callbacks=[\n",
    "        # this callback can end the training process early if some score/monitored\n",
    "        # quantity reaches some criterion, e.g. changes only very little.\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            # Monitor the validation split loss...\n",
    "            monitor=\"val_loss\",\n",
    "            \n",
    "            # ...and stop training when the loss hasn't decreaed by `min_delta`\n",
    "            # for `patience` epochs.\n",
    "            min_delta=0,\n",
    "            patience=5,\n",
    "        ),\n",
    "    ],\n",
    "    \n",
    "    # Print messages as training happens.\n",
    "    #  0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "    # \"auto\" = usuaully use 1.\n",
    "    verbose=\"auto\",\n",
    ")\n",
    "\n",
    "predictions = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bca92a-f575-4e45-9d9d-9222cb3fb54a",
   "metadata": {},
   "source": [
    "Stripping away a lot of the comments and filler, here's the basic outline of a Keras model:\n",
    "\n",
    "```python\n",
    "model = keras.Sequential([layers])\n",
    "model.compile(options)\n",
    "fit_history = model.fit(x, y, other_options)\n",
    "predictions = model.predict(new_x)\n",
    "```\n",
    "\n",
    "We can also print out a summary of our model, which will show the layers, how many \"neurons\" they have (a \"neuron\" is just an entry in the vector that comes out of each layer), and a few other things.  In order to get the summary, though, the model has to know how many features are in the input observations.  Keras will figure this out automatically when we call `.fit()`, but we can also call `.build()` and pass it the shape of each observation.  In our case, that's just the shape of one row in our dataset.  It would look like this:\n",
    "\n",
    "```python\n",
    "model = keras.Sequential([...])\n",
    "model.build(diamonds.shape[1])\n",
    "print(model.summary())\n",
    "```\n",
    "\n",
    "_Or,_ we can tell the `Sequential()` model what the input shape is when we construct it, by giving it a `keras.layers.Input(diamonds.shape[1])` as the very first layer.\n",
    "\n",
    "Printing out the model summary can be a nice way to get a feel for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c02674-06a8-408c-9d80-92ef0bdd8798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                864       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,009\n",
      "Trainable params: 3,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909e4720-b683-45d5-93bc-8db443c23d98",
   "metadata": {},
   "source": [
    "Anyways, let's see how well our model does on the test set, and just for kicks, let's compare it to a simple linear regression from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b49f40-74ee-4bfb-ad5f-38e365caccda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 105 ms\n",
      "Neural network R2 score:     0.8715\n",
      "Neural network MSE score:    2,076,119\n",
      "Linear Regression R2 score:  0.9197\n",
      "Linear Regression MSE score: 1,296,530\n"
     ]
    }
   ],
   "source": [
    "# Predict on our test set and print out R2 score.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "%time lr_preds = LinearRegression().fit(train_x, train_y).predict(test_x)\n",
    "\n",
    "print(f\"Neural network R2 score:     {r2_score(test_y, predictions):.4f}\")\n",
    "print(f\"Neural network MSE score:    {mean_squared_error(test_y, predictions):,.0f}\")\n",
    "print(f\"Linear Regression R2 score:  {r2_score(test_y, lr_preds):.4f}\")\n",
    "print(f\"Linear Regression MSE score: {mean_squared_error(test_y, lr_preds):,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14522b58-8f25-4cd9-80ad-fca9a66301ce",
   "metadata": {},
   "source": [
    "Oof.  That's...not looking too great for neural networks.  It was slow, it required a lot of finnicky installation of libaries, and it did worse!\n",
    "\n",
    "This is not the end of the story, though.  If our linear regression had done poorly, we might be out of luck; we could try doing some feature engineering, but we can only do so much, because the model itself is very simple.  A neural network, on the other hand, has a _lot_ more parameters we can tweak.  We could do this through cross-validation, but I'm going to skip all that and just show you a model that more or less matches our simple linear regression for performance.\n",
    "\n",
    "Here are the changes I made:\n",
    "- I changed each layer from ReLU to linear/identity activation.\n",
    "- I made the layers bigger.\n",
    "- I let the model train for more epochs.\n",
    "- I tweaked the learning rate for the ADAM optimizer.\n",
    "- I decreased the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46a6a6ba-459a-4dca-bf6d-fd0c9b1b2d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "618/618 [==============================] - 5s 6ms/step - loss: 8422584.0000 - val_loss: 2387401.0000\n",
      "Epoch 2/500\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 2324273.5000 - val_loss: 2185639.2500\n",
      "Epoch 3/500\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 2139218.0000 - val_loss: 1962681.1250\n",
      "Epoch 4/500\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 2032621.5000 - val_loss: 1962331.3750\n",
      "Epoch 5/500\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 1920030.8750 - val_loss: 1767171.1250\n",
      "Epoch 6/500\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 1802029.1250 - val_loss: 1799477.0000\n",
      "Epoch 7/500\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 1692313.5000 - val_loss: 1665633.5000\n",
      "Epoch 8/500\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 1571214.7500 - val_loss: 1457258.5000\n",
      "Epoch 9/500\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 1496149.7500 - val_loss: 1382408.5000\n",
      "Epoch 10/500\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 1425850.0000 - val_loss: 1334220.6250\n",
      "Epoch 11/500\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 1390936.6250 - val_loss: 1475327.3750\n",
      "Epoch 12/500\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 1384405.3750 - val_loss: 1380188.5000\n",
      "Epoch 13/500\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 1361093.5000 - val_loss: 1386871.8750\n",
      "Epoch 14/500\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 1352567.5000 - val_loss: 1364543.3750\n",
      "Epoch 15/500\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 1372294.3750 - val_loss: 1294532.5000\n",
      "Epoch 16/500\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 1354052.1250 - val_loss: 1284338.2500\n",
      "Epoch 17/500\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 1362390.1250 - val_loss: 1308920.7500\n",
      "Epoch 18/500\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 1366378.2500 - val_loss: 1325939.6250\n",
      "Epoch 19/500\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 1366808.2500 - val_loss: 1618665.2500\n",
      "Epoch 20/500\n",
      "618/618 [==============================] - 3s 6ms/step - loss: 1350527.8750 - val_loss: 1363575.3750\n",
      "Epoch 21/500\n",
      "618/618 [==============================] - 4s 6ms/step - loss: 1352145.6250 - val_loss: 1341593.6250\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "Better model score: 0.9164571581718427\n"
     ]
    }
   ],
   "source": [
    "# Define a simple model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation=\"linear\"),\n",
    "    keras.layers.Dense(128, activation=\"linear\"),\n",
    "    keras.layers.Dense(128, activation=\"linear\"),\n",
    "    keras.layers.Dense(1, activation=\"linear\"),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    ")\n",
    "\n",
    "fit_history = model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    batch_size=64,\n",
    "    epochs=500,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=0,\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "        ),\n",
    "    ],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "predictions = model.predict(test_x)\n",
    "print(f\"Better model score: {r2_score(test_y, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7613b98-06e9-4eef-9c6c-35ab26a2c576",
   "metadata": {},
   "source": [
    "This example, admittedly, is not a great one for neural networks; when a simple linear regression can get an $R^2$ above 0.9 with basically no work, there's no reason to use a big complex neural network.  However, for some kinds of problems, the neural network is the better solution:\n",
    "- Very sparse data, like text.\n",
    "- Classification tasks.\n",
    "- Anything where you need to optimize some more specialized quantity than the standard regression metrics.\n",
    "- _Extremely_ large datasets.\n",
    "\n",
    "It's not unheard of, in some cases, to end up with enormous networks, too.  If you network is doing poorly, you can always scale it up by adding way more layers, and making existing layers bigger.  Let's see another example, this time using the 20 Newsgroups dataset from sckit-learn.  We'll treat this like a 20-class classification problem: given the words of a post, identify the newsgroup it was posted to.  Our contenders:\n",
    "- Another neural network!\n",
    "- Naive Bayes\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "badb3610-80b4-42be-9f86-c10690bf46c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16961, 15000) (1885, 15000)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "x, y = fetch_20newsgroups(subset=\"all\", return_X_y=True)\n",
    "x = CountVectorizer(\n",
    "    min_df=5,\n",
    "    max_df=0.5,\n",
    "    max_features=15_000,\n",
    ").fit_transform(x)\n",
    "\n",
    "# we'll need this later for the neural network--its\n",
    "# target variables need to be structured differently.\n",
    "y_onehot = OneHotEncoder(sparse=False).fit_transform(y.reshape(-1,1))\n",
    "\n",
    "train_x, test_x, train_y, test_y, train_y_onehot, test_y_onehot = train_test_split(\n",
    "    x, y, y_onehot,\n",
    "    train_size=0.9, stratify=y, random_state=0\n",
    ")\n",
    "\n",
    "print(train_x.shape, test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8d1602a-2679-4dfe-b7bd-708a62dc35f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Complement Naive Bayes: 0.8531\n",
      "CPU times: total: 391 ms\n",
      "Wall time: 3.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import ComplementNB, GaussianNB\n",
    "\n",
    "nb = GridSearchCV(\n",
    "    ComplementNB(),\n",
    "    param_grid={\"alpha\": [10**i for i in range(-5, 0)] + [0]},\n",
    "    scoring=make_scorer(accuracy_score),\n",
    "    error_score=0,\n",
    "    cv=5,\n",
    "    n_jobs=4,\n",
    "    verbose=1,\n",
    ")\n",
    "# ComplementNB requires all positive values to be passed.\n",
    "minval = min(train_x.min(), test_x.min())\n",
    "nb.fit(train_x - minval, train_y)\n",
    "print(f\"Complement Naive Bayes: {accuracy_score(test_y, nb.predict(test_x - minval)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13306aa7-9f86-432c-8177-3532e807691a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: 0.8467\n",
      "CPU times: total: 1min 15s\n",
      "Wall time: 19.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Random forest--default settings are usually pretty good.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_jobs=4).fit(train_x, train_y)\n",
    "print(f\"Random Forest: {accuracy_score(test_y, rf.predict(test_x)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3e1f30-ffc4-4aaf-947e-bc925943c901",
   "metadata": {},
   "source": [
    "Hm, usually a random forest will outperform something like a Naive Bayes model.  We can just throw a bunch more estimator into the random forest (the default is 100).  More estimators never hurts the accuracy of a random forest; it just starts to taper off after a bit, and it of course runs slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "688f328d-b6d5-4e28-ac11-debf41be4a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: 0.8546\n",
      "CPU times: total: 10min 52s\n",
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Random forest--default settings are usually pretty good.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=1000, n_jobs=4).fit(train_x, train_y)\n",
    "print(f\"Random Forest: {accuracy_score(test_y, rf.predict(test_x)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4234f0-9000-4a04-81e4-5b75e119b4bb",
   "metadata": {},
   "source": [
    "That's better!  Now let's throw a pretty simple neural network at the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ae5de31-f228-45f8-bfbd-555ba1349163",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "477/477 - 3s - loss: 5.2589 - categorical_accuracy: 0.3756 - val_loss: 2.3984 - val_categorical_accuracy: 0.7166 - 3s/epoch - 7ms/step\n",
      "Epoch 2/500\n",
      "477/477 - 3s - loss: 2.8483 - categorical_accuracy: 0.8157 - val_loss: 1.5638 - val_categorical_accuracy: 0.8645 - 3s/epoch - 5ms/step\n",
      "Epoch 3/500\n",
      "477/477 - 3s - loss: 1.6054 - categorical_accuracy: 0.9195 - val_loss: 1.1795 - val_categorical_accuracy: 0.9010 - 3s/epoch - 6ms/step\n",
      "Epoch 4/500\n",
      "477/477 - 3s - loss: 1.0180 - categorical_accuracy: 0.9567 - val_loss: 0.9729 - val_categorical_accuracy: 0.9151 - 3s/epoch - 6ms/step\n",
      "Epoch 5/500\n",
      "477/477 - 3s - loss: 0.7170 - categorical_accuracy: 0.9756 - val_loss: 0.8522 - val_categorical_accuracy: 0.9187 - 3s/epoch - 6ms/step\n",
      "Epoch 6/500\n",
      "477/477 - 3s - loss: 0.5449 - categorical_accuracy: 0.9863 - val_loss: 0.7661 - val_categorical_accuracy: 0.9210 - 3s/epoch - 6ms/step\n",
      "Epoch 7/500\n",
      "477/477 - 3s - loss: 0.4422 - categorical_accuracy: 0.9920 - val_loss: 0.7073 - val_categorical_accuracy: 0.9281 - 3s/epoch - 6ms/step\n",
      "Epoch 8/500\n",
      "477/477 - 3s - loss: 0.3717 - categorical_accuracy: 0.9952 - val_loss: 0.6602 - val_categorical_accuracy: 0.9252 - 3s/epoch - 7ms/step\n",
      "Epoch 9/500\n",
      "477/477 - 3s - loss: 0.3210 - categorical_accuracy: 0.9967 - val_loss: 0.6233 - val_categorical_accuracy: 0.9258 - 3s/epoch - 7ms/step\n",
      "Epoch 10/500\n",
      "477/477 - 3s - loss: 0.2801 - categorical_accuracy: 0.9971 - val_loss: 0.5993 - val_categorical_accuracy: 0.9269 - 3s/epoch - 7ms/step\n",
      "Epoch 11/500\n",
      "477/477 - 3s - loss: 0.2512 - categorical_accuracy: 0.9977 - val_loss: 0.5710 - val_categorical_accuracy: 0.9252 - 3s/epoch - 7ms/step\n",
      "Epoch 12/500\n",
      "477/477 - 3s - loss: 0.2322 - categorical_accuracy: 0.9980 - val_loss: 0.5637 - val_categorical_accuracy: 0.9246 - 3s/epoch - 7ms/step\n",
      "59/59 [==============================] - 0s 2ms/step\n",
      "[16 15 10 ...  4  4  2]\n",
      "Neural network: 0.9045\n",
      "CPU times: total: 54.9 s\n",
      "Wall time: 38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Neural network\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# Can't use the `validation_split` argument with sparse arrays (not sure why)--\n",
    "# so manually do the split here.\n",
    "nn_train_x, val_x, nn_train_y, val_y = train_test_split(\n",
    "    train_x.astype(np.int32).toarray(),\n",
    "    train_y_onehot,\n",
    "    random_state=0,\n",
    "    train_size=0.9,\n",
    ")\n",
    "\n",
    "# Here's a very simple network: two hidden layers of 128 neurons, with\n",
    "# ReLU activation and a bit of regulatization and dropout to prevent\n",
    "# overfitting.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation=\"relu\", activity_regularizer=keras.regularizers.L2()),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(128, activation=\"relu\", activity_regularizer=keras.regularizers.L2()),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[keras.metrics.CategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "fit_history = model.fit(\n",
    "    nn_train_x,\n",
    "    nn_train_y,\n",
    "    batch_size=32,\n",
    "    epochs=500,\n",
    "    validation_data=(val_x, val_y),\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_categorical_accuracy\",\n",
    "            min_delta=0,\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "        ),\n",
    "    ],\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "preds = model.predict(\n",
    "    test_x.astype(np.int32).toarray()\n",
    ")\n",
    "preds = np.argmax(preds, axis=1).ravel()\n",
    "print(preds)\n",
    "print(f\"Neural network: {accuracy_score(test_y, preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5960bf-4253-481c-b7dc-dc0110e49d0d",
   "metadata": {},
   "source": [
    "There we go, the neural network--a pretty simple one--beat the other two models _handily._  This dataset is much more in line with something neural networks are good at: very sparse datasets with _lots_ of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a08811-35fb-40b4-844e-18124ae2d897",
   "metadata": {},
   "source": [
    "# Tensorboard: Better logging\n",
    "\n",
    "Reading all those outputs above is a bit of a mess.  Fortunately, there's a better way.  Tensorflow has an absolutely killer feature, which makes it well worth using in spite of the mess that is installing it: Tensorboard.\n",
    "\n",
    "Tensorboard is basically a fancy logging tool that gives you a browser-based dashboard that runs locally (so there's no actual internet connection).  Tensorboard gets installed alongside Tensorflow, but it's become such a standard tool for monitoring neural networks that most other neural net libraries can interface with it very easily.\n",
    "\n",
    "To use Tensorboard, add the Tensorboard callback to your model during fitting:\n",
    "\n",
    "```python\n",
    "from tensorflow import keras\n",
    "model = Sequential([...])\n",
    "model.compile(...)\n",
    "model.fit(\n",
    "    ...,\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(\"tensorboard_log_dir\"),\n",
    "        ...\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "This will create a folder \"tensorboard_log_dir\" in the folder the code is being run from.  As your model is fitting, it will update files in this directory in real-time.\n",
    "\n",
    "From your command line, with your Conda environment activated, navigate to the folder where \"tensorboard_log_dir\" got created, and run:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=tensorboard_log_dir\n",
    "```\n",
    "\n",
    "TensorBoard will open in your browser.\n",
    "\n",
    "TensorBoard also integrates with Jupyter Notebooks, which I'll use here to show what it looks like.  First, I needt to re-fit the above model and add the TensorBoard callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0148bc-9254-4da9-9e29-a1d0e2338ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "477/477 [==============================] - 5s 10ms/step - loss: 7.1350 - categorical_accuracy: 0.0748 - val_loss: 3.0108 - val_categorical_accuracy: 0.1179\n",
      "Epoch 2/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 6.6165 - categorical_accuracy: 0.1433 - val_loss: 2.9652 - val_categorical_accuracy: 0.2068\n",
      "Epoch 3/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 6.0474 - categorical_accuracy: 0.2435 - val_loss: 2.9210 - val_categorical_accuracy: 0.3076\n",
      "Epoch 4/500\n",
      "477/477 [==============================] - 4s 8ms/step - loss: 5.6607 - categorical_accuracy: 0.3491 - val_loss: 2.8642 - val_categorical_accuracy: 0.4078\n",
      "Epoch 5/500\n",
      "477/477 [==============================] - 4s 8ms/step - loss: 5.3263 - categorical_accuracy: 0.4567 - val_loss: 2.7939 - val_categorical_accuracy: 0.4950\n",
      "Epoch 6/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 4.9484 - categorical_accuracy: 0.5364 - val_loss: 2.7140 - val_categorical_accuracy: 0.5657\n",
      "Epoch 7/500\n",
      "477/477 [==============================] - 4s 8ms/step - loss: 4.5872 - categorical_accuracy: 0.6103 - val_loss: 2.6272 - val_categorical_accuracy: 0.6299\n",
      "Epoch 8/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 4.2488 - categorical_accuracy: 0.6741 - val_loss: 2.5362 - val_categorical_accuracy: 0.6659\n",
      "Epoch 9/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 3.9331 - categorical_accuracy: 0.7131 - val_loss: 2.4437 - val_categorical_accuracy: 0.6989\n",
      "Epoch 10/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 3.6436 - categorical_accuracy: 0.7550 - val_loss: 2.3505 - val_categorical_accuracy: 0.7395\n",
      "Epoch 11/500\n",
      "477/477 [==============================] - 5s 9ms/step - loss: 3.3923 - categorical_accuracy: 0.7879 - val_loss: 2.2593 - val_categorical_accuracy: 0.7672\n",
      "Epoch 12/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 3.1458 - categorical_accuracy: 0.8130 - val_loss: 2.1700 - val_categorical_accuracy: 0.7943\n",
      "Epoch 13/500\n",
      "477/477 [==============================] - 5s 11ms/step - loss: 2.8984 - categorical_accuracy: 0.8314 - val_loss: 2.0858 - val_categorical_accuracy: 0.8173\n",
      "Epoch 14/500\n",
      "477/477 [==============================] - 5s 11ms/step - loss: 2.6922 - categorical_accuracy: 0.8549 - val_loss: 2.0029 - val_categorical_accuracy: 0.8309\n",
      "Epoch 15/500\n",
      "477/477 [==============================] - 5s 10ms/step - loss: 2.4879 - categorical_accuracy: 0.8679 - val_loss: 1.9264 - val_categorical_accuracy: 0.8391\n",
      "Epoch 16/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 2.3264 - categorical_accuracy: 0.8789 - val_loss: 1.8530 - val_categorical_accuracy: 0.8491\n",
      "Epoch 17/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 2.1660 - categorical_accuracy: 0.8909 - val_loss: 1.7816 - val_categorical_accuracy: 0.8592\n",
      "Epoch 18/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 2.0058 - categorical_accuracy: 0.8994 - val_loss: 1.7155 - val_categorical_accuracy: 0.8656\n",
      "Epoch 19/500\n",
      "477/477 [==============================] - 4s 8ms/step - loss: 1.8775 - categorical_accuracy: 0.9080 - val_loss: 1.6536 - val_categorical_accuracy: 0.8733\n",
      "Epoch 20/500\n",
      "477/477 [==============================] - 4s 8ms/step - loss: 1.7546 - categorical_accuracy: 0.9166 - val_loss: 1.5946 - val_categorical_accuracy: 0.8786\n",
      "Epoch 21/500\n",
      "477/477 [==============================] - 4s 8ms/step - loss: 1.6320 - categorical_accuracy: 0.9230 - val_loss: 1.5385 - val_categorical_accuracy: 0.8798\n",
      "Epoch 22/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 1.5295 - categorical_accuracy: 0.9264 - val_loss: 1.4853 - val_categorical_accuracy: 0.8827\n",
      "Epoch 23/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 1.4457 - categorical_accuracy: 0.9334 - val_loss: 1.4356 - val_categorical_accuracy: 0.8880\n",
      "Epoch 24/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 1.3566 - categorical_accuracy: 0.9384 - val_loss: 1.3883 - val_categorical_accuracy: 0.8928\n",
      "Epoch 25/500\n",
      "477/477 [==============================] - 5s 10ms/step - loss: 1.2776 - categorical_accuracy: 0.9416 - val_loss: 1.3451 - val_categorical_accuracy: 0.8928\n",
      "Epoch 26/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 1.2060 - categorical_accuracy: 0.9477 - val_loss: 1.3038 - val_categorical_accuracy: 0.8975\n",
      "Epoch 27/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 1.1397 - categorical_accuracy: 0.9516 - val_loss: 1.2658 - val_categorical_accuracy: 0.8981\n",
      "Epoch 28/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 1.0785 - categorical_accuracy: 0.9530 - val_loss: 1.2279 - val_categorical_accuracy: 0.8992\n",
      "Epoch 29/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 1.0264 - categorical_accuracy: 0.9572 - val_loss: 1.1925 - val_categorical_accuracy: 0.9016\n",
      "Epoch 30/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 0.9761 - categorical_accuracy: 0.9600 - val_loss: 1.1586 - val_categorical_accuracy: 0.9039\n",
      "Epoch 31/500\n",
      "477/477 [==============================] - 5s 11ms/step - loss: 0.9279 - categorical_accuracy: 0.9640 - val_loss: 1.1276 - val_categorical_accuracy: 0.9034\n",
      "Epoch 32/500\n",
      "477/477 [==============================] - 5s 9ms/step - loss: 0.8814 - categorical_accuracy: 0.9653 - val_loss: 1.0968 - val_categorical_accuracy: 0.9039\n",
      "Epoch 33/500\n",
      "477/477 [==============================] - 4s 8ms/step - loss: 0.8438 - categorical_accuracy: 0.9688 - val_loss: 1.0687 - val_categorical_accuracy: 0.9057\n",
      "Epoch 34/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 0.8065 - categorical_accuracy: 0.9717 - val_loss: 1.0413 - val_categorical_accuracy: 0.9063\n",
      "Epoch 35/500\n",
      "477/477 [==============================] - 4s 8ms/step - loss: 0.7719 - categorical_accuracy: 0.9741 - val_loss: 1.0152 - val_categorical_accuracy: 0.9063\n",
      "Epoch 36/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 0.7384 - categorical_accuracy: 0.9754 - val_loss: 0.9915 - val_categorical_accuracy: 0.9063\n",
      "Epoch 37/500\n",
      "477/477 [==============================] - 4s 8ms/step - loss: 0.7092 - categorical_accuracy: 0.9779 - val_loss: 0.9680 - val_categorical_accuracy: 0.9075\n",
      "Epoch 38/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 0.6799 - categorical_accuracy: 0.9797 - val_loss: 0.9459 - val_categorical_accuracy: 0.9098\n",
      "Epoch 39/500\n",
      "477/477 [==============================] - 5s 11ms/step - loss: 0.6517 - categorical_accuracy: 0.9810 - val_loss: 0.9250 - val_categorical_accuracy: 0.9122\n",
      "Epoch 40/500\n",
      "477/477 [==============================] - 6s 12ms/step - loss: 0.6278 - categorical_accuracy: 0.9815 - val_loss: 0.9052 - val_categorical_accuracy: 0.9140\n",
      "Epoch 41/500\n",
      "477/477 [==============================] - 5s 9ms/step - loss: 0.6062 - categorical_accuracy: 0.9839 - val_loss: 0.8869 - val_categorical_accuracy: 0.9146\n",
      "Epoch 42/500\n",
      "477/477 [==============================] - 5s 10ms/step - loss: 0.5827 - categorical_accuracy: 0.9847 - val_loss: 0.8681 - val_categorical_accuracy: 0.9157\n",
      "Epoch 43/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 0.5625 - categorical_accuracy: 0.9857 - val_loss: 0.8524 - val_categorical_accuracy: 0.9157\n",
      "Epoch 44/500\n",
      "477/477 [==============================] - 4s 8ms/step - loss: 0.5418 - categorical_accuracy: 0.9880 - val_loss: 0.8351 - val_categorical_accuracy: 0.9163\n",
      "Epoch 45/500\n",
      "477/477 [==============================] - 5s 9ms/step - loss: 0.5246 - categorical_accuracy: 0.9891 - val_loss: 0.8198 - val_categorical_accuracy: 0.9175\n",
      "Epoch 46/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 0.5050 - categorical_accuracy: 0.9894 - val_loss: 0.8053 - val_categorical_accuracy: 0.9175\n",
      "Epoch 47/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 0.4883 - categorical_accuracy: 0.9904 - val_loss: 0.7906 - val_categorical_accuracy: 0.9175\n",
      "Epoch 48/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 0.4709 - categorical_accuracy: 0.9923 - val_loss: 0.7785 - val_categorical_accuracy: 0.9163\n",
      "Epoch 49/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 0.4557 - categorical_accuracy: 0.9924 - val_loss: 0.7662 - val_categorical_accuracy: 0.9169\n",
      "Epoch 50/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 0.4408 - categorical_accuracy: 0.9925 - val_loss: 0.7526 - val_categorical_accuracy: 0.9181\n",
      "Epoch 51/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 0.4279 - categorical_accuracy: 0.9932 - val_loss: 0.7420 - val_categorical_accuracy: 0.9181\n",
      "Epoch 52/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 0.4130 - categorical_accuracy: 0.9935 - val_loss: 0.7320 - val_categorical_accuracy: 0.9175\n",
      "Epoch 53/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 0.4001 - categorical_accuracy: 0.9945 - val_loss: 0.7212 - val_categorical_accuracy: 0.9175\n",
      "Epoch 54/500\n",
      "477/477 [==============================] - 4s 8ms/step - loss: 0.3874 - categorical_accuracy: 0.9948 - val_loss: 0.7117 - val_categorical_accuracy: 0.9187\n",
      "Epoch 55/500\n",
      "477/477 [==============================] - 4s 9ms/step - loss: 0.3768 - categorical_accuracy: 0.9948 - val_loss: 0.7014 - val_categorical_accuracy: 0.9157\n",
      "Epoch 56/500\n",
      "477/477 [==============================] - 4s 8ms/step - loss: 0.3650 - categorical_accuracy: 0.9950 - val_loss: 0.6922 - val_categorical_accuracy: 0.9169\n",
      "Epoch 57/500\n",
      "117/477 [======>.......................] - ETA: 3s - loss: 0.3585 - categorical_accuracy: 0.9973"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation=\"relu\", activity_regularizer=keras.regularizers.L2()),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(128, activation=\"relu\", activity_regularizer=keras.regularizers.L2()),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(20, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Changing some settings to make this run longer and slower--makes the graphs\n",
    "# easier to see.\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[keras.metrics.CategoricalAccuracy()],\n",
    ")\n",
    "fit_history = model.fit(\n",
    "    nn_train_x,\n",
    "    nn_train_y,\n",
    "    batch_size=32,\n",
    "    epochs=500,\n",
    "    validation_data=(val_x, val_y),\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_categorical_accuracy\",\n",
    "            min_delta=0,\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "        ),\n",
    "        keras.callbacks.TensorBoard(\n",
    "            \"tensorboard_log_dir\",\n",
    "            update_freq=\"batch\",\n",
    "        ),\n",
    "    ],\n",
    "    # We don't *need* this if we're going to use TensorBoard for monitoring,\n",
    "    # since TensorBoard can update in real-time, but it never hurts to have\n",
    "    # more outputs to monitor.\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de3f9ec-a6d6-46cc-bcd5-4cfff0af49ab",
   "metadata": {},
   "source": [
    "And that's about where we're gonna leave thing with Keras' Sequential API.\n",
    "\n",
    "I will say a few more words about TensorBoard, though.  TensorBoard is an amazing tool, with a _huge_ range of capabilities.  It's more helpful than you might expect when it comes to really big models that need to run for a really long time: if you have a model that's building overnight or over a weekend, TensorBoard is a great way to check up on its progress as it trains.  (but anything that runs \"over lunch\" rather than \"overnight,\" though, TensorBoard is probably overkill).  There are a lot of other really cool features we won't go into, like uploading subsets of your data and other cool logging stuff, but they are absolutely worth getting familiar with if you're considering a career in AI/ML/neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
