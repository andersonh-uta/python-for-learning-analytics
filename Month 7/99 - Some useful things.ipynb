{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fa2b5f6-5352-40cd-97d4-c717237e115b",
   "metadata": {},
   "source": [
    "# Some useful things\n",
    "\n",
    "A collection of miscellaneous useful things!\n",
    "\n",
    "# Parquet files: Better than CSV in (almost) every way\n",
    "\n",
    "CSV files and plain text files are great.  I love them.  They're a universal way to share data, whether it's in CSV, JSON, YAML, or some other format.  I love being able to very quickly and easily look at the structure of a file using any number of standard tools.\n",
    "\n",
    "...but, things like CSV and JSON have some big problems.  For one, the data always needs to be parsed, which can take a lot of time.  For two, the data is stored in _row-major_ order: as you read through the file one byte at a time, you're moving \"left to right\" through the rows.  This can make it hard to do something like \"only load these three columns\" with much efficiency, since you often need to read most of the row anyways to see where the columns start and stop.\n",
    "\n",
    "Enter Parquet, the format originally used by Apache Arrow, but now widely adopted by every tool that's kept up with the times.  Parquet differs from CSV and Excel and JSON and so on in a few major ways:\n",
    "- It is _column-major_ ordered.  As you iterate through the raw bytes of a file, you're moving down the columns, one column at a time.  There's a bit of header information in the file that says where each column starts and stops, so it's very easy and fast to only read in a few columns from a file that might have thousands.\n",
    "- It is a _binary_ file format.  It can't be opened and read with a basic text editor.\n",
    "- It has excellent support for _transparent compression_ using a wide range of compression standards.  You _can_ compress a CSV file, but I find it's usually not as fast as Parquet compression.\n",
    "- It is way faster to read and write.  Most data structures like Pandas `DataFrame`s already store data as collections of columns--not rows--so less work is needed to reshape the data.\n",
    "\n",
    "Pandas and Dask both have built-in support for Parquet, as long as you have the PyArrow or fastparquet libraries installed.  PyArrow is the default that they will look for, but fastparquet is a much smaller installation.  I generally use PyArrow; I've had a few issues in the past getting fastparquet to work, but I think those have all been resolved now.  PyArrow is a bit more feature-rich and supports more compression algorithms, but the difference is honestly pretty minimal if you're only reading from/writing to Parquet file with Pandas.  Install either library with:\n",
    "\n",
    "```bash\n",
    "conda install pyarrow\n",
    "# or\n",
    "conda install fastparquet\n",
    "```\n",
    "\n",
    "Then just use the `read_parquet()` and `to_parquet()` functions in Pandas and Dask.  (I have PyArrow installed for the below examples).\n",
    "\n",
    "The following cells show the speed and file size differences between CSV and Parquet, when Parquet is compressed with Zstandard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a605b7-2c4d-4a1a-ab18-502cf963718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some data--this is a ~50mb CSV file from the US Census Bureau.\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "if not (os.path.isfile(\"parquet_demo.csv\") and os.path.isfile(\"parquet_demo.parquet\")):\n",
    "    df = pd.read_csv(\"https://www2.census.gov/programs-surveys/bds/tables/time-series/bds2019_cty_fzc.csv\")\n",
    "    df.to_csv(\"parquet_demo.csv\", index=False)\n",
    "    df.to_parquet(\"parquet_demo.parquet\", index=False, compression=\"zstd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cee0225-bdd9-4748-8184-0db3d9f6ee07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file: 52,790kb\n",
      "Parquet file: 16,590kb\n"
     ]
    }
   ],
   "source": [
    "# File sizes\n",
    "print(f\"CSV file: {os.path.getsize('parquet_demo.csv') // 1_000:,}kb\")\n",
    "print(f\"Parquet file: {os.path.getsize('parquet_demo.parquet') // 1_000:,}kb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c57f8534-0dde-4b2c-84e1-51d81429ecef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV load time (all columns):\n",
      "CPU times: total: 1.48 s\n",
      "Wall time: 1.49 s\n",
      "\n",
      "Parquet load time (all columns):\n",
      "CPU times: total: 1.69 s\n",
      "Wall time: 635 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load times--all columns\n",
    "print(\"CSV load time (all columns):\")\n",
    "%time pd.read_csv(\"parquet_demo.csv\")\n",
    "print(\"\\nParquet load time (all columns):\")\n",
    "%time pd.read_parquet(\"parquet_demo.parquet\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d0dab7-4355-4175-8668-b896d4935be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV load time (2 columns):\n",
      "CPU times: total: 391 ms\n",
      "Wall time: 404 ms\n",
      "\n",
      "Parquet load time (2 columns):\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 40.7 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load times--just two columns\n",
    "print(\"CSV load time (2 columns):\")\n",
    "%time pd.read_csv(\"parquet_demo.csv\", usecols=[\"year\", \"net_job_creation\"])\n",
    "print(\"\\nParquet load time (2 columns):\")\n",
    "%time pd.read_parquet(\"parquet_demo.parquet\", columns=[\"year\", \"net_job_creation\"])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "994e101c-e25d-42fd-9bdb-531cc9defb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV serialization time:\n",
      "CPU times: total: 3.02 s\n",
      "Wall time: 3.02 s\n",
      "\n",
      "Parquet serialization time:\n",
      "CPU times: total: 1.38 s\n",
      "Wall time: 1.36 s\n"
     ]
    }
   ],
   "source": [
    "# Serialization time--if we don't specify a file, we get back the raw\n",
    "# text/bytes that would be saved to file.\n",
    "df = pd.read_parquet(\"parquet_demo.parquet\")\n",
    "print(\"CSV serialization time:\")\n",
    "%time _ = df.to_csv(index=False)\n",
    "print(\"\\nParquet serialization time:\")\n",
    "%time _ = df.to_parquet(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a1ebf3-cdfb-4b9c-a10b-ea86d0fb0913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snappy\n",
      "\tSave time:  12.93s\n",
      "\tLoad time:  5.87s\n",
      "\tSaved size: 186,501kb\n",
      "\n",
      "gzip, compression level 1\n",
      "\tSave time:  92.70s\n",
      "\tLoad time:  6.63s\n",
      "\tSaved size: 169,826kb\n",
      "\n",
      "gzip, compression level 3\n",
      "\tSave time:  64.81s\n",
      "\tLoad time:  6.34s\n",
      "\tSaved size: 165,906kb\n",
      "\n",
      "gzip, compression level 9\n",
      "\tSave time:  28.35s\n",
      "\tLoad time:  6.83s\n",
      "\tSaved size: 153,338kb\n",
      "\n",
      "brotli, compression level 9\n",
      "\tSave time:  37.84s\n",
      "\tLoad time:  6.07s\n",
      "\tSaved size: 118,255kb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rapid-fire compression comparison in PyArrow.\n",
    "from time import time\n",
    "\n",
    "# make the dataframe a lot bigger to emphasize the differences\n",
    "# between algorithms.\n",
    "_df = pd.concat((df for i in range(10)))\n",
    "\n",
    "for (alg, level) in [\n",
    "    (\"snappy\", None),\n",
    "    (\"gzip\", 1),\n",
    "    (\"gzip\", 3),\n",
    "    (\"gzip\", 9),\n",
    "    (\"brotli\", 9),\n",
    "    (\"lz4\", 9),\n",
    "    (\"zstd\", 1),\n",
    "    (\"zstd\", 6),\n",
    "    (\"zstd\", 19),\n",
    "    (None, None)\n",
    "]:\n",
    "    save_start = time()\n",
    "    serialized = _df.to_parquet(compression=alg, compression_level=level)\n",
    "    save_end = time()\n",
    "    \n",
    "    with open(\"_testing.parquet\", \"wb\") as OUT: OUT.write(serialized)\n",
    "    \n",
    "    load_start = time()\n",
    "    pd.read_parquet(\"_testing.parquet\")\n",
    "    load_end = time()\n",
    "    \n",
    "    size = len(serialized) // 1_000\n",
    "    \n",
    "    if level is not None:\n",
    "        print(f\"{alg}, compression level {level}\")\n",
    "    elif alg is None:\n",
    "        print(\"No compression\")\n",
    "    else:\n",
    "        print(alg)\n",
    "    print(f\"\\tSave time:  {save_end - save_start:.2f}s\")\n",
    "    print(f\"\\tLoad time:  {load_end - load_start:.2f}s\")\n",
    "    print(f\"\\tSaved size: {len(serialized) // 1_000:,}kb\")\n",
    "    print()\n",
    "    \n",
    "os.remove(\"_testing.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dfd7b4-bcd7-4256-aed5-2649ac130342",
   "metadata": {},
   "source": [
    "# `black`: format your code\n",
    "\n",
    "[`black`](https://github.com/psf/black) is a Python code formatter.  It reads your Python code, and applies a very particular and highly opinionated set of formatting conventions, but it does so without changing anything about _what your code does_ or _how it runs_.\n",
    "\n",
    "I love `black`.  It is an extremely easy way to guarantee that your code follows some pretty sensible layout choices, making your code a lot easier to read!  It can't do things like pick better variable/function names for you, but I don't know of anything that can.\n",
    "\n",
    "Install black with:\n",
    "\n",
    "```bash\n",
    "conda install black\n",
    "```\n",
    "\n",
    "Then run it from the command line with:\n",
    "\n",
    "```bash\n",
    "python -m black [your file].py\n",
    "```\n",
    "Black will then reformat your code and _overwrite_ the original file's contents.\n",
    "\n",
    "Most editors, including PyCharm, let you add arbitrary commands to run.  I've got `black` configured to run in PyCharm with a single keystroke, and reformat everything in my project.  I use it constantly.\n",
    "\n",
    "There are a few configuration options/settings you can use with `black`, but there are only two that probably matter:\n",
    "\n",
    "```bash\n",
    "-l [number]           How long a line should be before it wraps\n",
    "-t [py39|py310|...]   Python version(s) to support\n",
    "```\n",
    "\n",
    "`-l` can usually be set to somethinh around 90.  `-t` should be set to whatever version of Python you're using, though `py311` is pretty much always a good default, since it'll work for older versions too.\n",
    "\n",
    "You can disable `black`'s formatting in some parts of your code using two \"magic comments\":\n",
    "```python\n",
    "# fmt: off\n",
    "[code that will not be formatted]\n",
    "# fmt: on\n",
    "\n",
    "[code that will be formatted]\n",
    "```\n",
    "\n",
    "You can also manually put a comma at the end of the last item in a list, tuple, dictionary, function arguments, or anywhere else you use commas.  `black` will see that, and will take that as a signal to force every item in that list, tuple, etc. to be on it own line.  Not adding a final comma lets `black` decide whether to put everything on its own line or not.\n",
    "\n",
    "```python\n",
    "# before black\n",
    "my_list = [1, 2, 3, 4,]\n",
    "\n",
    "# after black\n",
    "my_list = [\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "]\n",
    "```\n",
    "\n",
    "(Taking away the final comma in the above example and re-running black will put everything back on one line).\n",
    "\n",
    "Here's a quick look at a more extensive before and after.\n",
    "\n",
    "```python\n",
    "# Before black.  You should be ashamed if you write code this ugly.\n",
    "def   foo (x,y= 10,\n",
    "        z = 0\n",
    "           ):\n",
    "    if isinstance(x, int) and isinstance(y, int) and isinstance(z, int) and x % 2 == 0 and y % 2 == 0 and z % 2 == 0 and x > 0 and y > 0 and z > 0:\n",
    "        print(\n",
    "            \"All arguments\"\n",
    "            \" are even positive integers\"\n",
    "        )\n",
    "\n",
    "    else: print(\"At least one arguent is negative, non-integer, or not even\")\n",
    "    \n",
    "# After black.  Much nicer!\n",
    "def foo(x, y=10, z=0):\n",
    "    if (\n",
    "        isinstance(x, int)\n",
    "        and isinstance(y, int)\n",
    "        and isinstance(z, int)\n",
    "        and x % 2 == 0\n",
    "        and y % 2 == 0\n",
    "        and z % 2 == 0\n",
    "        and x > 0\n",
    "        and y > 0\n",
    "        and z > 0\n",
    "    ):\n",
    "        print(\"All arguments\" \" are even positive integers\")\n",
    "\n",
    "    else:\n",
    "        print(\"At least one arguent is negative, non-integer, or not even\")\n",
    "```\n",
    "\n",
    "# `isort`: sort your imports!\n",
    "\n",
    "`isort` is like `black`, but it serves only one purpose: it sorts your import statements.  The general convention in Python is sort imports as follows:\n",
    "- All standard library imports, alphabetized by module name.\n",
    "- All third-party imports, alphabetized by module name.\n",
    "- All imports of other files in your program, alphabetized by module/program name.\n",
    "\n",
    "Conventionally, there's also a single whitespace between each of these blocks.\n",
    "\n",
    "Install `isort` with:\n",
    "\n",
    "```bash\n",
    "conda install isort\n",
    "```\n",
    "\n",
    "Like with `black`, run it from the command line (or as an action in your editor):\n",
    "```bash\n",
    "python -m isort [your file].py\n",
    "```\n",
    "\n",
    "I use `isort` and `black` together all the time.  I find you get the nicest results if you run `isort` first, then `black`; that's actually what my keybinding in PyCharm is set up to do.\n",
    "\n",
    "# `chime`: play alert sounds\n",
    "\n",
    "Here's something I never knew I wanted until I heard about it: [`chime`](https://github.com/MaxHalford/chime).  All it does it give you a few functions that, when called, play short alert sounds.  That's it.  But, this is surprisingly useful: scatter a few of these functions throughout your code and you can get auditory updates!\n",
    "\n",
    "E.g.: put `chime.success()` at the very end of your code, and you'll get a little sound when it finished running.  Or put `chime.error()` somewhere inside a `try-except` block.  (Or the reverse.  The name of the function just says what sound it plays.  There's no reason you can't use the error sound to indicate success!).\n",
    "\n",
    "Install `chime` with:\n",
    "\n",
    "```bash\n",
    "conda isntall -c conda-forge chime\n",
    "```\n",
    "\n",
    "Then use it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "847226cf-1453-4a3b-a94b-125efd3f7c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big-sur', 'chime', 'mario', 'material', 'zelda']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import chime\n",
    "\n",
    "# List the available themes\n",
    "print(chime.themes())\n",
    "\n",
    "# I like the Big Sur theme, so change to it.\n",
    "chime.theme(\"big-sur\")\n",
    "\n",
    "chime.info()\n",
    "for i in range(10):\n",
    "    time.sleep(0.5)\n",
    "chime.success()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed60bf08-4c1b-4fcc-9a32-b5b84b56f4bb",
   "metadata": {},
   "source": [
    "(You may not be able to hear the chime unles you run the above snippet locally on your own machine).  There are also functions to play your own custom sounds instead.  Definitely a neat little library!\n",
    "\n",
    "# `TPOT`: Genetic programming for ML tuning\n",
    "\n",
    "I'm only going to _barely_ scrape the surface here.  Genetic programming is a huge and fascinating topic.  But, in brief: genetic programming is one approach to optimizing systems, e.g. machine learning pipelines, using techniques based on analogies to natural selection and evolution.  You start with a \"generation\" of candidate solutions (e.g., data processing pipelines, or models), usually randomly initialized.  You then see how well each member of the generation does at a given task (e.g., you check their predictive accuracy on some data after fitting them).  All but the top few candidates \"die,\" and the next generation of candidates is made by generating random \"mutations\" of candidates in the previous generation, and/or by mixing-and-matching pieces from different candidates.  You keep running this process until you've either got a good enough set of candidates, or you've expended all your compute resources.\n",
    "\n",
    "Genetic programming can find some extremely weird, but effective, solutions.  The downside is that it is _absurdly_ slow.  It's common to see time limits placed on genetic programming models, e.g., \"run for 10 minutes, and then give me whatever the best candidate is at that point in time.\"\n",
    "\n",
    "TPOT is a Python library that uses genetic programming to find good pipelines for predictive modeling.  It's got built-in support for a huge range of models: scikit-learn, XGBoost, Dask, PyTorch (for neural networks), and more.  Install TPOT with:\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge tpot\n",
    "```\n",
    "\n",
    "Or, install TPOT and most of its optional dependencies with:\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge tpot xgboost dask dask-ml scikit-mdr skrebate\n",
    "```\n",
    "\n",
    "Then just import `TPOTCLassifier` or `TPOTRegressor` and treat them like any standard scikit-learn model.  Maybe don't do cross-validation with them, since they're designed to do that themselves--just create and fit them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fabbe7-a9c2-431d-8cb7-ebd7ae2cd3c9",
   "metadata": {},
   "source": [
    "Here's one of the TPOT example programs that finds a classification pipeline for the scikit-learn \"digits\" dataset (a set of hand-written digits):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10cdb390-b8b0-42d1-a7b3-2d6346c4ec01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andersonh\\Miniconda3\\envs\\PFDA_Month_7\\lib\\site-packages\\tpot\\builtins\\__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96217b3a9d0f4881a60f3f5d35811e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.9844058928817294\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.9844058928817294\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.9866363761531047\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.9866363761531047\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.9873743632107945\n",
      "\n",
      "Best pipeline: KNeighborsClassifier(Normalizer(input_matrix, norm=l2), n_neighbors=2, p=2, weights=distance)\n",
      "0.9866666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andersonh\\Miniconda3\\envs\\PFDA_Month_7\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:765: FutureWarning: sklearn.metrics.SCORERS is deprecated and will be removed in v1.3. Please use sklearn.metrics.get_scorer_names to get a list of available scorers and sklearn.metrics.get_metric to get scorer.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    digits.data,\n",
    "    digits.target,\n",
    "    train_size=0.75,\n",
    "    test_size=0.25,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Here's the TPOT part of the code.\n",
    "# This will run for about an hour.\n",
    "tpot_clf = TPOTClassifier(\n",
    "    generations=5,        # 5 generation...\n",
    "    population_size=50,   # ...each generation having 50 candidates.\n",
    "    verbosity=2,\n",
    "    random_state=42,\n",
    "    max_eval_time_mins=1, # max 1 min to evaluate each pipeline--for sanity's sake\n",
    "    n_jobs=10,            # my computer has 12 threads, so check 10 models at once\n",
    ")\n",
    "tpot_clf.fit(X_train, y_train)\n",
    "print(tpot_clf.score(X_test, y_test))\n",
    "# Export the model to a .py file for easy re-use.\n",
    "tpot_clf.export('tpot_digits_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ff59c1-7d35-4d5e-af9f-a978a31e1f89",
   "metadata": {},
   "source": [
    "This is a bit of a toy example--it's a very easy dataset to get high accuracy on, and the accuracy hit 98.4% in the first generation--but if you have a lot of time and/or compute power to throw at a problem, TPOT can often find really, _really_ good solution that might be hard to come up with on your own.\n",
    "\n",
    "For reference, here's what the final pipeline looked like when I ran this myself.  It might look different if you re-run this notebook.  (This is what got saved to the \"tpot_digits_pipeline.py\" file in `tpot.export()` statement of the above code).\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from tpot.export_utils import set_param_recursive\n",
    "\n",
    "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
    "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = tpot_data.drop('target', axis=1)\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'], random_state=42)\n",
    "\n",
    "# Average CV score on the training set was: 0.9873743632107945\n",
    "exported_pipeline = make_pipeline(\n",
    "    Normalizer(norm=\"l2\"),\n",
    "    KNeighborsClassifier(n_neighbors=2, p=2, weights=\"distance\")\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
