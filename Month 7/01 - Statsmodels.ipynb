{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af63d031-a3bb-4232-8efa-06cdaba97f8b",
   "metadata": {},
   "source": [
    "# Statsmodels\n",
    "\n",
    "Statsmodels is the most general go-to library in Python for doing fancy statistical modeling: linear regressions, GLMs, hierarchical models, mixed effects models, etc.; basically, if it's a commonly used tool and it's not in Scipy's `stats` module, it's probably in Statsmodels.  Statsmodels tends to focus on the mode widely-used, standard models; it usually doesn't have cutting-edge or exotic models, but in my experiences, it's still not as mature or well-polished as the other libraries we've covered (scikit-learn, Pandas, etc).\n",
    "\n",
    "Install statsmodels with:\n",
    "\n",
    "```bash\n",
    "conda install statsmodels\n",
    "```\n",
    "\n",
    "You can find the documentation at [statsmodels.org](https://www.statsmodels.org/stable/index.html).\n",
    "\n",
    "## A note about Statsmodels versus R\n",
    "\n",
    "The general rule of thumb: R has everything Statsmodels has, and more.  But Statsmodels has it all in one place, maintained by one team, with one (generally) consistent API.  R has its models scattered across a bunch of different packages (sometimes competing ones that do the same thing), some of which may be abandoned, some of which use their own idiosyncratic conventions, and so on.  Statsmodels won't have a lot of cutting-edge models and tests, but R will.\n",
    "\n",
    "If you have a lot of experience using other statistical packages (R, SPSS, Stata, etc.), you might be tempted to build the same model using them, and then using Statsmodels.  This is a great way to learn Statsmodels!  However: you should avoid trying to _validate_ Statsmodels' implementations this way.  There are a lot of pitfalls that lead you astray: different default settings, different mathematical formulations, different choices for low-level math libraries, different thresholds for numeric tolerance, etc.  You can learn a lot by trying to make something from Statsmodels match something from a different implementation, but in order to truly validate the models, you would need to do a pretty thorough audit of the source code.  (Which you could, in theory do--it's all open source!).  Remember: \"same\" does not mean \"correct!\"\n",
    "\n",
    "# Statsmodels vs Scipy and scikit-learn\n",
    "\n",
    "Scipy's `stats` module is usually what I reach for instead of Statsmodels.  The API tends to be a lot simpler, the tests tend to be a lot faster, and it usually has everything that I personally need.  (With the caveat that I don't do very fancy statistics in my projects).  Scipy's tests are also easier to manipulate programmtically.  But, Statsmodels will give you more detailed output, and has a lot of stuff that Scipy doesn't.\n",
    "\n",
    "If scikit-learn has an implementation of something that's also in Statsmodels, scikit-learn will usually be faster--sometimes by a _huge_ margin, especially for large datasets.  But, scikit-learn is designed for _predictive modeling,_ not _explanatory modeling_, so it doesn't calculate p-values, confidence intervals, etc (they're not nearly as important as speed, or other evaluation metrics, when doing predictive modeling).  \n",
    "\n",
    "I generally find Statsmodels lacking in a few areas.  First, its documentation is nowhere near as good as most other big libaries we've looked at.  It's not terrible, but it doesn't give you as much detail on _how to actually use_ things in the library.  Second, I find Statsmodels is much more prone to undocumented behavior, bugs, and strange inconsistencies once you need to do anything too complex with it.  E.g.: I've never managed to get the missing value imputation to work.  It always throws errors, and I can't figure out from the documentation what I'm supposed to do to actually impute missing values.\n",
    "\n",
    "# Statsmodels Quickstart\n",
    "\n",
    "Say it with me now: Statsmodels integrates with Pandas `DataFrame`s and Numpy `array`s.  Here's a few examples of basic models, using a synthetically created regression dataset from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fffd86c1-ddfb-4594-b0c6-7a215116d9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Col0      Col1      Col2      Col3      Col4      Col5      Col6  \\\n",
      "0   0.355482 -0.807648  0.766663 -1.115897  0.814520 -0.598654 -1.768538   \n",
      "1  -0.977278  0.410599  0.978738  0.400157  0.950088  1.764052  1.867558   \n",
      "2  -0.149635  0.407462  0.298238 -1.099401 -0.435154  0.376426 -0.694568   \n",
      "3   0.684501  1.719589 -0.309114  0.800298  0.370825 -1.446535  1.732721   \n",
      "4  -0.663478 -0.437820 -0.744755  1.713343  1.126636 -0.068242 -0.098453   \n",
      "5   1.895889  1.054452  0.465662  0.900826  1.178780 -1.165150  1.488252   \n",
      "6  -0.390953  2.064493  1.955912 -2.772593  0.493742  0.399046 -0.652409   \n",
      "7   1.943621  1.480515 -1.270485 -1.347759 -0.413619  1.883151 -1.173123   \n",
      "8   0.781198  0.676908 -0.542861 -0.493320  1.494485 -1.424061 -1.156182   \n",
      "9  -0.460720 -0.159573 -0.118164  1.658131 -1.334258 -1.306527  0.666383   \n",
      "10  0.063262 -0.237922 -0.463596 -0.345982  0.156507 -0.955945 -1.540797   \n",
      "11 -1.602058  1.543015 -0.643618 -1.374951 -1.104383 -0.353994  0.625231   \n",
      "12 -0.521189  0.620358  1.364532 -1.437791 -1.843070 -0.764144 -0.652294   \n",
      "13 -0.761492  0.852552 -0.353432 -0.222675  0.857924  0.567290 -0.291837   \n",
      "14 -0.684810  0.056165  1.139401  0.128983 -0.870797  0.729091  0.402342   \n",
      "15  0.706573  0.401989  0.208275  1.222445  0.010500 -0.403177  0.356366   \n",
      "16  0.428332 -0.362741 -0.510805  0.386902  0.066517 -0.895467 -0.028182   \n",
      "17 -1.454366  1.469359  0.864436  0.653619  0.045759 -2.552990  2.269755   \n",
      "18  0.944479 -0.461585  0.166673  0.439392 -0.912822 -1.491258  2.383145   \n",
      "19  0.156349 -0.302303 -0.887786  0.378163  1.230291  0.154947 -0.347912   \n",
      "20  0.094352 -0.107305 -1.126826  1.077744 -0.042171 -0.133702 -0.384880   \n",
      "21 -0.312292  0.943261  0.274516 -0.812993 -0.157667 -0.719604 -1.157355   \n",
      "22  0.608844  1.301846 -0.692050  1.020173 -1.045253 -0.110541  0.286344   \n",
      "23  2.021044 -0.050604  0.463130  0.820248 -0.468864 -1.054628  0.338904   \n",
      "24 -1.676004 -1.466424 -0.132881 -0.397272  1.152332 -0.637437 -0.309013   \n",
      "25 -0.115107 -0.110389 -0.085931 -1.159421  0.457416 -0.395229  0.875833   \n",
      "26 -2.655619  0.220508 -0.908763  1.029439  1.513328  0.771406  0.862596   \n",
      "27  1.136891  0.370056  2.303917 -0.481027  0.097725 -0.628088 -0.135950   \n",
      "28 -1.656715  0.164228 -2.255564  0.387280 -0.985511 -1.698106  0.038631   \n",
      "29  0.333674 -0.854096  0.761038  1.454274  1.494079  0.144044  0.443863   \n",
      "30  0.606320  1.659551  0.773253 -1.188945 -1.755891  0.747188 -2.659172   \n",
      "31  0.844363  0.316943  0.949421  1.929532 -1.000215 -0.498032 -1.225436   \n",
      "32 -0.171546  1.336528 -0.039283  0.267051  0.771791 -1.292857  0.523277   \n",
      "33 -0.073925 -0.077855  1.100284 -0.349943 -0.658553 -1.029935  2.696224   \n",
      "34  0.314817  0.078260  0.868963 -1.416906  0.821586 -0.466846 -0.971105   \n",
      "35 -0.438074 -0.212740 -1.706270 -1.420018 -1.252795 -1.048553 -0.509652   \n",
      "36  0.994394  0.496001 -0.275671 -0.539455  1.319137 -2.016407  1.738873   \n",
      "37  0.093953  0.269904 -2.094603  0.582225  0.943046  0.929505 -0.130107   \n",
      "38  0.694749  0.610379  0.141953 -0.575788 -0.725597  0.521065  0.691539   \n",
      "39  0.493837  0.880179  0.841631 -0.993124  0.643314  0.280442  0.049495   \n",
      "40  2.412454  0.251484 -0.439190 -0.978830 -0.960504 -0.517519 -0.502817   \n",
      "41 -1.616956 -0.098150  1.099660 -0.239379 -0.024326 -0.369182  0.640132   \n",
      "42 -0.410050 -0.042257  0.786328  0.317218 -0.017020  0.910179 -0.944446   \n",
      "43 -0.280355  0.349654 -0.687838 -0.453386 -0.364694  1.068509 -0.440923   \n",
      "44 -0.401781  0.051945 -0.813146 -0.359553 -1.630198 -0.672460  0.177426   \n",
      "45  0.188779  0.097400 -0.596314 -0.506816  0.523891 -1.188859 -1.936280   \n",
      "46  0.681595  0.017479  0.856831  0.318728 -0.803410  0.920859 -1.034243   \n",
      "47  0.160928 -1.128011  0.931848  0.003771 -0.190653  0.698457 -0.015682   \n",
      "48  0.676433 -1.093062 -0.674333  0.539249  0.576591 -0.769916 -0.635846   \n",
      "49  0.802456  0.922207 -0.861226  0.906045  0.947252  1.867559 -0.268003   \n",
      "\n",
      "        Col7      Col8      Col9      Target  \n",
      "0   0.356293 -0.185054  0.058926 -136.715703  \n",
      "1   2.240893 -0.103219 -0.151357  119.841457  \n",
      "2   1.326386  0.672295  1.849264  -90.532277  \n",
      "3  -0.233467  1.519995  0.142062   12.311048  \n",
      "4  -0.826439 -1.147469 -1.079932  164.782090  \n",
      "5  -1.536244 -1.070753 -0.179925   35.023408  \n",
      "6   0.390093 -2.030684 -0.116104 -253.464882  \n",
      "7   0.969397  1.922942 -0.747455  -46.004258  \n",
      "8   0.416050  0.426259 -2.069985 -113.431442  \n",
      "9  -0.680178  0.693773 -1.346718  102.774605  \n",
      "10  0.481481 -0.597316  0.232181  -77.597214  \n",
      "11 -2.223403 -0.739563  0.052165 -150.919649  \n",
      "12 -0.689449 -0.479656 -0.477974 -175.824015  \n",
      "13 -1.616474  1.466579  1.141102    4.107178  \n",
      "14 -1.234826 -0.311553 -0.578850   45.963562  \n",
      "15  0.976639  0.126912  1.785870  101.367456  \n",
      "16 -1.180632 -0.634322  0.302472   -3.013097  \n",
      "17 -0.742165  1.532779 -0.187184  -52.634476  \n",
      "18  0.635031 -1.315907  1.117016  -25.100503  \n",
      "19 -1.980796 -0.387327  1.202380   44.140510  \n",
      "20 -0.730678 -0.061626 -0.286887   99.503886  \n",
      "21 -0.890915 -0.704700  2.256723 -112.560399  \n",
      "22  1.536377  0.689818  1.211145   94.920725  \n",
      "23  0.279096  0.199300 -2.201441   32.178480  \n",
      "24 -0.297791 -0.813364  1.079619  -68.065759  \n",
      "25  0.194293 -0.782629 -0.964612 -131.683130  \n",
      "26 -0.424318 -0.045704  0.553132  136.138912  \n",
      "27 -1.060016 -0.399449  0.582954  -75.846138  \n",
      "28 -1.022507  1.648135 -1.471835  -39.661387  \n",
      "29  0.121675  0.313068 -0.205158  149.096963  \n",
      "30 -1.183881 -0.684011  0.450934  -82.361229  \n",
      "31  0.087551  1.188030 -1.544771  166.323863  \n",
      "32 -1.168093  2.163236  0.823504  -32.921179  \n",
      "33  1.298022 -1.018042 -0.514234  -81.367226  \n",
      "34  0.276872  0.800565  0.005293 -160.189046  \n",
      "35  1.950775 -1.613898  0.777490 -187.081440  \n",
      "36 -0.709728  1.128594 -0.882419 -145.026153  \n",
      "37  0.123722 -0.569312 -2.739677   99.539662  \n",
      "38 -0.319328 -1.582938 -1.383364  -32.609325  \n",
      "39 -0.249459 -0.206904 -1.570623  -84.504539  \n",
      "40  0.181338 -2.288620 -0.793117 -119.575300  \n",
      "41  0.655264  0.279925 -0.738031  -40.332037  \n",
      "42 -0.466419  2.259309  0.379152   72.686714  \n",
      "43 -1.214077  0.578521  0.156704    4.407058  \n",
      "44 -1.726283 -0.907298  0.462782  -65.970249  \n",
      "45 -0.052567 -0.310886  0.088422 -104.003947  \n",
      "46 -0.651026 -0.455533 -0.689550   73.322769  \n",
      "47  0.339965 -0.267734 -0.394850   32.293129  \n",
      "48  0.031831  0.396007 -0.208299   17.654700  \n",
      "49  1.910065  0.614079 -0.155010  174.147382  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression\n",
    "X, Y = make_regression(\n",
    "    n_samples=50,\n",
    "    n_features=10,\n",
    "    n_informative=2,\n",
    "    random_state=0, # so you can get the same output as I do\n",
    ")\n",
    "df = pd.DataFrame(X, columns=[f\"Col{i}\" for i in range(X.shape[1])])\n",
    "df[\"Target\"] = Y\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c89b34-aeef-4d66-93af-f95d11a9ba52",
   "metadata": {},
   "source": [
    "The `api` module contains all the models you'd want to fit.  It's conventionally imported as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c02af3-269c-4396-9c0b-af5b170531af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacc0c47-8ccb-40e9-840c-7c5beedd5f4d",
   "metadata": {},
   "source": [
    "Let's fit a basic model: an ordinary least squares regression to the Numpy `array` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755d718b-98c9-4f60-b2d9-bf98fbfb69e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                      y   R-squared (uncentered):                   1.000\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              1.000\n",
      "Method:                 Least Squares   F-statistic:                          1.271e+30\n",
      "Date:                Tue, 16 Aug 2022   Prob (F-statistic):                        0.00\n",
      "Time:                        18:16:07   Log-Likelihood:                          1394.5\n",
      "No. Observations:                  50   AIC:                                     -2769.\n",
      "Df Residuals:                      40   BIC:                                     -2750.\n",
      "Df Model:                          10                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1          7.283e-14   3.26e-14      2.233      0.031    6.92e-15    1.39e-13\n",
      "x2          -3.02e-14   4.15e-14     -0.728      0.471   -1.14e-13    5.36e-14\n",
      "x3         -1.776e-14   3.22e-14     -0.551      0.585   -8.29e-14    4.74e-14\n",
      "x4            97.9962   3.79e-14   2.59e+15      0.000      97.996      97.996\n",
      "x5         -8.882e-15   3.31e-14     -0.268      0.790   -7.58e-14     5.8e-14\n",
      "x6            45.7059   3.16e-14   1.45e+15      0.000      45.706      45.706\n",
      "x7         -2.653e-14   3.19e-14     -0.833      0.410   -9.09e-14    3.78e-14\n",
      "x8          4.707e-14   3.12e-14      1.509      0.139    -1.6e-14     1.1e-13\n",
      "x9          5.862e-14   3.24e-14      1.810      0.078   -6.85e-15    1.24e-13\n",
      "x10        -6.661e-15   2.92e-14     -0.229      0.820   -6.56e-14    5.23e-14\n",
      "==============================================================================\n",
      "Omnibus:                        5.332   Durbin-Watson:                   1.598\n",
      "Prob(Omnibus):                  0.070   Jarque-Bera (JB):                4.206\n",
      "Skew:                          -0.586   Prob(JB):                        0.122\n",
      "Kurtosis:                       3.804   Cond. No.                         2.52\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "lm = sm.OLS(endog=Y, exog=X)\n",
    "lm = lm.fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e83c6e5-ceab-4dcf-8c8f-3986c382bfb4",
   "metadata": {},
   "source": [
    "Now let's re-fit it using the `DataFrame` data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "807e3c8d-955b-43ec-b725-7403dafb3b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                 Target   R-squared (uncentered):                   1.000\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              1.000\n",
      "Method:                 Least Squares   F-statistic:                          1.270e+30\n",
      "Date:                Tue, 16 Aug 2022   Prob (F-statistic):                        0.00\n",
      "Time:                        18:18:23   Log-Likelihood:                          1394.4\n",
      "No. Observations:                  50   AIC:                                     -2769.\n",
      "Df Residuals:                      40   BIC:                                     -2750.\n",
      "Df Model:                          10                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Col0        7.283e-14   3.26e-14      2.233      0.031     6.9e-15    1.39e-13\n",
      "Col1        -3.02e-14   4.15e-14     -0.728      0.471   -1.14e-13    5.36e-14\n",
      "Col2       -1.776e-14   3.22e-14     -0.551      0.585   -8.29e-14    4.74e-14\n",
      "Col3          97.9962   3.79e-14   2.58e+15      0.000      97.996      97.996\n",
      "Col4       -8.882e-15   3.31e-14     -0.268      0.790   -7.58e-14    5.81e-14\n",
      "Col5          45.7059   3.16e-14   1.45e+15      0.000      45.706      45.706\n",
      "Col6       -2.653e-14   3.19e-14     -0.833      0.410   -9.09e-14    3.79e-14\n",
      "Col7        4.707e-14   3.12e-14      1.509      0.139    -1.6e-14     1.1e-13\n",
      "Col8        5.862e-14   3.24e-14      1.809      0.078   -6.86e-15    1.24e-13\n",
      "Col9       -6.661e-15   2.92e-14     -0.228      0.820   -6.56e-14    5.23e-14\n",
      "==============================================================================\n",
      "Omnibus:                        3.628   Durbin-Watson:                   1.586\n",
      "Prob(Omnibus):                  0.163   Jarque-Bera (JB):                2.552\n",
      "Skew:                          -0.447   Prob(JB):                        0.279\n",
      "Kurtosis:                       3.654   Cond. No.                         2.52\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "lm = sm.OLS(endog=df[\"Target\"], exog=df.drop(columns=[\"Target\"]))\n",
    "lm = lm.fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a91347f-cebb-40cb-a51c-693faef21815",
   "metadata": {},
   "source": [
    "Note how the only difference between the two approaches is that with a Numpy `array`, our variable names are all in the form \"x1\", \"x2\", etc; with a `DataFrame`, the column names get used as variable names in the summary table.\n",
    "\n",
    "Pretty simple!  Now let's try a more complex model.  Let's fit a GLM with a Bonimial family and the default link function, just to show how it's done.  (let's ignore how this is absolutely not the right model for this data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59641f81-c019-4b58-9862-8dfe1566be5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                   50\n",
      "Model:                            GLM   Df Residuals:                       40\n",
      "Model Family:                Binomial   Df Model:                            9\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                    inf\n",
      "Date:                Tue, 16 Aug 2022   Deviance:                   3.6311e+05\n",
      "Time:                        18:25:09   Pearson chi2:                 2.48e+21\n",
      "No. Iterations:                     2   Pseudo R-squ. (CS):                nan\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1         -2.034e+12   1.05e+07  -1.94e+05      0.000   -2.03e+12   -2.03e+12\n",
      "x2         -1.799e+15   1.33e+07  -1.35e+08      0.000    -1.8e+15    -1.8e+15\n",
      "x3         -1.957e+14   1.03e+07  -1.89e+07      0.000   -1.96e+14   -1.96e+14\n",
      "x4          6.599e+17   1.22e+07   5.42e+10      0.000     6.6e+17     6.6e+17\n",
      "x5         -5.429e+14   1.06e+07  -5.11e+07      0.000   -5.43e+14   -5.43e+14\n",
      "x6          3.087e+17   1.01e+07   3.05e+10      0.000    3.09e+17    3.09e+17\n",
      "x7          5.142e+14   1.02e+07   5.03e+07      0.000    5.14e+14    5.14e+14\n",
      "x8          2.616e+14      1e+07   2.61e+07      0.000    2.62e+14    2.62e+14\n",
      "x9          4.926e+14   1.04e+07   4.74e+07      0.000    4.93e+14    4.93e+14\n",
      "x10         3.775e+14   9.36e+06   4.03e+07      0.000    3.78e+14    3.78e+14\n",
      "==============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andersonh\\Miniconda3\\envs\\PFDA_Month_7\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:187: RuntimeWarning: overflow encountered in exp\n",
      "  t = np.exp(-z)\n",
      "C:\\Users\\andersonh\\Miniconda3\\envs\\PFDA_Month_7\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1014: RuntimeWarning: invalid value encountered in log\n",
      "  special.gammaln(n - y + 1) + y * np.log(mu / (1 - mu + 1e-20)) +\n",
      "C:\\Users\\andersonh\\Miniconda3\\envs\\PFDA_Month_7\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1014: RuntimeWarning: divide by zero encountered in log\n",
      "  special.gammaln(n - y + 1) + y * np.log(mu / (1 - mu + 1e-20)) +\n"
     ]
    }
   ],
   "source": [
    "glm = sm.GLM(endog=Y, exog=X, family=sm.families.Binomial(link=None))\n",
    "glm = glm.fit()\n",
    "print(glm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaa3f74-82c0-434b-b5c1-6c5e9a316fcf",
   "metadata": {},
   "source": [
    "(The results are nonsense, but the point is that the code is pretty simple).\n",
    "\n",
    "Pretty much everything in Statsmodels follows the above three-line paradigm:\n",
    "\n",
    "- Create the model, and specify your dependent/independent variables here.  This creates an uninitialized/unfitted model.\n",
    "- Call `model.fit()`.  This fits the model and returns the fitted instance, which contains the fit statistics.\n",
    "    - Note: unlike scikit-learn, `.fit()` is NOT an in-place method!  It does not _update_ the model object; it _returns_ a new kind of object that contains the information from when the model was fit.  So you _have_ to use the `model = model.fit()` line--if you just say `model.fit()`, you'll be calculating the results, but not saving them to a variable, so they get immediately discarded.\n",
    "- Call `model.summary()`, or other methods/attributes on the object, to see the results.\n",
    "    - You will usually use `.summary()` if you're manually inspecting the results.\n",
    "    - You'll use other methods to, e.g., pull out just the P-values, or just the coefficients, etc., if you're programmatically manipulating the results.\n",
    "        - E.g.: run a test for normality, then based on the results, run either a t-test or a Mann-Whitney U-test.  You could do this by hand, but you should probably do this programmatically if you're analyzing anything other than a fixed, never-going-to-be-updated-again dataset.\n",
    "\n",
    "Much like scikit-learn, the API for Statsmodels is super consistent across the different models.  Each model really only differs in terms of the arguments it takes when you're creating it.\n",
    "\n",
    "An important note on Statsmodel's terminology.  Rather than \"x and y,\" or \"independent and dependent,\" or \"features and target,\" Statsmodels uses the terms _endogenous_ to refer to the y/dependent/target variables, and _exogenous_ to refer to the x/dependent/feature variables.  These are abbreviated to _endog_ and _exog_ when used as arguments to models.  A helpful mnemonic (courtesy of the [Statsmodels page on the topic](https://www.statsmodels.org/stable/endog_exog.html)): _exogenous_ has an \"x\" in it, so it's the \"x\" variable.\n",
    "\n",
    "There really isn't a whole lot more to the core of the library than that.  Do your data prep using Pandas, Numpy, scikit-learn, pure Python, or whatever else, then just create your Statsmodels model, fit it, and look at the results.  As always, there is a lot more you can do with this library that we'll be going into.  But, probably moreso than any other library we're looking at, the absolute basics will get you very, very far with Statsmodels.\n",
    "\n",
    "# Formulas\n",
    "\n",
    "If you're coming from R, you'll be familiar with formula expressions, like:\n",
    "\n",
    "```R\n",
    "price ~ weight + mpg + (make * model)\n",
    "```\n",
    "\n",
    "Well, Statsmodels supports formulas like this!  There's one major difference: in R, formulas are a dedicated piece of syntax in the language, which make use of some of R's metaprogramming and delayed evaluation tools.  In Statsmodels, all formulas are _strings,_ since Python does not have a built-in notion of formulas, and Python has none of the metaprogramming/delayed evaluation tools that R does.\n",
    "\n",
    "There are a few syntactic differences between formulas in Statsmodels and R.  Statsmodels uses the [Patsy](https://patsy.readthedocs.io/en/latest/) language to parse formulas, which is very similar to R (identical, for a lot of cases), but has some differences.  You'll need to read the documentation for more details.\n",
    "\n",
    "Formulas work best when your exogenous variable is a Pandas `DataFrame`, since you can just use the column names in the formulas.  You'll also need to import a different part of Statsmodels.  The conventional import is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f366daea-23a7-41b9-bacd-fd0cfaf86917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d94edd-0675-43e7-a7be-4a3067bf4b5a",
   "metadata": {},
   "source": [
    "From there, it should all look pretty straightforward.  The model creation step changes, so it now looks like:\n",
    "\n",
    "```python\n",
    "model = smf.modelname(data=my_data, formula=\"y ~ x1 + x2*x3\")\n",
    "```\n",
    "\n",
    "Most model name are _all lowercase_ when creating them this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20fadd70-1375-4b7c-9eb9-8493835d0843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                  1.000\n",
      "Method:                 Least Squares   F-statistic:                 8.983e+30\n",
      "Date:                Tue, 16 Aug 2022   Prob (F-statistic):               0.00\n",
      "Time:                        18:29:19   Log-Likelihood:                 1444.7\n",
      "No. Observations:                  50   AIC:                            -2867.\n",
      "Df Residuals:                      39   BIC:                            -2846.\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   1.732e-14   1.27e-14      1.363      0.181   -8.38e-15     4.3e-14\n",
      "Col0        -3.73e-14   1.21e-14     -3.084      0.004   -6.18e-14   -1.28e-14\n",
      "Col1       -3.375e-14   1.68e-14     -2.014      0.051   -6.76e-14    1.42e-16\n",
      "Col2       -3.109e-14   1.19e-14     -2.601      0.013   -5.53e-14   -6.92e-15\n",
      "Col3          97.9962   1.41e-14   6.94e+15      0.000      97.996      97.996\n",
      "Col4       -8.882e-15   1.24e-14     -0.719      0.477   -3.39e-14    1.61e-14\n",
      "Col5          45.7059   1.23e-14   3.73e+15      0.000      45.706      45.706\n",
      "Col6       -1.288e-14    1.2e-14     -1.076      0.288   -3.71e-14    1.13e-14\n",
      "Col7        8.882e-15   1.16e-14      0.768      0.447   -1.45e-14    3.23e-14\n",
      "Col8       -2.132e-14   1.21e-14     -1.755      0.087   -4.59e-14    3.26e-15\n",
      "Col9        9.326e-15   1.09e-14      0.855      0.398   -1.27e-14    3.14e-14\n",
      "==============================================================================\n",
      "Omnibus:                        0.960   Durbin-Watson:                   2.270\n",
      "Prob(Omnibus):                  0.619   Jarque-Bera (JB):                0.449\n",
      "Skew:                          -0.214   Prob(JB):                        0.799\n",
      "Kurtosis:                       3.181   Cond. No.                         2.75\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "X_df = pd.DataFrame(X, columns=[f\"Col{i}\" for i in range(X.shape[1])])\n",
    "X_df[\"Target\"] = Y\n",
    "\n",
    "# smf.ols; compare to sm.OLS, which is in all capital letters\n",
    "lm = smf.ols(\n",
    "    data=X_df,\n",
    "    formula=\"Target ~ Col0 + Col1 + Col2 + Col3 + Col4 + Col5 + Col6 + Col7 + Col8 + Col9 + 1\"\n",
    ")\n",
    "lm = lm.fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c34314-d942-4099-bcea-98ecfe184d02",
   "metadata": {},
   "source": [
    "Since formulas are just strings, you can manipulate them like strings, and construct them programmatically.  You probably shouldn't be doing this, but there might be times when it's useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17ddd3b7-5540-43ba-a50b-189fbfef7db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                  1.000\n",
      "Method:                 Least Squares   F-statistic:                 8.983e+30\n",
      "Date:                Tue, 16 Aug 2022   Prob (F-statistic):               0.00\n",
      "Time:                        18:32:55   Log-Likelihood:                 1444.7\n",
      "No. Observations:                  50   AIC:                            -2867.\n",
      "Df Residuals:                      39   BIC:                            -2846.\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   1.732e-14   1.27e-14      1.363      0.181   -8.38e-15     4.3e-14\n",
      "Col0        -3.73e-14   1.21e-14     -3.084      0.004   -6.18e-14   -1.28e-14\n",
      "Col1       -3.375e-14   1.68e-14     -2.014      0.051   -6.76e-14    1.42e-16\n",
      "Col2       -3.109e-14   1.19e-14     -2.601      0.013   -5.53e-14   -6.92e-15\n",
      "Col3          97.9962   1.41e-14   6.94e+15      0.000      97.996      97.996\n",
      "Col4       -8.882e-15   1.24e-14     -0.719      0.477   -3.39e-14    1.61e-14\n",
      "Col5          45.7059   1.23e-14   3.73e+15      0.000      45.706      45.706\n",
      "Col6       -1.288e-14    1.2e-14     -1.076      0.288   -3.71e-14    1.13e-14\n",
      "Col7        8.882e-15   1.16e-14      0.768      0.447   -1.45e-14    3.23e-14\n",
      "Col8       -2.132e-14   1.21e-14     -1.755      0.087   -4.59e-14    3.26e-15\n",
      "Col9        9.326e-15   1.09e-14      0.855      0.398   -1.27e-14    3.14e-14\n",
      "==============================================================================\n",
      "Omnibus:                        0.960   Durbin-Watson:                   2.270\n",
      "Prob(Omnibus):                  0.619   Jarque-Bera (JB):                0.449\n",
      "Skew:                          -0.214   Prob(JB):                        0.799\n",
      "Kurtosis:                       3.181   Cond. No.                         2.75\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "lm = smf.ols(\n",
    "    data=X_df,\n",
    "    # *definitely* unnecessary here, but this migth be useful in more\n",
    "    # unusual circumtances.\n",
    "    formula=\"Target ~ \" + \" + \".join(f\"Col{i}\" for i in range(X.shape[1])) + \"+ 1\"\n",
    ")\n",
    "lm = lm.fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd046a99-2d37-491b-b4c0-9ce4bbb91414",
   "metadata": {},
   "source": [
    "Note that you can also construct models like this using the `statsmodels.api` interface.  In fact, the implementations in `statsmodels.formula.api` just call these methods for you in the background.  So it doesn't really matter which convention you use for formulas, but you should pick one and stick to it as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aa47fb-945f-4d62-81eb-99e834c0f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "lm = sm.OLS.from_formula(\n",
    "    data=X_df,\n",
    "    formula=\"Target ~ Col0 + Col1 + Col2 + Col3 + Col4 + Col5 + Col6 + Col7 + Col8 + Col9 + 1\"\n",
    ")\n",
    "lm = lm.fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7f404c-49f7-4d25-8cf3-e5261238b89e",
   "metadata": {},
   "source": [
    "We can also use formulas to specify on-the-fly data transformations.  Consult the Patsy documentation for full details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "417f6bfe-4f06-4a87-a82e-03a863128904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   R-squared:                       0.731\n",
      "Model:                            OLS   Adj. R-squared:                  0.462\n",
      "Method:                 Least Squares   F-statistic:                     2.715\n",
      "Date:                Tue, 16 Aug 2022   Prob (F-statistic):             0.0896\n",
      "Time:                        18:39:02   Log-Likelihood:                -83.451\n",
      "No. Observations:                  17   AIC:                             184.9\n",
      "Df Residuals:                       8   BIC:                             192.4\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================================\n",
      "                                  coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Intercept                     -21.4474     48.139     -0.446      0.668    -132.455      89.560\n",
      "np.log1p(Col0)                  6.1011     13.635      0.447      0.666     -25.341      37.544\n",
      "np.exp(Col1)                   -6.5383     15.974     -0.409      0.693     -43.374      30.298\n",
      "np.sin(Col2)                   29.6643     20.519      1.446      0.186     -17.653      76.982\n",
      "np.sqrt(Col3 + Col4 + Col5)   102.3895     32.075      3.192      0.013      28.425     176.354\n",
      "Col6                          -33.3126     29.442     -1.131      0.291    -101.207      34.582\n",
      "Col7                           13.9556     12.028      1.160      0.279     -13.781      41.692\n",
      "Col8                          -17.5910     22.486     -0.782      0.457     -69.443      34.261\n",
      "Col9                           -3.2435     13.861     -0.234      0.821     -35.207      28.720\n",
      "==============================================================================\n",
      "Omnibus:                        1.787   Durbin-Watson:                   2.485\n",
      "Prob(Omnibus):                  0.409   Jarque-Bera (JB):                1.182\n",
      "Skew:                           0.632   Prob(JB):                        0.554\n",
      "Kurtosis:                       2.732   Cond. No.                         11.8\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andersonh\\Miniconda3\\envs\\PFDA_Month_7\\lib\\site-packages\\pandas\\core\\arraylike.py:397: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\andersonh\\Miniconda3\\envs\\PFDA_Month_7\\lib\\site-packages\\pandas\\core\\arraylike.py:397: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\andersonh\\Miniconda3\\envs\\PFDA_Month_7\\lib\\site-packages\\scipy\\stats\\_stats_py.py:1769: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=17\n",
      "  warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "lm = sm.OLS.from_formula(\n",
    "    data=X_df,\n",
    "    formula=\"Target ~ np.log1p(Col0) + np.exp(Col1) + np.sin(Col2) + np.sqrt(Col3 + Col4 + Col5) + Col6 + Col7 + Col8 + Col9 + 1\"\n",
    ")\n",
    "lm = lm.fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490a5cf-9d32-48c9-9325-8cf19d24c260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
