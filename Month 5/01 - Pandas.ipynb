{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddbbd4bc-574a-4901-b8cf-a6f367fd202c",
   "metadata": {},
   "source": [
    "# Pandas: Possibly the best way to work with tabular data\n",
    "\n",
    "Most of the data you'll ever work with can be represented as tabular data, with rows and columns.  CSV, Excel, Parquet, SQL, and even a lot of JSON data are all great examples.  The Pandas library provides a lot of extremely nice tools for manipulating and analyzing this kind of tabular data.  In many ways, Pandas is *the* Python libary for tabular data analysis--much like how Numpy is *the* Python library for a lot of numeric analysis.  (Numpy and Pandas are actually very similar--Pandas actually builds on top of Numpy, and adds extra functionality that's more specifically tailored to analyzing real-world datasets, whereas Numpy is highly specialized for purely numeric data).\n",
    "\n",
    "Install Pandas with:\n",
    "\n",
    "```bash\n",
    "conda install pandas\n",
    "```\n",
    "\n",
    "We'll also need the `openpyxl` library to read and write Excel files:\n",
    "\n",
    "```bash\n",
    "conda install openpyxl\n",
    "```\n",
    "\n",
    "NOTE: we'll be seeing a *lot* of stuff tonight, and we'll only be going over it very briefly.  There are a lot of functions and methods in Pandas that we will only just scratch the surface of.  Moreso than before, you *need* to spend some time with the Pandas documentation, learning the different arguments and how to use the different things we'll see tonight.\n",
    "\n",
    "# Basic design principles of Pandas\n",
    "\n",
    "Pandas is, like many libraries we've seen recently, *huge.*  There is a mind-boggling amount of stuff in it.  But, in terms of the basic building blocks, there are only really a few important concepts:\n",
    "\n",
    "- `DataFrame`s: the format your data gets converted into, and the thing you'll spend most of your time interacting with.  (R users: yes, this is the same kind of object as `data.frame`/`data.table`/`tibble`s!  In fact, it was originally inspired by R's `data.frame`, though in my opinion, Pandas' `DataFrame` have long since surpassed the offerings on R).\n",
    "- `Series`: the data structure used for storing columns and one-dimensional data.  They act like a cross between Numpy `array`s and Python `dict`ionaries.  You won't interact with these directly all that often.\n",
    "- Indexes: `DataFrame`s and `Series` objects have row indexes: unique values (or unique combinations of values) that identify each row.  These can be the cause of a lot of frustration and weird behavior when starting out.\n",
    "- Method chaining: this isn't unique to Pandas, but Pandas is very friendly to *method chaining.*  This is Python's equivalent of *piping* (e.g., using `magrittr`'s `%>%` in R, or `|` in shell/command line scripting).\n",
    "- There are a few new data types that Pandas introduces--like time and date formats, and categorical formats.\n",
    "\n",
    "If you understand the basics of `DataFrame`s, are aware of `Series` and indexes and the fact that there are new data types, and you're comfortable with the idea of method chaining, almost everything else in Pandas will boil down to learning how a particular function/method changes your `DataFrame`.\n",
    "\n",
    "## The `DataFrame`\n",
    "\n",
    "The `DataFrame` is the most important piece of the library, and is kind of what the whole library is built around.  Fortunately, the basic idea behind `DataFrame`s is very simple:  *a `DataFrame` should look and act like a `dict` of Numpy `array`s.*  While they store tabular data, and while you can (quite easily) access data by selecting specific rows, the \"basic\" interface to `DataFrame`s prioritizes easy *column* access.  This might sound a bit odd, but most of the time, this is how you want to interact with your tabular data anyways.\n",
    "\n",
    "Remember all the time we spent on Numpy last month?  Here's where it's going to come back: `DataFrame`s are almost fully compatible with Numpy (since they're ultimately built on Numpy's `array` objects).  This means all the fast Numpy math functions can be run on your `DataFrame`s!  In fact, a `DataFrame` containing just numeric data will often act like a Numpy `array` with named columns.  But *unlike* Numpy `array`s, `DataFrame`s are type-heterogeneous.  Sort of.  Every *column* must be the same type (like Numpy, Pandas calls these `dtype`s (for `d`ata `type`s).  But, one `DataFrame` can have multiple columns of different types.\n",
    "\n",
    "This also mean that most operations over `DataFrame`s are vectorized!\n",
    "\n",
    "## Indexes\n",
    "\n",
    "All Pandas objects have some notion of an *index*.  An index is just a set of unique identifiers for each *row* that can be used for very fast row-level lookups.  Indexes are usually columns in your `DataFrame`s, and there might be more than one column that goes into an index.  Indexes have a few important properties:\n",
    "- They are NOT accessed like normal columns.  You can't just index your `DataFrame` to get them.\n",
    "- You can convert your index columns to normal columns using the `.reset_index()` method.  You can convert normal columns to index columns using the `.set_index()` method.\n",
    "- They are OMITTED from most operations over `DataFrames`, e.g., if you calculate the per-column maximum, the indexes aren't used in that calculation.\n",
    "- Indexes must be UNIQUE for each row.  (if you have multiple columns serving as your index: the *combinations* must be unique).\n",
    "- Some ways of combining dataframes or adding columns can sometimes cause weird behavior if the indexes don't line up.  This can be a huge source of frustration.  Fortunately: you can usually get around it by calling `.reset_index()` \n",
    "\n",
    "This probably seems pretty abstract and esoteric at the moment, and that's fine.  Just remember: indexes are to rows what column names are to, well, columns.  And they can sometimes cause issues.\n",
    "\n",
    "## The `Series`\n",
    "\n",
    "This deserves only a brief mention.  Technically, a `DataFrame` is not a `dict`ionary of Numpy `array`s, but an *indexable collection of Pandas `Series` objects.*  (This difference is mostly in the implementation--if you think of them like `dict`ionaries of `array`s, you'll be fine, at least when getting started).  A Pandas `Series` is essentially just a one-dimensional Numpy `array`, but with an index you can use to access elements.  So, they act kind of like a weird mix of `list`s, Numpy `array`s, and `dict`ionaries.  You usually won't be dealing with `Series` objects directly, at least not at first, so they're not critical to understand just now.  But they're important to have on your radar.\n",
    "\n",
    "The one place they might cause problems, though, is if you're copying the columnd of one `DataFrame` into another.  Even if they have the same number of rows, the indexes might be different.\n",
    "\n",
    "## Method chaining\n",
    "\n",
    "Most of the transformations and manipulations we do with `DataFrame`s are done through method calls.  Pretty much every method call on a `DataFrame` will return a new `DataFrame`, meaning you can do something called *method chaining.*  Consider the following example with strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778325ef-47e8-4022-9f32-7bf6252a4f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001000 1100101 1101100 1101100 1101111 100001\n",
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "my_string = \"1001000 1100101 1101100 1101100 1101111 100001\"\n",
    "print(my_string)\n",
    "my_string = (\n",
    "    my_string\n",
    "    .replace(\"1001000\", \"H\")\n",
    "    .replace(\"1100101\", \"e\")\n",
    "    .replace(\"1101100\", \"l\")\n",
    "    .replace(\"1101111\", \"o\")\n",
    "    .replace(\"100001\", \"!\")\n",
    "    .replace(\" \", \"\")\n",
    ")\n",
    "print(my_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a214d72-f826-473e-a094-eab275f6e7f8",
   "metadata": {},
   "source": [
    "(Note: the parentheses starting on the third line are syntactically important here.  They tell Python that everything inside of them is a single expression, even if it's spread across multiple lines.  This lets us avoid having one very, very long line with a lot of .replace() calls, and it lets us not have to use an ugly backslash at the end of every line to tell Python \"the expression continues on the next line\").\n",
    "\n",
    "Every time we call `my_string.replace(...)`, we get a brand new `str`ing back.  So, we can just tack another `.replace(...)` right after that expression.  Remember that Python essentially evaluates these expressions \"from the inside out\": `my_string.replace(...)` gets evaluated, and then replaced with whatever its result (i.e., its *return value*) is.  Since the return value is a string, we now have `[a string].replace(...)`, and the process repeats.\n",
    "\n",
    "This paradigm is called *method chaining,* since we're \"chaining\" multiple method calls, one after another, without stopping.  We *could* write the code like this:\n",
    "\n",
    "```python\n",
    "my_string = \"1001000 1100101 1101100 1101100 1101111 100001\"\n",
    "my_string = my_string.replace(\"1001000\", \"H\")\n",
    "my_string = my_string.replace(\"1100101\", \"e\")\n",
    "[etc]\n",
    "```\n",
    "\n",
    "And we would get the exact same result.  In most real-world cases, there isn't much practical reason to prefer either of these approaches.  I personally love method chaining, so I'll be using it throughout this notebook where I can.\n",
    "\n",
    "Another note for R users: if you've used the `magrittr` package for the pipe operatig, `%>%`, this will look very familiar.  Piping and method chaining are the same thing, just in slightly different clothing.\n",
    "\n",
    "## Some new data types\n",
    "\n",
    "Pandas introduces a few new datatypes and special values, but there are three you're likely to encounter.\n",
    "\n",
    "1. Pandas adds support for R-style missing/NA values.  These are *not* like IEEE/Numpy-style `NaN` values; these are proper, data analysis-style missing values.  In most cases, Pandas will treat `NaN` like a missing value.\n",
    "2. Date and time formats.  These are extremely useful.\n",
    "3. A dedicated categorical data type.  I don't use these very often, but they can be very useful.  E.g.: some libraries will look for Pandas categorical data and handle it separately.  If you have a very large dataset, this can also save memory compared to storing strings or other types.\n",
    "\n",
    "## And that's really about it.\n",
    "\n",
    "Conceptually, those are the big concepts in Pandas!  We're going to create `DataFrame`s, do most of our work by calling a bunch of methods or Numpy functions on them, and occasionally run into issues with `Series` and indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54774aa-d341-420b-8bd1-31fc99498137",
   "metadata": {},
   "source": [
    "# Quick-start\n",
    "\n",
    "Let's do a very quick run through of some common, foundational tasks that you'll do all the time in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aff3909-c828-48e5-beb8-921eaed16c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                  5.1               3.5                1.4               0.2   \n",
      "1                  4.9               3.0                1.4               0.2   \n",
      "2                  4.7               3.2                1.3               0.2   \n",
      "3                  4.6               3.1                1.5               0.2   \n",
      "4                  5.0               3.6                1.4               0.2   \n",
      "..                 ...               ...                ...               ...   \n",
      "145                6.7               3.0                5.2               2.3   \n",
      "146                6.3               2.5                5.0               1.9   \n",
      "147                6.5               3.0                5.2               2.0   \n",
      "148                6.2               3.4                5.4               2.3   \n",
      "149                5.9               3.0                5.1               1.8   \n",
      "\n",
      "       species  \n",
      "0       setosa  \n",
      "1       setosa  \n",
      "2       setosa  \n",
      "3       setosa  \n",
      "4       setosa  \n",
      "..         ...  \n",
      "145  virginica  \n",
      "146  virginica  \n",
      "147  virginica  \n",
      "148  virginica  \n",
      "149  virginica  \n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from a csv file.\n",
    "# `df` is a standard variable name when you only have one\n",
    "# dataframe in the current scope.\n",
    "df = pd.read_csv(\"iris.csv\")\n",
    "\n",
    "# Pandas integrates nicely with Jupyter--but it will pretty-print to a text\n",
    "# console too.\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ca61c3-464b-4436-8e8f-75608aa5d8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm)                                                876.5\n",
      "sepal width (cm)                                                 458.6\n",
      "petal length (cm)                                                563.7\n",
      "petal width (cm)                                                 179.9\n",
      "species              setosasetosasetosasetosasetosasetosasetosaseto...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Select just one column and take its sum\n",
    "# print(df[\"sepal length (cm)\"].sum())\n",
    "print(df.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eecc1056-bfab-4725-973a-b8590be955ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                  5.1               3.5                1.4               0.2   \n",
      "1                  4.9               3.0                1.4               0.2   \n",
      "2                  4.7               3.2                1.3               0.2   \n",
      "3                  4.6               3.1                1.5               0.2   \n",
      "4                  5.0               3.6                1.4               0.2   \n",
      "..                 ...               ...                ...               ...   \n",
      "145                6.7               3.0                5.2               2.3   \n",
      "146                6.3               2.5                5.0               1.9   \n",
      "147                6.5               3.0                5.2               2.0   \n",
      "148                6.2               3.4                5.4               2.3   \n",
      "149                5.9               3.0                5.1               1.8   \n",
      "\n",
      "       species  New Column  \n",
      "0       setosa         100  \n",
      "1       setosa         100  \n",
      "2       setosa         100  \n",
      "3       setosa         100  \n",
      "4       setosa         100  \n",
      "..         ...         ...  \n",
      "145  virginica         100  \n",
      "146  virginica         100  \n",
      "147  virginica         100  \n",
      "148  virginica         100  \n",
      "149  virginica         100  \n",
      "\n",
      "[150 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Add a new column.  Setting it equal to a single value will\n",
    "# vectorize the assignment operation.\n",
    "df[\"New Column\"] = 100\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "743ffa95-6bf0-452b-a5d1-6f08f5d48a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                  5.1               3.5                1.4               0.2   \n",
      "1                  4.9               3.0                1.4               0.2   \n",
      "2                  4.7               3.2                1.3               0.2   \n",
      "3                  4.6               3.1                1.5               0.2   \n",
      "4                  5.0               3.6                1.4               0.2   \n",
      "..                 ...               ...                ...               ...   \n",
      "145                6.7               3.0                5.2               2.3   \n",
      "146                6.3               2.5                5.0               1.9   \n",
      "147                6.5               3.0                5.2               2.0   \n",
      "148                6.2               3.4                5.4               2.3   \n",
      "149                5.9               3.0                5.1               1.8   \n",
      "\n",
      "       species  New Column  \n",
      "0       setosa        10.0  \n",
      "1       setosa        10.0  \n",
      "2       setosa        10.0  \n",
      "3       setosa        10.0  \n",
      "4       setosa        10.0  \n",
      "..         ...         ...  \n",
      "145  virginica        10.0  \n",
      "146  virginica        10.0  \n",
      "147  virginica        10.0  \n",
      "148  virginica        10.0  \n",
      "149  virginica        10.0  \n",
      "\n",
      "[150 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Update a column\n",
    "df[\"New Column\"] = df[\"New Column\"] / 10\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b45b791c-01db-4abf-9d19-87858a3e2da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm)          7.9\n",
      "sepal width (cm)           4.4\n",
      "petal length (cm)          6.9\n",
      "petal width (cm)           2.5\n",
      "species              virginica\n",
      "New Column                10.0\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Take the maximum of each column; this will return a Series.\n",
    "print(df.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72750cf6-f0bf-495e-b8ee-ed2466bfbd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      10.0\n",
      "1      10.0\n",
      "2      10.0\n",
      "3      10.0\n",
      "4      10.0\n",
      "       ... \n",
      "145    10.0\n",
      "146    10.0\n",
      "147    10.0\n",
      "148    10.0\n",
      "149    10.0\n",
      "Length: 150, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andersonh\\AppData\\Local\\Temp\\ipykernel_2416\\1085101789.py:4: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  print(df.max(axis=1))\n"
     ]
    }
   ],
   "source": [
    "# Or: take the maximum of rows.  the `axis=` keyword behaves just like\n",
    "# it does in Numpy.  (the default value is 0, i.e., calculate values\n",
    "# within columns).\n",
    "print(df.max(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e15659b-72cb-47b9-bbf4-05fc34738c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a CSV.\n",
    "df.to_csv(\"New Data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7256ba2-fc15-4b31-bd6c-38643c8299c9",
   "metadata": {},
   "source": [
    "# An example analysis: highest paid industries in Texas\n",
    "\n",
    "Let's do a real-world analysis and see what it looks like in Pandas.  We'll be grabbing some data from the US Census Bureau (fun fact: Pandas can download data over the internet, very easily!) that tracks employment and payroll information for different industries, at the county-level.  We'll select just Texas, and identify the counties and professions with the highest average annual payroll.\n",
    "\n",
    "This will involve a lot of basic data manipulation in Pandas, which we'll talk about as we see them.\n",
    "\n",
    "First, we need to get the data.  The cell below passes a URL to the `pd.read_csv()` function, which Pandas will identify as a URL, and try to download a file from.  There's a little bit of logic to check if the file has already been downloaded and saved, to avoid re-downloading the same file multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd1d3112-2de8-4921-b4dc-67691093a324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         fipstate  fipscty   naics emp_nf    emp qp1_nf    qp1 ap_nf      ap  \\\n",
      "0               1        1  ------      G  11265      G  94865     G  385785   \n",
      "1               1        1  11----      H     92      G   1183     H    5232   \n",
      "2               1        1  113///      H     82      G   1075     G    4741   \n",
      "3               1        1  1133//      H     82      G   1075     G    4741   \n",
      "4               1        1  11331/      H     82      G   1075     G    4741   \n",
      "...           ...      ...     ...    ...    ...    ...    ...   ...     ...   \n",
      "1082429        56      999  611310      J     25      G    102     G     396   \n",
      "1082430        56      999  62----      J     24      H    354     H    1327   \n",
      "1082431        56      999  621///      J     24      H    354     H    1327   \n",
      "1082432        56      999  81----      J     21      H    522     H    2789   \n",
      "1082433        56      999  813///      G     12      G    236     G     766   \n",
      "\n",
      "         est  ... n20_49 n50_99 n100_249 n250_499 n500_999 n1000 n1000_1  \\\n",
      "0        879  ...    104     32        9        N        N     N       N   \n",
      "1         10  ...      N      N        N        N        N     N       N   \n",
      "2          7  ...      N      N        N        N        N     N       N   \n",
      "3          7  ...      N      N        N        N        N     N       N   \n",
      "4          7  ...      N      N        N        N        N     N       N   \n",
      "...      ...  ...    ...    ...      ...      ...      ...   ...     ...   \n",
      "1082429    4  ...      N      N        N        N        N     N       N   \n",
      "1082430    4  ...      N      N        N        N        N     N       N   \n",
      "1082431    4  ...      N      N        N        N        N     N       N   \n",
      "1082432    6  ...      N      N        N        N        N     N       N   \n",
      "1082433    4  ...      N      N        N        N        N     N       N   \n",
      "\n",
      "        n1000_2 n1000_3 n1000_4  \n",
      "0             N       N       N  \n",
      "1             N       N       N  \n",
      "2             N       N       N  \n",
      "3             N       N       N  \n",
      "4             N       N       N  \n",
      "...         ...     ...     ...  \n",
      "1082429       N       N       N  \n",
      "1082430       N       N       N  \n",
      "1082431       N       N       N  \n",
      "1082432       N       N       N  \n",
      "1082433       N       N       N  \n",
      "\n",
      "[1082434 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "if not os.path.isfile(\"Census Data.csv\"):\n",
    "    census_data = pd.read_csv(\n",
    "        r\"https://www2.census.gov/programs-surveys/cbp/datasets/2020/cbp20co.zip\",\n",
    "        compression=\"zip\"\n",
    "    )\n",
    "    census_data.to_csv(\"Census Data.csv\", index=False)\n",
    "else:\n",
    "    census_data = pd.read_csv(\"Census Data.csv\")\n",
    "\n",
    "print(census_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf876f4-a508-43bc-9f1f-92d39666641c",
   "metadata": {},
   "source": [
    "Note how the far left column doesn't have a name.  This is the *index* of the `DataFrame`, and every `DataFrame` has one.  If you're loading or creating data without explicitly providing an index, Pandas creates one for you, using the row's position (starting from 0, like everything in Python) as the index value.\n",
    "\n",
    "Also note that the very last line printed out the number of rows and columns.  We can also check the number of rows and columns using the `.shape` attribute, just like for a Numpy array.  `DataFrame.shape` returns a tuple containing `(num_rows, num_cols)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf04262e-1090-43e0-9601-d3d3c93cc690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1082434, 23)\n"
     ]
    }
   ],
   "source": [
    "print(census_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3358c4b8-f13e-4b45-be28-137d7b7e0169",
   "metadata": {},
   "source": [
    "Let's just pull out the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b6e5900-d293-4b52-8aca-375d755047d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fipstate', 'fipscty', 'naics', 'emp_nf', 'emp', 'qp1_nf', 'qp1',\n",
      "       'ap_nf', 'ap', 'est', 'n<5', 'n5_9', 'n10_19', 'n20_49', 'n50_99',\n",
      "       'n100_249', 'n250_499', 'n500_999', 'n1000', 'n1000_1', 'n1000_2',\n",
      "       'n1000_3', 'n1000_4'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(census_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5dd20-1fa1-475d-9974-6f7bb6ad28c9",
   "metadata": {},
   "source": [
    "Most of these are not very easy to read, so let's rename them.  The `.rename(columns={\"old name\": \"new name\"})` method gives us a nice way to easily do this.  Like most method calls on dataframes, this *returns a new dataframe*--Pandas will essentially never update a `DataFrame` \"in place\" unless you tell it to.  (You can see the definitions of these columns [in the Census Bureau's documentation](https://www2.census.gov/programs-surveys/cbp/technical-documentation/records-layouts/2020_record_layouts/county-layout-2020.txt) for the dataset we're pulling).\n",
    "\n",
    "Two important things to know about these columns that aren't immediately apparent from that documentation:\n",
    "\n",
    "1. \"FIPS\" is the Federal Information Processing Standard.  Basically, it's a set of standard numeric codes associated with different localities, e.g. states and counties.\n",
    "2. NAICS is the North American Industry Classification System.  It's kind of like a Library of Congress classification system, but for different industrial sectors.  E.g., there are numeric codes for agriculture, engineering, etc.\n",
    "\n",
    "There are also a few columns we're going to ignore for our analysis, namely, the first-quarter payroll, the `_nf` columns, and the columns at the end starting with `n`.  So we'll drop them with `DataFrame.drop(columns=[\"column name\", \"column name\", ...])`.  (you can also use `.drop()` to drop values by their index, but that's pretty uncommon in my experience).\n",
    "\n",
    "Since we need to do two processing steps in sequence, we'll chain the method calls together.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a9d9cd4-57d8-4f89-86cc-be2956895c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         State FIPS Code  County FIPS Code Industry NAICS Code  \\\n",
      "0                      1                 1              ------   \n",
      "1                      1                 1              11----   \n",
      "2                      1                 1              113///   \n",
      "3                      1                 1              1133//   \n",
      "4                      1                 1              11331/   \n",
      "...                  ...               ...                 ...   \n",
      "1082429               56               999              611310   \n",
      "1082430               56               999              62----   \n",
      "1082431               56               999              621///   \n",
      "1082432               56               999              81----   \n",
      "1082433               56               999              813///   \n",
      "\n",
      "         Number of Employees  Annual Payroll  Number of Establishments  \n",
      "0                      11265          385785                       879  \n",
      "1                         92            5232                        10  \n",
      "2                         82            4741                         7  \n",
      "3                         82            4741                         7  \n",
      "4                         82            4741                         7  \n",
      "...                      ...             ...                       ...  \n",
      "1082429                   25             396                         4  \n",
      "1082430                   24            1327                         4  \n",
      "1082431                   24            1327                         4  \n",
      "1082432                   21            2789                         6  \n",
      "1082433                   12             766                         4  \n",
      "\n",
      "[1082434 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "census_data = (\n",
    "    census_data\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"fipstate\": \"State FIPS Code\",\n",
    "            \"fipscty\": \"County FIPS Code\",\n",
    "            \"naics\": \"Industry NAICS Code\",\n",
    "            \"emp\": \"Number of Employees\",\n",
    "            \"ap\": \"Annual Payroll\",\n",
    "            \"est\": \"Number of Establishments\"\n",
    "        }\n",
    "    )\n",
    "    .drop(\n",
    "        columns=[\n",
    "            \"qp1\",\n",
    "            \"emp_nf\",\n",
    "            \"qp1_nf\",\n",
    "            \"ap_nf\",\n",
    "            \"n<5\",\n",
    "            \"n5_9\",\n",
    "            \"n10_19\",\n",
    "            \"n20_49\",\n",
    "            \"n50_99\",\n",
    "            \"n100_249\",\n",
    "            \"n250_499\",\n",
    "            \"n500_999\",\n",
    "            \"n1000\",\n",
    "            \"n1000_1\",\n",
    "            \"n1000_2\",\n",
    "            \"n1000_3\",\n",
    "            \"n1000_4\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "print(census_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe31897-18ee-462b-b813-71d0aebea066",
   "metadata": {},
   "source": [
    "Now, we need to filter down to just Texas counties.  Texas' state FIPS code is 48, so we want to select only rows where the `\"State FIPS Code\"` column is 48.  This is going to look complicated at first, but it's built out of things we've already seen before (and should look *very* similar to some of the Numpy stuff from last month).\n",
    "\n",
    "First: remember that, like Numpy `array`s, Pandas `DataFrame`s support *vectorized operations.*\n",
    "\n",
    "Second: we need to talk about indexing `DataFrames`.  See the `01a - Pandas - Indexing Dataframes.ipynb` file; I've put all the how-to over there to keep this notebook clean.\n",
    "\n",
    "We want to only keep the rows where the state FIPS code is 48.  So, let's just make a subquery, and then check how many rows we got rid of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d72a774-8bae-473d-bbec-65d4809c4aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1009688\n"
     ]
    }
   ],
   "source": [
    "before_rows = census_data.shape[0]\n",
    "census_data = census_data[census_data[\"State FIPS Code\"] == 48]\n",
    "after_rows = census_data.shape[0]\n",
    "print(before_rows - after_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa24093b-0a0c-46c3-a41c-00254c01c81e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we can get rid of the `\"State FIPS Code\"` column, since we don't need it any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36c2d6fb-f0e9-44d0-bca7-e312b30c176a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        County FIPS Code Industry NAICS Code  Number of Employees  \\\n",
      "882454                 1              ------                11735   \n",
      "882455                 1              21----                  512   \n",
      "882456                 1              211///                    6   \n",
      "882457                 1              2111//                    6   \n",
      "882458                 1              21112/                    5   \n",
      "...                  ...                 ...                  ...   \n",
      "955195               999              813///                   70   \n",
      "955196               999              8132//                   46   \n",
      "955197               999              81321/                   46   \n",
      "955198               999              8133//                   21   \n",
      "955199               999              81331/                   21   \n",
      "\n",
      "        Annual Payroll  Number of Establishments  \n",
      "882454          451573                       880  \n",
      "882455           31901                        29  \n",
      "882456             671                         6  \n",
      "882457             671                         6  \n",
      "882458             635                         4  \n",
      "...                ...                       ...  \n",
      "955195            5292                         8  \n",
      "955196            2976                         4  \n",
      "955197            2976                         4  \n",
      "955198            2048                         3  \n",
      "955199            2048                         3  \n",
      "\n",
      "[72746 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "census_data = census_data.drop(columns=[\"State FIPS Code\"])\n",
    "print(census_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d732818e-9d21-4d0a-8993-83046919b3f5",
   "metadata": {},
   "source": [
    "## Some additional cleanup\n",
    "\n",
    "Some of the columns in our data are nonsensical numeric codes, most notable the county (which is still in the county FIPS code format) and the NAICS codes, which are used to indicate the kind of industry a business operates in (e.g., agriculture versus manufacturing).  Let's clean these columns up.\n",
    "\n",
    "First, the county FIPS codes.  I've grabbed a list of all the county-level Texas FIPS codes from [Wikipedia](https://en.wikipedia.org/wiki/List_of_United_States_FIPS_codes_by_county) and turned them into a CSV file, which is in the same directory as this notebook.  Let's load it up and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbb8c8ec-2953-48d4-8521-b76629dfdebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       County FIPS Code Industry NAICS Code  Number of Employees  \\\n",
      "0                     1              ------                11735   \n",
      "1                     1              21----                  512   \n",
      "2                     1              211///                    6   \n",
      "3                     1              2111//                    6   \n",
      "4                     1              21112/                    5   \n",
      "...                 ...                 ...                  ...   \n",
      "72741               999              813///                   70   \n",
      "72742               999              8132//                   46   \n",
      "72743               999              81321/                   46   \n",
      "72744               999              8133//                   21   \n",
      "72745               999              81331/                   21   \n",
      "\n",
      "       Annual Payroll  Number of Establishments  \n",
      "0              451573                       880  \n",
      "1               31901                        29  \n",
      "2                 671                         6  \n",
      "3                 671                         6  \n",
      "4                 635                         4  \n",
      "...               ...                       ...  \n",
      "72741            5292                         8  \n",
      "72742            2976                         4  \n",
      "72743            2976                         4  \n",
      "72744            2048                         3  \n",
      "72745            2048                         3  \n",
      "\n",
      "[72746 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "census_data = census_data.reset_index(drop=True)\n",
    "print(census_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "563b477e-150e-4f9f-b98c-bd6b2fae0fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     FIPS Code      County Name\n",
      "0        48001  Anderson County\n",
      "1        48003   Andrews County\n",
      "2        48005  Angelina County\n",
      "3        48007   Aransas County\n",
      "4        48009    Archer County\n",
      "..         ...              ...\n",
      "249      48499      Wood County\n",
      "250      48501    Yoakum County\n",
      "251      48503     Young County\n",
      "252      48505    Zapata County\n",
      "253      48507    Zavala County\n",
      "\n",
      "[254 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "counties = pd.read_csv(\"County FIPS codes.csv\")\n",
    "print(counties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341a838-5df8-46d0-bf5c-30c7a75653c8",
   "metadata": {},
   "source": [
    "Something to note: all of the county-level FIPS codes have the state FIPS code prepended to them.  If we want to combine these two datasets to get the county names, we'll need to make sure the FIPS codes are identical in both datasets.  We can either prepend 48 to the beginning of the FIPS codes in the census data, or remove it from the FIPS codes in the data we just loaded.  (Though, since they're numeric values, we'd need to add/subtract 48,000 to get this effect, but that's easy to do).  It doesn't really matter which way we go, so I'll remove the state FIPS code from the data we just loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b15def8-5c5f-4bd3-b837-4937c8078bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     FIPS Code      County Name\n",
      "0            1  Anderson County\n",
      "1            3   Andrews County\n",
      "2            5  Angelina County\n",
      "3            7   Aransas County\n",
      "4            9    Archer County\n",
      "..         ...              ...\n",
      "249        499      Wood County\n",
      "250        501    Yoakum County\n",
      "251        503     Young County\n",
      "252        505    Zapata County\n",
      "253        507    Zavala County\n",
      "\n",
      "[254 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "counties[\"FIPS Code\"] = counties[\"FIPS Code\"] - 48000\n",
    "print(counties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297bbd1-9a56-460b-8297-818a68ce839d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, we have to figure out how we get this data into our census data `DataFrame`.  There's a very nice solution to this: database-style joins (aka *merges*).  Those of you with some SQL background should be familiar with things like left joins and inner joins; Pandas supports these pretty well, but it's harder to specify super detailed join conditions compared to SQL.\n",
    "\n",
    "Pandas has veyr good support for *database-style joins* for combining multiple `DataFrames` together.  For details, see the `01 - Pandas - Database joins.ipynb` file.\n",
    "\n",
    "For this analysis, we have a `DataFrame` containing our FIPS codes and the corresponding counties, and a `DataFrame` with our main census data.  We need to combine the two.  So, let's just do a join.  But: notice that there are some county FIPS codes in the census data that are 999.  So we'll have to decide ho to deal with those, since they don't appear in the county data from Wikipedia.  Let's check how many there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b0cf0e1-611d-4bc6-a867-eaea24bd8d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201    1704\n",
      "113    1616\n",
      "439    1503\n",
      "453    1400\n",
      "29     1396\n",
      "       ... \n",
      "301       5\n",
      "345       4\n",
      "263       2\n",
      "443       2\n",
      "33        1\n",
      "Name: County FIPS Code, Length: 254, dtype: int64\n",
      "[  1   3   5   7   9  11  13  15  17  19  21  23  25  27  29  31  33  35\n",
      "  37  39  41  43  45  47  49  51  53  55  57  59  61  63  65  67  69  71\n",
      "  73  75  77  79  81  83  85  87  89  91  93  95  97  99 101 103 105 107\n",
      " 109 111 113 115 117 119 121 123 125 127 129 131 133 135 137 139 141 143\n",
      " 145 147 149 151 153 155 157 159 161 163 165 167 169 171 173 175 177 179\n",
      " 181 183 185 187 189 191 193 195 197 199 201 203 205 207 209 211 213 215\n",
      " 217 219 221 223 225 227 229 231 233 235 237 239 241 243 245 247 249 251\n",
      " 253 255 257 259 261 263 265 267 271 273 275 277 279 281 283 285 287 289\n",
      " 291 293 295 297 299 301 303 305 307 309 311 313 315 317 319 321 323 325\n",
      " 327 329 331 333 335 337 339 341 343 345 347 349 351 353 355 357 359 361\n",
      " 363 365 367 369 371 373 375 377 379 381 383 385 387 389 391 393 395 397\n",
      " 399 401 403 405 407 409 411 413 415 417 419 421 423 425 427 429 431 433\n",
      " 435 437 439 441 443 445 447 449 451 453 455 457 459 461 463 465 467 469\n",
      " 471 473 475 477 479 481 483 485 487 489 491 493 495 497 499 501 503 505\n",
      " 507 999]\n"
     ]
    }
   ],
   "source": [
    "# `df[column].value_counts()` --> prints each value and how many times it appears\n",
    "print(census_data[\"County FIPS Code\"].value_counts())\n",
    "print(census_data[\"County FIPS Code\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edf3ee11-a777-4779-a893-96e7eeced07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277\n",
      "72746\n"
     ]
    }
   ],
   "source": [
    "# Hm, that's kind of hard to read.  Well, this returns a `pd.Series`, which we can index\n",
    "# like a dictionary.\n",
    "print(census_data[\"County FIPS Code\"].value_counts()[999])\n",
    "print(census_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a087a-68f6-401a-ab8a-9cd26580ae53",
   "metadata": {},
   "source": [
    "277 out of a few tens of thousands of rows isn't a whole lot.  But, just to be sure, let's check how many businesses and employees these entries account for.  If it's not a lot, we'll use an inner join and drop these records.  Otherwise, we'll do a left join and keep them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ec79ebc-7ab4-4743-9827-21235ccddd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Employees         2924477\n",
      "Number of Establishments       8058\n",
      "dtype: int64\n",
      "\n",
      "Number of Employees         62365725\n",
      "Number of Establishments     3602504\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    census_data[\n",
    "        census_data[\"County FIPS Code\"] == 999\n",
    "    ][\n",
    "        [\"Number of Employees\", \"Number of Establishments\"]\n",
    "    ]\n",
    "    .sum()\n",
    ")\n",
    "print()\n",
    "print(\n",
    "    census_data[\n",
    "        census_data[\"County FIPS Code\"] != 999\n",
    "    ][\n",
    "        [\"Number of Employees\", \"Number of Establishments\"]\n",
    "    ]\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a95016f-b785-469f-a4a7-bfb1c36b6254",
   "metadata": {},
   "source": [
    "Hm, that's about 5% of the total employee count.  Let's use a left join and leave these in, just to be safe.  We'll also call `.drop()` to drop the FIPS code columns after the merge, since we don't need them--we only need the county names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b79973f2-93b8-4648-a16e-a16e83858c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Industry NAICS Code  Number of Employees  Annual Payroll  \\\n",
      "0                  ------                11735          451573   \n",
      "1                  21----                  512           31901   \n",
      "2                  211///                    6             671   \n",
      "3                  2111//                    6             671   \n",
      "4                  21112/                    5             635   \n",
      "...                   ...                  ...             ...   \n",
      "72741              813///                   70            5292   \n",
      "72742              8132//                   46            2976   \n",
      "72743              81321/                   46            2976   \n",
      "72744              8133//                   21            2048   \n",
      "72745              81331/                   21            2048   \n",
      "\n",
      "       Number of Establishments      County Name  \n",
      "0                           880  Anderson County  \n",
      "1                            29  Anderson County  \n",
      "2                             6  Anderson County  \n",
      "3                             6  Anderson County  \n",
      "4                             4  Anderson County  \n",
      "...                         ...              ...  \n",
      "72741                         8              NaN  \n",
      "72742                         4              NaN  \n",
      "72743                         4              NaN  \n",
      "72744                         3              NaN  \n",
      "72745                         3              NaN  \n",
      "\n",
      "[72746 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "census_data = pd.merge(\n",
    "    left=census_data,\n",
    "    right=counties,\n",
    "    left_on=\"County FIPS Code\",\n",
    "    right_on=\"FIPS Code\",\n",
    "    how=\"left\",\n",
    ").drop(\n",
    "    columns=[\"County FIPS Code\", \"FIPS Code\"]\n",
    ")\n",
    "print(census_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843b898b-23e3-4a1a-859f-c62025a74dff",
   "metadata": {},
   "source": [
    "Note how we have a lot of `NaN` values at the bottom of our \"County Name\" column.  Most things in Pandas will skip over `NaN` values, which might be a problem when we do a by-county analysis.  Let's fill the missing vaues with `\"[Unknown]\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51e10733-b140-4e2e-9a0a-a6463f1709d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Industry NAICS Code  Number of Employees  Annual Payroll  \\\n",
      "0                  ------                11735          451573   \n",
      "1                  21----                  512           31901   \n",
      "2                  211///                    6             671   \n",
      "3                  2111//                    6             671   \n",
      "4                  21112/                    5             635   \n",
      "...                   ...                  ...             ...   \n",
      "72741              813///                   70            5292   \n",
      "72742              8132//                   46            2976   \n",
      "72743              81321/                   46            2976   \n",
      "72744              8133//                   21            2048   \n",
      "72745              81331/                   21            2048   \n",
      "\n",
      "       Number of Establishments      County Name  \n",
      "0                           880  Anderson County  \n",
      "1                            29  Anderson County  \n",
      "2                             6  Anderson County  \n",
      "3                             6  Anderson County  \n",
      "4                             4  Anderson County  \n",
      "...                         ...              ...  \n",
      "72741                         8        [Unknown]  \n",
      "72742                         4        [Unknown]  \n",
      "72743                         4        [Unknown]  \n",
      "72744                         3        [Unknown]  \n",
      "72745                         3        [Unknown]  \n",
      "\n",
      "[72746 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "census_data[\"County Name\"] = census_data[\"County Name\"].fillna(\"[Unknown]\")\n",
    "print(census_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43be0021-987c-43f3-9016-2f9e430cfd05",
   "metadata": {},
   "source": [
    "If we want to remove these rows from the analysis later, we can always use a subquery.\n",
    "\n",
    "Next up, we'll do the same thing with the NAICS codes.  Load the data, do a bit of cleaning, and left join so that we keep any unknown codes.  The data for NAICS codes needs to be loaded a bit differently; [the file from the Census Bureau](https://www.census.gov/naics/2022NAICS/2022_NAICS_Structure.xlsx) containing all the codes has two extra rows at the top that we want to skip, but that's easy to do in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a9089dc-54e4-4cfb-a895-df487253e810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Change Indicator 2022 NAICS Code  \\\n",
      "0                 NaN              11   \n",
      "1                 NaN             111   \n",
      "2                 NaN            1111   \n",
      "3                 NaN           11111   \n",
      "4                 NaN          111110   \n",
      "...               ...             ...   \n",
      "2139              NaN            9281   \n",
      "2140              NaN           92811   \n",
      "2141              NaN          928110   \n",
      "2142              NaN           92812   \n",
      "2143              NaN          928120   \n",
      "\n",
      "                                  2022 NAICS Title  \n",
      "0      Agriculture, Forestry, Fishing and HuntingT  \n",
      "1                                Crop ProductionT   \n",
      "2                       Oilseed and Grain FarmingT  \n",
      "3                                 Soybean FarmingT  \n",
      "4                                  Soybean Farming  \n",
      "...                                            ...  \n",
      "2139  National Security and International Affairs   \n",
      "2140                            National Security   \n",
      "2141                            National Security   \n",
      "2142                        International Affairs   \n",
      "2143                        International Affairs   \n",
      "\n",
      "[2144 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "naics = pd.read_excel(\n",
    "    \"https://www.census.gov/naics/2022NAICS/2022_NAICS_Structure.xlsx\",\n",
    "    # skiprows= takes a list of positions for rows to skip; [0,1] means \"skip the first\n",
    "    # and second row\".  Rows get skipped *before* any parsing of the data, e.g., before\n",
    "    # looking at the first row to determine column headers.\n",
    "    skiprows=[0, 1]\n",
    ")\n",
    "print(naics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ee00c-ab33-470b-b192-2f8c18fa5633",
   "metadata": {},
   "source": [
    "There's a bit of cleanup we'll need to do:\n",
    "\n",
    "1. Drop the Change Indicator column--we don't care about it for this analysis.\n",
    "2. The \"2022 NAICS Title\" column has some random capital Ts at the end of some of the entries.  (these are superscripted in the original text, but Pandas un-superscripts them.\n",
    "3. The NAICS codes here are just numbers; the NAICS codes in the main dataset have slashes at the end when the number is less than 6 digits long.  Like wih the FIPS codes, we'll need to do something to make both of these match.  I'm going to change the above `DataFrame` to match the census data's conventions, rather than the other way around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf22365b-8f0d-4dfa-a63f-68d2b0757bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     2022 NAICS Code                              2022 NAICS Title\n",
      "0             11////   Agriculture, Forestry, Fishing and HuntingT\n",
      "1             111///                             Crop ProductionT \n",
      "2             1111//                    Oilseed and Grain FarmingT\n",
      "3             11111/                              Soybean FarmingT\n",
      "4             111110                               Soybean Farming\n",
      "...              ...                                           ...\n",
      "2139          9281//  National Security and International Affairs \n",
      "2140          92811/                            National Security \n",
      "2141          928110                            National Security \n",
      "2142          92812/                        International Affairs \n",
      "2143          928120                        International Affairs \n",
      "\n",
      "[2144 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Drop the Change Indicator column.\n",
    "naics = naics.drop(columns=[\"Change Indicator\"])\n",
    "\n",
    "# Convert the 6-digit codes to \"/\"-padded strings.\n",
    "naics[\"2022 NAICS Code\"] = [\n",
    "    # \"/<6\" is doing a lot of stuff:\n",
    "    #    - \"<6\" means \"if the value `i` is less than 6 characters long, \n",
    "    #      pad it to 6 characters and left-align it.  (i.e.: add padding\n",
    "    #      at the right end).\n",
    "    #    - \"/\" (or any other character you put to the left of the \">\") means\n",
    "    #      that \"/\" will be used as the padding character instead of spaces.\n",
    "    f\"{i:/<6}\" for i in naics[\"2022 NAICS Code\"]\n",
    "]\n",
    "print(naics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf04afd4-9570-4a91-a042-beb0a9ddb6d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Remove the random \"T\"s from the end of the NAICS titles.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m naics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2022 NAICS Title\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# This is something called a \"ternary expression\".  It's super handy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# for list comprehension.  General syntax: `VAL if CONDITION else VAL`\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     i[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m i[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m i\n\u001b[0;32m      6\u001b[0m     \n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# DataFrame[column].apply(some_function) --> calls some_function() on every\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# value in the column, and returns a new Series with the results.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Here I'm using an anonymous Lamnda function that just removed leading/trailing\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# whitespaces, since some of the entries have these (which I discovered the\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# hard way when prepping this code).\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnaics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2022 NAICS Title\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m ]\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\PythonForDataAnalysis\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\PythonForDataAnalysis\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\PythonForDataAnalysis\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\PythonForDataAnalysis\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Remove the random \"T\"s from the end of the NAICS titles.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m naics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2022 NAICS Title\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# This is something called a \"ternary expression\".  It's super handy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# for list comprehension.  General syntax: `VAL if CONDITION else VAL`\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     i[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m i[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m i\n\u001b[0;32m      6\u001b[0m     \n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# DataFrame[column].apply(some_function) --> calls some_function() on every\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# value in the column, and returns a new Series with the results.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Here I'm using an anonymous Lamnda function that just removed leading/trailing\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# whitespaces, since some of the entries have these (which I discovered the\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# hard way when prepping this code).\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m naics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2022 NAICS Title\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m())\n\u001b[0;32m     13\u001b[0m ]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "# Remove the random \"T\"s from the end of the NAICS titles.\n",
    "naics[\"2022 NAICS Title\"] = [\n",
    "    # This is something called a \"ternary expression\".  It's super handy\n",
    "    # for list comprehension.  General syntax: `VAL if CONDITION else VAL`\n",
    "    i[:-1] if i[:-1].endswith(\"T\") else i\n",
    "    \n",
    "    # DataFrame[column].apply(some_function) --> calls some_function() on every\n",
    "    # value in the column, and returns a new Series with the results.\n",
    "    # Here I'm using an anonymous Lamnda function that just removed leading/trailing\n",
    "    # whitespaces, since some of the entries have these (which I discovered the\n",
    "    # hard way when prepping this code).\n",
    "    for i in naics[\"2022 NAICS Title\"].apply(lambda x: x.strip())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b7df58-826c-4244-aa68-07293feb4e86",
   "metadata": {},
   "source": [
    "Uh-oh.  That's an error.  And unfortunately, this is an error you'll see a *lot*.  So I'll cut to the chase and tell you what it usually means: you have mising values.\n",
    "\n",
    "Missing values are ultimately stored as `numpy.nan` values (though Pandas will treat them slightly differently than Numpy does when crunching numbers).  `NaN` values are implemented as floating point numbers, and when you try to call something like `.strip()` on a float, that method doesn't exist (it only exists for strings, e.g., `\"something\".strip()`).\n",
    "\n",
    "Pandas columns can actually store data of mixed types--you generally shouldn't do that, but you can--so sometimes this might be due to an actual floating point number, like `1.234`, being mixed in with text data.  But that's quite rare in my experience, so the above error usually means you have missing values.\n",
    "\n",
    "Let's confirm this by using the `.count()` method on columns, which will tell us how many non-missing values there are, and compare that to the number of rows in the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61240844-d938-4703-94a4-be9c4c253634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-missing NAICS titles: 2125\n",
      "Number of rows in the dataframe: 2144\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of non-missing NAICS titles: {naics['2022 NAICS Title'].count()}\")\n",
    "print(f\"Number of rows in the dataframe: {naics.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eca1f7-ef96-414e-9272-705475cf660b",
   "metadata": {},
   "source": [
    "Yep, we have some missing values.  We could fill them in with something like `\"[Unknown]\"` without any issues.  But, we don't know if we'll have a one-to-one match between the NAICS codes here and the NAICS codes in our other dataset.  So what I'm going to do is just remove these rows, and fill in missing values *after we join the datasets.*\n",
    "\n",
    "This is where the `pd.isnull()` function is super handy: it takes a `Series` or Numpy `array` as its argument, and returns `True` or `False` for each value depending on whether it's `NaN`, `NA`, or any other standard missing value.  I'll use this in a subquery to remove rows with missing NAICS titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc2d58e5-bea1-49e9-9d00-8ad2cd030a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     2022 NAICS Code                             2022 NAICS Title\n",
      "0             11////   Agriculture, Forestry, Fishing and Hunting\n",
      "1             111///                              Crop Production\n",
      "2             1111//                    Oilseed and Grain Farming\n",
      "3             11111/                              Soybean Farming\n",
      "4             111110                              Soybean Farming\n",
      "...              ...                                          ...\n",
      "2139          9281//  National Security and International Affairs\n",
      "2140          92811/                            National Security\n",
      "2141          928110                            National Security\n",
      "2142          92812/                        International Affairs\n",
      "2143          928120                        International Affairs\n",
      "\n",
      "[2125 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "naics = naics[~pd.isnull(naics[\"2022 NAICS Title\"])]\n",
    "naics[\"2022 NAICS Title\"] = [\n",
    "    # This is something called a \"ternary expression\".  It's super handy\n",
    "    # for list comprehension.  General syntax: `VAL if CONDITION else VAL`\n",
    "    i[:-1] if i.endswith(\"T\") else i\n",
    "    \n",
    "    # DataFrame[column].apply(some_function) --> calls some_function() on every\n",
    "    # value in the column, and returns a new Series with the results.\n",
    "    # Here I'm using an anonymous Lamnda function that just removed leading/trailing\n",
    "    # whitespaces, since some of the entries have these (which I discovered the\n",
    "    # hard way when prepping this code).\n",
    "    for i in naics[\"2022 NAICS Title\"].apply(lambda x: x.strip())\n",
    "]\n",
    "print(naics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6692a881-1ac0-468b-848d-4b79767d7804",
   "metadata": {},
   "source": [
    "## Digression: Vectorizing boolean operations\n",
    "\n",
    "Note that I used a `~` in the above code.  This is because of a quirk in Python: the boolean operators `not`, `and`, and `or` can't really be vectorized easily.  (It definitely feels like they should, though).  If you're interested in the details, here's the very, very abridged version.  When you call any of the mathematical or bitwise operators, like `x + y`, Python actually calls a special method on `x`: `x.__add__(y)`.  When you're writing your own classes, you can change what `__add__()` does for instances of that class.  This is true for almost all of these binary operators, meaning you can override what addition, multiplication, subtraction, exponentiation, etc. do.  This is the basic trick that Numpy uses to get vectorization: it just changes the definition of most of the math operators.  For whatever reason, the boolean operators `not`, `and`, and `or` can't be overridden this way.  (Well, `not` can, but Pandas/Numpy made the reasonable decision that \"if we can't vectorize *all* boolean operations, then we won't vectorize *any*, just to avoid confusion\").\n",
    "\n",
    "But, the *bitwise* operators can be vectrized just fine.  A *bitwise* operation operates independently on the 1s and 0s of the binary representation of something.  E.g.: the number 5, in binary, is 101, and the value 4 is 100.  `5 & 4` will create a new binary value that has 1s where *both* of the inputs have 1s, and zeros everywhere else.  So `5 & 4 = 101 & 100 = 100 = 4`.  Python has four *bitwise* boolean operators: `x & y` is a bitwise `and`; `x | y` is a bitwise `or`; `x ^ y` is a bitwise exclusive `or` (Python doesn't have a dedicated non-bitwise exclusive or operator); and `~x` is a bitwise `not`.\n",
    "\n",
    "Pandas and Numpy both override how these work in order to vectorize boolean operations over `array`s and `Series`.  So:\n",
    "\n",
    "1. `np.array([True, False]) & np.array([True, True]) = np.array([True, False])`\n",
    "2. `np.array([True, False]) | np.array([True, True]) = np.array([True, True])`\n",
    "3. `~np.array([True, False]) = np.array([False, True])`\n",
    "\n",
    "And the same thing goes for Pandas `Series`, which, remember, are 1) built on top of Numpy `array`s, and 2) are what's used to store columns in `DataFrame`s.\n",
    "\n",
    "So, breaking down `~pd.isnull(naics[\"2022 NAICS Title\"])`:\n",
    "1. `pd.isnull(naics[\"2022 NAICS Title\"])` returns a `Series` of `True` and `False` values.  The values are `True` if the corresponding value/postion in `naics[\"2022 NAICS Title\"]` is any `NaN`/`NA`/missing-like value, and False otherwise. Intuitively, this tells us whether the original values were missing or not.\n",
    "2. `~pd.isnull(naics[\"2022 NAICS Title\"])` vectorized the `not` operation over those results, so that the `Series` now tells us whether the original value was *non-missing*.\n",
    "3. Now, we index the original dataframe with this result, which will be `True` everywhere that the NAICS title is *not* missing, meaning we'll drop any rows where it *is* missing.\n",
    "\n",
    "## *End of digression.*\n",
    "\n",
    "## Back to the census data\n",
    "\n",
    "Okay, now that we've cleaned up our NAICS codes, let's join them to our census data.  Nothing fancy going on here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e451d96-cff1-44cf-8890-a3877f4d2a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Number of Employees  Annual Payroll  Number of Establishments  \\\n",
      "0                    11735          451573                       880   \n",
      "1                      512           31901                        29   \n",
      "2                        6             671                         6   \n",
      "3                        6             671                         6   \n",
      "4                        5             635                         4   \n",
      "...                    ...             ...                       ...   \n",
      "72741                   70            5292                         8   \n",
      "72742                   46            2976                         4   \n",
      "72743                   46            2976                         4   \n",
      "72744                   21            2048                         3   \n",
      "72745                   21            2048                         3   \n",
      "\n",
      "           County Name                                   2022 NAICS Title  \n",
      "0      Anderson County                                          [Unknown]  \n",
      "1      Anderson County                                          [Unknown]  \n",
      "2      Anderson County                             Oil and Gas Extraction  \n",
      "3      Anderson County                             Oil and Gas Extraction  \n",
      "4      Anderson County                         Crude Petroleum Extraction  \n",
      "...                ...                                                ...  \n",
      "72741        [Unknown]  Religious, Grantmaking, Civic, Professional, a...  \n",
      "72742        [Unknown]                    Grantmaking and Giving Services  \n",
      "72743        [Unknown]                    Grantmaking and Giving Services  \n",
      "72744        [Unknown]                      Social Advocacy Organizations  \n",
      "72745        [Unknown]                      Social Advocacy Organizations  \n",
      "\n",
      "[72746 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "census_data = pd.merge(\n",
    "    left=census_data,\n",
    "    right=naics,\n",
    "    left_on=\"Industry NAICS Code\",\n",
    "    right_on=\"2022 NAICS Code\",\n",
    "    how=\"left\",\n",
    ").drop(\n",
    "    columns=[\"2022 NAICS Code\", \"Industry NAICS Code\"]\n",
    ")\n",
    "\n",
    "# Fill in any missing NAICS titles with \"[Unknown]\".\n",
    "census_data[\"2022 NAICS Title\"] = census_data[\"2022 NAICS Title\"].fillna(\"[Unknown]\")\n",
    "\n",
    "print(census_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e47e899-04cb-4fb4-81e8-57ae0428f942",
   "metadata": {},
   "source": [
    "Okay!  At long, long last, we have our data ready to analyze!  Remember what we're trying to do: find counties and industries that have the highest average annual payroll.\n",
    "\n",
    "In order to do this, we're going to need to do something like the following:\n",
    "\n",
    "1. For each unique value of the \"County Name\" column, subset/subquery the `DataFrame` to contain just rows with that value.\n",
    "2. Calculate the mean \"Annual Payroll\" for that subset.\n",
    "3. Log the county name and average value.\n",
    "4. Sort the values and pull the top, say, 10.\n",
    "\n",
    "We could do this by hand, and it wouldn't be too hard.  But there's a better way: using `.groupby()`.  This is a method on `DataFrame`s that will do the first step for us, and let us calculate things like means, maximums, and really anything else we want, but just within the resulting groups.  It's very easy to use.  Call `DataFrame.groupby(column)`, then interact with the resulting object more or less like you would a normal `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bce7a356-1d2e-4788-bae2-fb415c5372e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "County Name\n",
      "Anderson County      7133.672956\n",
      "Andrews County       9583.793103\n",
      "Angelina County     10651.570621\n",
      "Aransas County       3140.855769\n",
      "Archer County        1828.254902\n",
      "                       ...      \n",
      "Yoakum County        5500.918367\n",
      "Young County         4405.339207\n",
      "Zapata County        2519.103896\n",
      "Zavala County        2198.811321\n",
      "[Unknown]          658795.494585\n",
      "Name: Annual Payroll, Length: 254, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    census_data\n",
    "    .groupby(\"County Name\")\n",
    "    [\"Annual Payroll\"] # this is indexing the column \"Annual Payroll\" out of the previous line's result\n",
    "    .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8944544e-3978-4524-abe4-3b73086411b2",
   "metadata": {},
   "source": [
    "And there we go.  Now we just need to add a method call to sort the values, and we're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e59acca2-e62b-4169-b8bc-d8007e9c43be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "County Name\n",
      "Montgomery County     57762.589238\n",
      "Williamson County     58836.014286\n",
      "Denton County         60101.561102\n",
      "Collin County        156609.611249\n",
      "Bexar County         157625.417622\n",
      "Tarrant County       169089.186294\n",
      "Travis County        189018.471429\n",
      "Dallas County        378943.978342\n",
      "Harris County        486440.964789\n",
      "[Unknown]            658795.494585\n",
      "Name: Annual Payroll, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    census_data\n",
    "    .groupby(\"County Name\")\n",
    "    [\"Annual Payroll\"]\n",
    "    .mean()\n",
    "    .sort_values()\n",
    "    [-10:] # indexing the result of the last step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44f3a6-a6d6-4a57-8e32-e51e2c662fd9",
   "metadata": {},
   "source": [
    "There we go.  Nice and easy.\n",
    "\n",
    "But: what if we want to find *combination of county and NAICS industry* with the highest average annual payroll?  Also easy!  Instead of passing a single string to `.groupby()`, just pass a list of column names.  Groups will be created based on the unique combinations of the columns you specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ec0f426-2eef-4a53-a95d-7c41fd36359a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "County Name    2022 NAICS Title                                    \n",
      "Dallas County  Corporate, Subsidiary, and Regional Managing Offices     7787489.0\n",
      "               Management of Companies and Enterprises                  8203732.0\n",
      "Travis County  Professional, Scientific, and Technical Services        10538150.0\n",
      "[Unknown]      Professional Employer Organizations                     12815624.0\n",
      "Dallas County  Professional, Scientific, and Technical Services        15411211.0\n",
      "[Unknown]      Employment Services                                     16253725.0\n",
      "Harris County  Corporate, Subsidiary, and Regional Managing Offices    16414248.0\n",
      "[Unknown]      Administrative and Support Services                     16521170.0\n",
      "Harris County  Management of Companies and Enterprises                 17031550.0\n",
      "               Professional, Scientific, and Technical Services        18259471.0\n",
      "Name: Annual Payroll, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    census_data\n",
    "    .groupby([\"County Name\", \"2022 NAICS Title\"])\n",
    "    [\"Annual Payroll\"]\n",
    "    .mean()\n",
    "    .sort_values()\n",
    "    [-10:] # indexing the result of the last step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965d960a-de99-4909-b87e-ba0fb9d90f96",
   "metadata": {},
   "source": [
    "# Some miscellaneous stuff you can do in Pandas\n",
    "\n",
    "## Access the underlying Numpy data\n",
    "\n",
    "Access the underlying Numpy data directly with the `.values` attribute of a `Series` or `DataFrame`.  This is sometimes useful, but usually only in edge cases.  (e.g.: there are some things that are faster to do over Numpy `array`s than over Pandas objects, but you usually need to have huge `DataFrame`s for that to matter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09243122-7ba1-4f70-b5fb-1489735f7301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x  y\n",
      "0  1  4\n",
      "1  2  5\n",
      "2  3  6\n",
      "\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      "[1 2 3]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\"x\": [1,2,3], \"y\": [4,5,6]})\n",
    "print(df)\n",
    "print()\n",
    "print(df.values)\n",
    "print(type(df.values))\n",
    "print()\n",
    "print(df[\"x\"].values)\n",
    "print(type(df[\"x\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd02ae2e-d238-45c3-8df9-72f6a72773a6",
   "metadata": {},
   "source": [
    "## Datetime values\n",
    "\n",
    "Pandas has very nice support for working with datetime values, e.g., subtractin two dates and computing the number of seconds/minutes/days/weeks/etc. between them, parsing strings to datetime (using the same general syntax as Python's `datetime` module in the standard library), and working with \"time deltas\" that can be added and subtracted from dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47525f7d-0c3a-471c-8871-997877d1ff1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2020-01-01', '2020-02-02', '2020-03-03', '2020-04-04'], dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "Subtracting 2020-01-01 00:00:00 from 2020-01-01 00:00:00:\n",
      "32 days 00:00:00\n",
      "<class 'pandas._libs.tslibs.timedeltas.Timedelta'>\n",
      "32\n",
      "\n",
      "Adding 15 days to the datetime series:\n",
      "DatetimeIndex(['2020-01-16', '2020-02-17', '2020-03-18', '2020-04-19'], dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "Adding 15 days + two weeks + 2 hours to the datetime series:\n",
      "DatetimeIndex(['2020-01-30 12:00:00', '2020-03-02 12:00:00',\n",
      "               '2020-04-01 12:00:00', '2020-05-03 12:00:00'],\n",
      "              dtype='datetime64[ns]', freq=None)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_dates = [\n",
    "    \"January 1 2020\",\n",
    "    \"February 2 2020\",\n",
    "    \"March 3 2020\",\n",
    "    \"April 4 2020\",\n",
    "]\n",
    "\n",
    "# convert to datetime.\n",
    "# For details on the `format` option, see:\n",
    "# https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes\n",
    "# (Pandas uses all the same %-codes as Python).\n",
    "datetime = pd.to_datetime(raw_dates, format=\"%B %d %Y\")\n",
    "print(datetime)\n",
    "\n",
    "# Subtract two dates from each other and get the number of days.\n",
    "diff = datetime[1] - datetime[0]\n",
    "print(f\"\\nSubtracting {datetime[0]} from {datetime[0]}:\")\n",
    "print(diff)\n",
    "print(type(diff))\n",
    "print(diff.days)\n",
    "\n",
    "# TimeDelta --> can add/subtract from any date to move forward or backward\n",
    "# by a specified amount.\n",
    "delta = pd.Timedelta(days=15)\n",
    "print(f\"\\nAdding 15 days to the datetime series:\")\n",
    "print(datetime + delta)\n",
    "\n",
    "delta = pd.Timedelta(days=15, weeks=2, hours=12)\n",
    "print(f\"\\nAdding 15 days + two weeks + 2 hours to the datetime series:\")\n",
    "print(datetime + delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b585c84-c7d8-487f-a933-0cdf7e140baf",
   "metadata": {},
   "source": [
    "The main things to know about in the Pandas time and date ecosystem:\n",
    "- `pd.to_datetime(string_or_series, format=\"format string\")` will convert a string, or series of strings, to a Pandas datetime object.\n",
    "- `pd.Timestamp` objects store date and time information in a unified way.  Pandas keeps track of all the nitty gritty details for you.  (there are a few specific forms of timestamps, e.g., `pd.datetime64`, which you can see in the above cells; don't worry about th differences between these).\n",
    "- `pd.Timedelta()` is a value you can add to/subtract from `pd.datetime64` values to move dates forward/backward by a certain amount.  (again, Pandas will handle day/month/etc changes).\n",
    "- If you have a series of `Timedelta` objects, you can vectorize accessing the number of days/weeks/etc by doing `series.dt.days`, `series.dt.weeks`, etc.\n",
    "\n",
    "## Convenient subsets\n",
    "\n",
    "You can get subsets of your data in a few different ways.  These methods work on `DataFrame`s as well as groupby objects; if run on a groupby object, then like most methods, this will be called individually on each group, and the results concatenated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73e45c1d-cbc0-44dd-b9a5-7665cb9f13c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       County Name  Number of Employees\n",
      "0  Anderson County                11735\n",
      "1  Anderson County                  512\n",
      "2  Anderson County                    6\n",
      "3  Anderson County                    6\n",
      "4  Anderson County                    5\n",
      "5  Anderson County                    5\n",
      "6  Anderson County                  490\n",
      "7  Anderson County                  490\n",
      "8  Anderson County                  490\n",
      "9  Anderson County                   39\n",
      "\n",
      "      County Name  Number of Employees\n",
      "72736   [Unknown]                 2078\n",
      "72737   [Unknown]                 1990\n",
      "72738   [Unknown]                  450\n",
      "72739   [Unknown]                  450\n",
      "72740   [Unknown]                   18\n",
      "72741   [Unknown]                   70\n",
      "72742   [Unknown]                   46\n",
      "72743   [Unknown]                   46\n",
      "72744   [Unknown]                   21\n",
      "72745   [Unknown]                   21\n",
      "\n",
      "             County Name  Number of Employees\n",
      "65775   Van Zandt County                   99\n",
      "67337  Washington County                13018\n",
      "30464      Harris County                 5001\n",
      "56868        Rusk County                   29\n",
      "2257      Bastrop County                    7\n",
      "51629   Ochiltree County                   13\n",
      "40359        Kerr County                   18\n",
      "20844     El Paso County                  609\n",
      "5326       Bosque County                   36\n",
      "36107        Hunt County                   78\n",
      "\n",
      "              County Name  Number of Employees\n",
      "586       Angelina County                   43\n",
      "12096       Collin County                  103\n",
      "1783        Austin County                   60\n",
      "63221       Travis County                  297\n",
      "28341    Guadalupe County                   19\n",
      "...                   ...                  ...\n",
      "10593         Cass County                   21\n",
      "71813         Wood County                   48\n",
      "33690      Hidalgo County                  275\n",
      "19738        Ellis County                   46\n",
      "49522  Nacogdoches County                  185\n",
      "\n",
      "[727 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove some columns just so the printing is nicer\n",
    "df = census_data[[\"County Name\", \"Number of Employees\"]]\n",
    "\n",
    "# Top N rows\n",
    "print(df.head(10))\n",
    "print()\n",
    "\n",
    "# Last N rows\n",
    "print(df.tail(10))\n",
    "print()\n",
    "\n",
    "# Random sample of rows, *without* replacement.\n",
    "# Set `replace` to True to sample *with* replacement.\n",
    "# important syntax: df.sample(n=None, frac=None, replace=False).\n",
    "# There are some other kwargs, check the documentation.\n",
    "# Pass `n` to specify the *number of rows* to resample.\n",
    "# Pass `frac` to specify a *percentage of the original rows*\n",
    "# to resample.\n",
    "# `replace` is True to sample with replacement, False to sample without.\n",
    "print(df.sample(n=10, replace=False))\n",
    "print()\n",
    "\n",
    "# Sample a *percentage* of the rows.\n",
    "print(df.sample(frac=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c8e9b5-171f-4909-a35b-2946efcd81b1",
   "metadata": {},
   "source": [
    "A neat trick: sometimes you need to randomly shuffle the rows in your `DataFrame`.  You don't need to do this a lot, but sometimes it can be helpful.  You can use `DataFrame.sample()` to do this pretty efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2c2aaff-8bfc-4165-af4c-c78246f5a3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       County Name  Number of Employees\n",
      "0  Anderson County                11735\n",
      "1  Anderson County                  512\n",
      "2  Anderson County                    6\n",
      "3  Anderson County                    6\n",
      "4  Anderson County                    5\n",
      "           County Name  Number of Employees\n",
      "66473  Victoria County                   60\n",
      "7809   Brewster County                    6\n",
      "41740  Lampasas County                  170\n",
      "5013      Bexar County                 4168\n",
      "5011      Bexar County                    8\n"
     ]
    }
   ],
   "source": [
    "shuffled = df.sample(frac=1, replace=False)\n",
    "print(df.head())\n",
    "print(shuffled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e206c0-2670-49e4-b8b8-26b8d1303846",
   "metadata": {},
   "source": [
    "## Run a battery of descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0c494f9-66cb-4b2b-a6ad-48441b46a6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Number of Employees  Annual Payroll  Number of Establishments\n",
      "count         7.274600e+04    7.274600e+04              72746.000000\n",
      "mean          8.975092e+02    4.922268e+04                 49.632447\n",
      "std           1.259636e+04    7.960757e+05                613.507725\n",
      "min           0.000000e+00    4.000000e+00                  3.000000\n",
      "25%           2.500000e+01    8.680000e+02                  4.000000\n",
      "50%           7.500000e+01    2.908000e+03                  8.000000\n",
      "75%           2.810000e+02    1.153350e+04                 20.000000\n",
      "max           2.113874e+06    1.382669e+08             105710.000000\n"
     ]
    }
   ],
   "source": [
    "# .describe() ignores every column that's not numeric\n",
    "print(census_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78282b7-2daa-4658-87bd-a464a8788c9b",
   "metadata": {},
   "source": [
    "## Pivot tables\n",
    "\n",
    "Convert data from \"long\" to \"wide\" using any of:\n",
    "- `DataFrame.pivot()` (for very simple pivots)\n",
    "- `DataFrame.pivot_table()` (for more general pivots--I recommend this one)\n",
    "- `pd.pivot()` and `pd.pivot_table()`, which are equivalent to the above methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3b14cc6-0df0-446e-add9-e8cdc748f4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022 NAICS Title  Abrasive Product Manufacturing  Accommodation  \\\n",
      "County Name                                                       \n",
      "Anderson County                              NaN          145.0   \n",
      "Andrews County                               NaN           32.0   \n",
      "Angelina County                              NaN          170.0   \n",
      "Aransas County                               NaN          216.0   \n",
      "Archer County                                NaN            NaN   \n",
      "...                                          ...            ...   \n",
      "Yoakum County                                NaN           27.0   \n",
      "Young County                                 NaN            NaN   \n",
      "Zapata County                                NaN            6.0   \n",
      "Zavala County                                NaN            NaN   \n",
      "[Unknown]                                    NaN            NaN   \n",
      "\n",
      "2022 NAICS Title  Accounting, Tax Preparation, Bookkeeping, and Payroll Services  \\\n",
      "County Name                                                                        \n",
      "Anderson County                                                48.0                \n",
      "Andrews County                                                 17.0                \n",
      "Angelina County                                               196.0                \n",
      "Aransas County                                                 59.0                \n",
      "Archer County                                                  13.0                \n",
      "...                                                             ...                \n",
      "Yoakum County                                                   NaN                \n",
      "Young County                                                   81.0                \n",
      "Zapata County                                                   NaN                \n",
      "Zavala County                                                   NaN                \n",
      "[Unknown]                                                   10579.0                \n",
      "\n",
      "2022 NAICS Title  Activities Related to Credit Intermediation  \\\n",
      "County Name                                                     \n",
      "Anderson County                                           7.0   \n",
      "Andrews County                                            NaN   \n",
      "Angelina County                                          12.0   \n",
      "Aransas County                                            NaN   \n",
      "Archer County                                             NaN   \n",
      "...                                                       ...   \n",
      "Yoakum County                                             NaN   \n",
      "Young County                                              NaN   \n",
      "Zapata County                                             NaN   \n",
      "Zavala County                                             NaN   \n",
      "[Unknown]                                               368.0   \n",
      "\n",
      "2022 NAICS Title  Activities Related to Real Estate  Adhesive Manufacturing  \\\n",
      "County Name                                                                   \n",
      "Anderson County                                23.0                     NaN   \n",
      "Andrews County                                 31.0                     NaN   \n",
      "Angelina County                               143.0                     NaN   \n",
      "Aransas County                                 57.0                     NaN   \n",
      "Archer County                                   NaN                     NaN   \n",
      "...                                             ...                     ...   \n",
      "Yoakum County                                   NaN                     NaN   \n",
      "Young County                                    3.0                     NaN   \n",
      "Zapata County                                   NaN                     NaN   \n",
      "Zavala County                                   NaN                     NaN   \n",
      "[Unknown]                                      10.0                     NaN   \n",
      "\n",
      "2022 NAICS Title  Administrative Management and General Management Consulting Services  \\\n",
      "County Name                                                                              \n",
      "Anderson County                                                 7.0                      \n",
      "Andrews County                                                  2.0                      \n",
      "Angelina County                                               119.0                      \n",
      "Aransas County                                                  NaN                      \n",
      "Archer County                                                   NaN                      \n",
      "...                                                             ...                      \n",
      "Yoakum County                                                   NaN                      \n",
      "Young County                                                    8.0                      \n",
      "Zapata County                                                   NaN                      \n",
      "Zavala County                                                   NaN                      \n",
      "[Unknown]                                                    5297.0                      \n",
      "\n",
      "2022 NAICS Title  Administrative and Support Services  Advertising Agencies  \\\n",
      "County Name                                                                   \n",
      "Anderson County                                 167.0                   NaN   \n",
      "Andrews County                                   25.0                   NaN   \n",
      "Angelina County                                1485.0                   5.0   \n",
      "Aransas County                                   89.0                   NaN   \n",
      "Archer County                                   153.0                   NaN   \n",
      "...                                               ...                   ...   \n",
      "Yoakum County                                    14.0                   NaN   \n",
      "Young County                                    101.0                   NaN   \n",
      "Zapata County                                     3.0                   NaN   \n",
      "Zavala County                                     NaN                   NaN   \n",
      "[Unknown]                                    338901.0                   NaN   \n",
      "\n",
      "2022 NAICS Title  Advertising Material Distribution Services  ...  Wineries  \\\n",
      "County Name                                                   ...             \n",
      "Anderson County                                          NaN  ...       NaN   \n",
      "Andrews County                                           NaN  ...       NaN   \n",
      "Angelina County                                          NaN  ...       NaN   \n",
      "Aransas County                                           NaN  ...       NaN   \n",
      "Archer County                                            NaN  ...       NaN   \n",
      "...                                                      ...  ...       ...   \n",
      "Yoakum County                                            NaN  ...       NaN   \n",
      "Young County                                             NaN  ...       NaN   \n",
      "Zapata County                                            NaN  ...       NaN   \n",
      "Zavala County                                            NaN  ...       NaN   \n",
      "[Unknown]                                                NaN  ...       NaN   \n",
      "\n",
      "2022 NAICS Title  Wiring Device Manufacturing  \\\n",
      "County Name                                     \n",
      "Anderson County                           NaN   \n",
      "Andrews County                            NaN   \n",
      "Angelina County                           NaN   \n",
      "Aransas County                            NaN   \n",
      "Archer County                             NaN   \n",
      "...                                       ...   \n",
      "Yoakum County                             NaN   \n",
      "Young County                              NaN   \n",
      "Zapata County                             NaN   \n",
      "Zavala County                             NaN   \n",
      "[Unknown]                                 NaN   \n",
      "\n",
      "2022 NAICS Title  Wood Container and Pallet Manufacturing  \\\n",
      "County Name                                                 \n",
      "Anderson County                                       NaN   \n",
      "Andrews County                                        NaN   \n",
      "Angelina County                                       NaN   \n",
      "Aransas County                                        NaN   \n",
      "Archer County                                         NaN   \n",
      "...                                                   ...   \n",
      "Yoakum County                                         NaN   \n",
      "Young County                                          NaN   \n",
      "Zapata County                                         NaN   \n",
      "Zavala County                                         NaN   \n",
      "[Unknown]                                             NaN   \n",
      "\n",
      "2022 NAICS Title  Wood Kitchen Cabinet and Countertop Manufacturing  \\\n",
      "County Name                                                           \n",
      "Anderson County                                                 NaN   \n",
      "Andrews County                                                  NaN   \n",
      "Angelina County                                                 NaN   \n",
      "Aransas County                                                  NaN   \n",
      "Archer County                                                   NaN   \n",
      "...                                                             ...   \n",
      "Yoakum County                                                   NaN   \n",
      "Young County                                                    NaN   \n",
      "Zapata County                                                   NaN   \n",
      "Zavala County                                                   NaN   \n",
      "[Unknown]                                                       NaN   \n",
      "\n",
      "2022 NAICS Title  Wood Office Furniture Manufacturing  Wood Preservation  \\\n",
      "County Name                                                                \n",
      "Anderson County                                   NaN                NaN   \n",
      "Andrews County                                    NaN                NaN   \n",
      "Angelina County                                   NaN                NaN   \n",
      "Aransas County                                    NaN                NaN   \n",
      "Archer County                                     NaN                NaN   \n",
      "...                                               ...                ...   \n",
      "Yoakum County                                     NaN                NaN   \n",
      "Young County                                      NaN                NaN   \n",
      "Zapata County                                     NaN                NaN   \n",
      "Zavala County                                     NaN                NaN   \n",
      "[Unknown]                                         NaN                NaN   \n",
      "\n",
      "2022 NAICS Title  Wood Product Manufacturing  \\\n",
      "County Name                                    \n",
      "Anderson County                          NaN   \n",
      "Andrews County                           NaN   \n",
      "Angelina County                        656.0   \n",
      "Aransas County                           NaN   \n",
      "Archer County                            NaN   \n",
      "...                                      ...   \n",
      "Yoakum County                            NaN   \n",
      "Young County                             NaN   \n",
      "Zapata County                            NaN   \n",
      "Zavala County                            NaN   \n",
      "[Unknown]                                NaN   \n",
      "\n",
      "2022 NAICS Title  Wood Window and Door Manufacturing  \\\n",
      "County Name                                            \n",
      "Anderson County                                  NaN   \n",
      "Andrews County                                   NaN   \n",
      "Angelina County                                  NaN   \n",
      "Aransas County                                   NaN   \n",
      "Archer County                                    NaN   \n",
      "...                                              ...   \n",
      "Yoakum County                                    NaN   \n",
      "Young County                                     NaN   \n",
      "Zapata County                                    NaN   \n",
      "Zavala County                                    NaN   \n",
      "[Unknown]                                        NaN   \n",
      "\n",
      "2022 NAICS Title  Zoos and Botanical Gardens     [Unknown]  \n",
      "County Name                                                 \n",
      "Anderson County                          NaN    400.637681  \n",
      "Andrews County                           NaN    322.842105  \n",
      "Angelina County                          NaN    586.685950  \n",
      "Aransas County                           NaN    222.377778  \n",
      "Archer County                            NaN    103.304348  \n",
      "...                                      ...           ...  \n",
      "Yoakum County                            NaN    169.555556  \n",
      "Young County                             NaN    267.469388  \n",
      "Zapata County                            NaN    140.000000  \n",
      "Zavala County                            NaN    148.750000  \n",
      "[Unknown]                                NaN  21582.808511  \n",
      "\n",
      "[254 rows x 1088 columns]\n"
     ]
    }
   ],
   "source": [
    "pivoted = census_data.pivot_table(\n",
    "    columns=\"2022 NAICS Title\",\n",
    "    index=\"County Name\",\n",
    "    values=\"Number of Employees\"\n",
    ")\n",
    "print(pivoted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a28b18-d935-48ae-b454-362a0c5efd51",
   "metadata": {},
   "source": [
    "Note that missing value get filled in with `NaN`.  There are options in each of these methods/functions to control how missing values are handled, e.g., to pick a sensible value to use instead, like 0.  Also note that the values we passed to the `index` argument became the index of the resulting pivoted data.\n",
    "\n",
    "There's also something very interesting here: if we check the columns of the pivoted `DataFrame`, we'll see that there's a \"name\" associated with the columns.  The columns are, in this case, stored as something like a `pd.Series`, which can have a \"name\" stored as a piece of metadata.  These names are not something I interact with a lot, and they *have no relationship* to the name of the variable that stores the object.  This will be very convenient for us in just a second, since Pandas will use this name--which it created during the pivot--when deciding column names after un-pivoting.\n",
    "\n",
    "To \"un-pivot\" a `DataFrame` from \"wide\" to \"long\" format, use `pd.melt()`/`DataFrame.melt()`.  In this example we need to use `DataFrame.reset_index()` to get the index back as a normal column, so we can pass it to `id_vars=`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d12278df-38eb-48a3-a5df-f96fe00cf4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "melted = pivoted.reset_index().melt(id_vars=\"County Name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec002d-58f2-436f-8026-73328824d7dc",
   "metadata": {},
   "source": [
    "Note that we have a \"value\" column.  We can rename this explicitly using `DataFrame.rename()`, or we can pass another argument to `.melt()`.  Either way is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4441697f-b85a-4b45-9334-dff0adac332c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            County Name                2022 NAICS Title  Number of Employees\n",
      "0       Anderson County  Abrasive Product Manufacturing                  NaN\n",
      "1        Andrews County  Abrasive Product Manufacturing                  NaN\n",
      "2       Angelina County  Abrasive Product Manufacturing                  NaN\n",
      "3        Aransas County  Abrasive Product Manufacturing                  NaN\n",
      "4         Archer County  Abrasive Product Manufacturing                  NaN\n",
      "...                 ...                             ...                  ...\n",
      "276347    Yoakum County                       [Unknown]           169.555556\n",
      "276348     Young County                       [Unknown]           267.469388\n",
      "276349    Zapata County                       [Unknown]           140.000000\n",
      "276350    Zavala County                       [Unknown]           148.750000\n",
      "276351        [Unknown]                       [Unknown]         21582.808511\n",
      "\n",
      "[276352 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "melted = pivoted.reset_index().melt(id_vars=\"County Name\", value_name=\"Number of Employees\")\n",
    "print(melted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39253aa4-19a6-4793-a537-4c537a2b82c7",
   "metadata": {},
   "source": [
    "Seeing this in action with a dataframe that starts in pivoted format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e464541b-295d-4b3c-9e5b-322a5a12f333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x  y\n",
      "0  1  4\n",
      "1  2  5\n",
      "2  3  6\n",
      "\n",
      "  variable  value\n",
      "0        x      1\n",
      "1        x      2\n",
      "2        x      3\n",
      "3        y      4\n",
      "4        y      5\n",
      "5        y      6\n",
      "\n",
      "  Original Variable  Value\n",
      "0                 x      1\n",
      "1                 x      2\n",
      "2                 x      3\n",
      "0                 y      4\n",
      "1                 y      5\n",
      "2                 y      6\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\"x\": [1,2,3], \"y\": [4,5,6]})\n",
    "print(df)\n",
    "print()\n",
    "print(df.melt())\n",
    "print()\n",
    "# `ignore_index=False` --> keep the original indices, possible with repetition.\n",
    "print(df.melt(var_name=\"Original Variable\", value_name=\"Value\", ignore_index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dccce1-c895-41de-80d8-cc6db1827ab7",
   "metadata": {},
   "source": [
    "## Dummy coding\n",
    "\n",
    "For a lot of analyses, you'll end up with some sort of categorica data you want to \"dummy code\" before feeding it int your downstream model.  This is pretty common in both machine learning and in statistics, but it might also be useful for some within-Pandas analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a64dd921-fdbd-4a0b-8d44-8dd75ad2d7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x  y\n",
      "0  1  4\n",
      "1  2  5\n",
      "2  3  6\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92c2643a-5162-40cf-a64a-19e17b18405a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1  2  3\n",
      "0  1  0  0\n",
      "1  0  1  0\n",
      "2  0  0  1\n"
     ]
    }
   ],
   "source": [
    "print(pd.get_dummies(df[\"x\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b553e-ded3-4515-881e-5cccd4a039a8",
   "metadata": {},
   "source": [
    "There isn't really a way to do this \"in place,\" so you'll have to re-merge the dummy coded data with the original data.  E.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5dd50909-7f03-410a-8e49-e73aa7d78053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y  x_1  x_2  x_3\n",
      "0  4    1    0    0\n",
      "1  5    0    1    0\n",
      "2  6    0    0    1\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\"x\": [1,2,3], \"y\": [4,5,6]})\n",
    "df = pd.merge(\n",
    "    df.drop(columns=[\"x\"]),\n",
    "    pd.get_dummies(df[\"x\"], prefix=\"x\"),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how=\"inner\"\n",
    ")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352b1fc6-1983-4e35-86fe-6c764a0597bb",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "If you have Matplotlib installed (which you should!), Pandas can manage some of your plotting for you, by accessing the `DataFrame.plot` attribute and calling various plotting methods on it.  All of this is just a wrapper around Matplotlib code, so you can access and work with the resulting plots just like ones you create yourself.\n",
    "\n",
    "(If you're running this in a normal Python script, you'll need to call `plt.show()` assuming you've used `import matplotlib.pyplot as plt`, or save the figure like any other Matplotlib figure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c21fbe7a-befd-42db-a6ca-8a7c152f244c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Number of Employees', ylabel='Annual Payroll'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "census_data.plot.scatter(\n",
    "    x=\"Number of Employees\",\n",
    "    y=\"Annual Payroll\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157642ec-441f-4fef-81eb-ea1fb6226fb2",
   "metadata": {},
   "source": [
    "I find that using the plotting tools in Pandas can sometimes be a bit more flexible--e.g., it can automatically generate subplots in very sensible ways--but you don't get quite as much control as you do using Matplotlib directly.  But for data that's already in a `DataFrame` that you just need a plot for, and you don't need a lot of fancy stuff, it' great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b902fe0a-262f-4b6b-a059-b486d98a875a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
