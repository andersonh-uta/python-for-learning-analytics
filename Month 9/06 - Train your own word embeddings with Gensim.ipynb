{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19eb89a-e498-49b1-9a22-54347c4994d0",
   "metadata": {},
   "source": [
    "# Training your own word embeddings with Gensim\n",
    "\n",
    "So far, we've been focusing on how to convert text to numbers by _counting words_ (but we can very easily generalize to \"counting features\" even if the features aren't strictly words).  But there are other ways.\n",
    "\n",
    "We saw that spaCy ships with _word vectors_ in the `en_core_web_lg` model.  Let's talk about these vectors for a second, then we'll see how we can train our own using Gensim.\n",
    "\n",
    "# Word vectors: Dense representations\n",
    "\n",
    "We were focused, before, on representing _documents_ as vectors.  But we can also represent _words_ as vectors, using one of two options:\n",
    "\n",
    "1. A word is the vector of documents it appears in, along with its counts.  (In other words: a word vector is the column in a document-term matrix).\n",
    "2. A word is a \"one-hot\" vector that just indicates what the current word is.\n",
    "\n",
    "The former definition is very rarely used, since the size of that vector will be as big as the number of documents you have, which can often be a lot.  E.g.: in our examples, we had tens of thousands of documents at the low end--having a bunch of 10,000-long vectors (and longer) is not really feasible.\n",
    "\n",
    "The latter definition is more common.  In practice it looks something like this:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\vec{the} &= [1,0,0,0,0]\\\\\n",
    "\\vec{cat} &= [0,1,0,0,0]\\\\\n",
    "\\vec{sat} &= [0,0,1,0,0]\\\\\n",
    "\\vec{on} &=  [0,0,0,1,0]\\\\\n",
    "\\vec{mat} &= [0,0,0,0,1]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "I.e.: each word is a vector of all zeros, but with a single 1.  The position of the 1 corresponds to which word it is: if the first number in the vector is 1, then the word is \"the\".  If the second number is 1, the word is \"cat.\"  (this 1 is sometimes called the \"hot\" value--since every vector has only one hot value, these are \"one hot\" vectors).\n",
    "\n",
    "We can represent a document by just summing the vectors of its words, element-wise:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{The cat sat on the mat} =& \\vec{the} + \\vec{cat} + \\vec{sat} + \\vec{on} + \\vec{the} + \\vec{mat} \\\\\n",
    "=&\\quad [1,0,0,0,0]\\\\\n",
    "&+ [0,1,0,0,0]\\\\\n",
    "&+ [0,0,1,0,0]\\\\\n",
    "&+ [0,0,0,1,0]\\\\\n",
    "&+ [1,0,0,0,0]\\\\\n",
    "&+ [0,0,0,0,1]\\\\\n",
    "=&\\quad [2,1,1,1,1]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146038f7-d5e6-4791-8f67-4c5de3901d81",
   "metadata": {},
   "source": [
    "However, there are some issues with this approach.  We've already mentioned the issue of _sparsity_: in the real world, you'll end up with document vectors that are mostly 0s, which can hurt both speed and accuracy.  But there's a deeper issue here.  This sort of representation can't encode anything about the similarity between words.  If every word is a one-hot vector, then the similarity between any two pairs of words is always the exact same number.\n",
    "\n",
    "Even though we're rarely concerned with \"is this linguistically sound,\" this should give us a bit of pause because of how egregiously un-language-like it is.  But there are also other, practical ramifications: if we show our model some new data full of words it's never seen before, it will just ignore them, even if they're actually important.  If we need to do some sort of similarity query between documents--e.g., \"give me documents that are semantically similar to this one\"--a word-count model will struggle, since it's only going to be able to look for documents that use _the same words_.  Yet, semantic similarity is much richer than \"using the same words.\"  Consider the following two sentences:\n",
    "- This book is great.\n",
    "- That novel was enjoyable.\n",
    "\n",
    "These two sentences could easily be saying the same thing, but they share no words in common.  A word-count approach will treat these two sentences as having nothing in common.  You might get some similarity if you're using a stemmer that maps \"this\" and \"that\" to a common form, and maps \"is\" and \"was\" to a common form, but \"uses the same stopwords\" is a bad way to measure similarity.  What we want is some representation that knows \"book\" and \"novel\" are similar, and that \"great\" and \"enjoyable\" are similar.\n",
    "\n",
    "Enter _word embeddings._  Word embeddings try to do exactly this.  Rather than representing words as large, spare, one-hot vectors, they represent words as smaller, dense, floating point vectors with (in practice) no zeros.  The vectors are created by training them on a large corpus of text, and letting the embedding algorithm figure out good vectors.  Each embedding algorithm has a different notion of what a \"good vector\" is, but all useful embedding algorithms will ultimately give you vectors that encode some meaningful notion of \"similarity\" between words.\n",
    "\n",
    "Once we have these embeddings, we can use them--instead of our one-hot encodings--to train our models.  (we can't do this with most topic models, though; the math behind them assumes, and requires, word counts.  But any supervised learning task, we can absolutely use word embeddings).\n",
    "\n",
    "Emprically, word embedding work extremely well.  There are a few reasons, but the most interesting one--to me--is that they sort of bring \"outside knowledge\" to bear on your problem.  You can train word embeddings on _unlabeled_ corpora, which are easy to get.  The word embeddings will then encode something about \"what words mean\" based on that corpus (specifically: based on what words tend to appear around what other words), and then you can apply that knowledge to your data, even if your data is a lot smaller.\n",
    "\n",
    "Word embedding still suffer from some of the issues of sparse/one-hot representations.  If you encounter a word that doesn't have an embedding, it gets ignored.  But this is a much rarer problem, since it's easy to train the embeddings on a huge corpus with lots of different words that might not be in your labeled data.\n",
    "\n",
    "Word embeddings are also very data-hungry.  A good rule of thumb is that it's not worth training your own embeddings until you have about a million words.  Below that very rough threshold, you probably won't see much benefit, and training your own word embeddings might give _worse_ results than using bag-of-words, or using someone else's word embeddings (like spaCy's, which are generally pretty good).\n",
    "\n",
    "Word embeddings also don't bypass the issue of domain specificity and inappropriate transfer.  If you train word embeddings on a bunch of scientific papers, they will learn what language in scientific papers looks like.  That probably won't transfer very well to something like reflective journaling of high school students.\n",
    "\n",
    "Anyways, let's train our own with Gensim.  We'll use the same dataset--just the electronics, like with the topic modeling--and train a Word2Vec embedding model.  (There are many alternatives--GloVe and FastText are the biggest--but Word2Vec was the first big breakthrough embedding model, and remains an excellent choice).  We'll apply the same preprocessing and filtering that we used for the topic modeling.  Unlike with the topic modeling, though, we're going to keep the number of stars.  We'll train the Word2Vec model on _all_ the reviews--regardless of how many stars--and then transform only a moderately sized subset of positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ecca93d-5a0b-42d2-bb0f-34792e18a1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,689,188 reviews.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if not os.path.isfile(\"electronics_topic_modeling.parquet\"):\n",
    "    reviews = pd.read_json(\n",
    "        \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz\",\n",
    "        lines=True,\n",
    "    )[[\"reviewText\", \"overall\"]] # we'll use the \"oveall\" column in the next notebook\n",
    "    reviews.to_parquet(\"electronics_topic_modeling.parquet\")\n",
    "else:\n",
    "    reviews = pd.read_parquet(\"electronics_topic_modeling.parquet\")\n",
    "    \n",
    "reviews = reviews.dropna()\n",
    "reviews[\"overall\"] = [\n",
    "    1 if i > 3 else 0 if i < 3 else np.nan \n",
    "    for i in reviews[\"overall\"]\n",
    "]\n",
    "print(f\"{reviews.shape[0]:,} reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dbe1bea-39fd-419d-966e-b7424cd00fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541b72ff28c64131bc4a0f3c198532fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/1.69M [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocessing the text for Word2Vec\n",
    "from joblib import Parallel, delayed\n",
    "from gensim.parsing import preprocessing\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def preprocess(s):\n",
    "    \"\"\"Apply some of gensim's preprocessing tools.\"\"\"\n",
    "    s = preprocessing.strip_punctuation(s)\n",
    "    s = preprocessing.strip_numeric(s)\n",
    "    s = preprocessing.remove_stopwords(s.lower())\n",
    "    s = preprocessing.strip_short(s)\n",
    "    s = preprocessing.stem_text(s)\n",
    "    return s.split()\n",
    "\n",
    "parsed = Parallel(-1)(\n",
    "    delayed(preprocess)(i)\n",
    "    for i in tqdm(reviews[\"reviewText\"], unit_scale=True, smoothing=0)\n",
    ")\n",
    "\n",
    "# We'll re-use this later, so save it back to the dataframe.\n",
    "reviews[\"Preprocessed\"] = parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7bd84ef-079a-4471-bc11-889400d4f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment variables, like before\n",
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MLK_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f3e10f-8d41-4d84-a37c-7cb7a7580f59",
   "metadata": {},
   "source": [
    "Like Gensim's LDA, Word2Vec does have a method for adding callbacks that run at the end of each pass over the data.  But these only run at the end of the epoch, so we'll re-use the `Corpus` class from the last notebook to track progress _within_ each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f431337c-c4cf-4f73-b55b-4e8a6bc9f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.n_passes = 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.it)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in tqdm(\n",
    "            self.it,\n",
    "            desc=f\"Pass {self.n_passes}\",\n",
    "            unit_scale=True,\n",
    "            smoothing=0,\n",
    "        ):\n",
    "            yield i\n",
    "        self.n_passes += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99b887ee-e0a8-4ee7-bb76-d0674645c1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1a1823f50c45c1828e1fde29d8dc87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pass 1:   0%|          | 0.00/1.04M [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84efb0e00a8f4509b88b661202732e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pass 2:   0%|          | 0.00/1.04M [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dedca3b878f409d9d161886a3eb6779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pass 3:   0%|          | 0.00/1.04M [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca41a279125c4b47868112c25e4b9fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pass 4:   0%|          | 0.00/1.04M [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f53eb50cdf4d4ead9ae9f606e074d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pass 5:   0%|          | 0.00/1.04M [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d815fb04745d4dd18d5f15b2af8413b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pass 6:   0%|          | 0.00/1.04M [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112d7ed3f2164ef38fe7e9eadd0f7db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pass 7:   0%|          | 0.00/1.04M [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac6f90b26d14ae38590acdcde731857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pass 8:   0%|          | 0.00/1.04M [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0864de4a899649e9ab8f827bc368b257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pass 9:   0%|          | 0.00/1.04M [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56ddd9ddc794b87ad5901f110654dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pass 10:   0%|          | 0.00/1.04M [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9843afb4b540a88da15cc7927834b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pass 11:   0%|          | 0.00/1.04M [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# Remove any very short documents.  I've picked a 20 word\n",
    "# threshold more or less at random.\n",
    "parsed = [i for i in parsed if len(i) >= 20]\n",
    "\n",
    "w2v = Word2Vec(\n",
    "    sentences=Corpus(parsed), # iterable of list of strings\n",
    "    vector_size=300, # 300-dimensional vectors--pretty standard size\n",
    "    sg=0, # skip-gram sampling\n",
    "    hs=0, # hierarchical softmax\n",
    "    workers=10,\n",
    "    epochs=10, # 2 passes over the data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68265aa8-8c2b-44f8-98f3-c3077e37c2cd",
   "metadata": {},
   "source": [
    "Note that this took some time.  You can tweak some of the settings to make this go faster, but training a Word2Vec model just takes some time.  _However:_ it is very common to train a very large Word2Vec model once, and re-use it for multiple projects later.  E.g., you may train one large Word2Vec model on a huge amount of learner writing data (forum posts, tweets, assignments, you name it), and re-use that same embedding model when you need to vectorize some text in the future.  Training a huge model and re-using it makes this a one-time cost.\n",
    "\n",
    "Side note: this idea of \"train it once, re-use it for a lot of things\" is pretty similar to _transfer learning,_ which has taken over NLP with the advent of Transformer neural network models.  Transformers, like Word2Vec, are trained once on a very large dataset, then re-used by researchers and practitioners later.  Unlike Word2Vec, though, it's extremely common to fine-tune the Transformer model on your particular task, allowing it to \"adapt\" to the specifics of your data.  There are a few ways you can do this with word embeddings--you could put them into an embedding layer of a neural network, essentially using them as \"starting values\" that the network will update; or, you can sometimes continue training the model by adding more specific data later, but thisget pretty complicated.  In practice, word embeddings are rarely fine-tuned for a particular dataset or task, though.\n",
    "\n",
    "We can get the vector for a single word by accessing the `.wv` attribute of the trained model, which behaves like a dictionary that stores word-vector mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61df5218-9bcd-4e53-baa5-8f1cccb2cb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.42554554  0.62747514 -0.22150028  0.944202   -1.1443033   0.20978229\n",
      " -0.65895957 -1.3480129   0.37489632  1.4652174  -0.85104567 -1.097804\n",
      " -1.3921007   1.1738586  -0.28471017 -0.2805354  -2.2720065   0.32445905\n",
      " -0.19563293 -1.1132339  -0.4860228  -1.0906961  -0.6809474   2.2965057\n",
      " -0.19865383  1.1043419   0.8857479  -1.2602749   0.74213815  0.02746249\n",
      " -0.85240495  2.7085278  -0.5600649  -0.04280163 -0.92991936  0.41168886\n",
      " -0.5439028   0.4707479  -0.5980841   0.28451464  1.1783094  -0.4997206\n",
      " -0.45165917  0.8025407  -2.1046035   0.6293314  -1.702422    0.19345719\n",
      "  0.79553413 -0.3079303   0.5627185  -0.8282605  -0.6337152   0.69203264\n",
      " -2.5596092   2.0322492   0.521643   -0.06507634 -1.4132161   1.7050178\n",
      " -2.25428    -1.2045457  -1.1985768   0.24758787  1.1664797  -0.40271232\n",
      "  1.7382778  -0.14926419  2.1421995   2.252167    1.787789    1.7050977\n",
      "  0.65031564 -1.2845238   0.52689946  0.01537131  0.9466607  -0.7311194\n",
      "  1.0215     -3.6198084   1.2724478   3.3253195   0.5235884  -1.5554594\n",
      "  0.84717005 -3.8309283   1.1072521  -1.5115271   1.0050216  -1.2076381\n",
      " -2.621387    0.3303773  -0.22918113 -0.716387    1.7485152   0.5379647\n",
      " -0.05777403  1.7230047   3.0352778  -1.3712054   0.8790868  -0.19776478\n",
      "  0.9718093  -0.4297739   2.0993817   0.02010626 -1.4665557  -0.2979824\n",
      " -1.5005335  -1.9452002  -1.3608562  -1.9434161  -2.796149    0.30737478\n",
      "  0.86702675  2.4571679   0.40394634 -0.58150136 -2.6159344   0.13893345\n",
      "  1.0568912   3.0856128   2.028211    0.8721442   0.4024745   2.2929387\n",
      " -0.6122476   0.2618207   2.3956096  -0.54747397 -0.17852265  1.1940634\n",
      "  0.81347394 -2.6019964  -0.3543997   1.5263076   0.68615276 -1.4596757\n",
      " -0.7540392  -3.3557427   0.15099423  0.93299586 -1.6474471   2.6589608\n",
      "  2.3917131  -1.2475582   0.65837437 -1.0090169   1.3358437  -1.7222316\n",
      "  1.2998066  -0.935824    0.5764718  -0.7236384  -1.0606918   0.21660684\n",
      "  1.3334916  -2.3661056  -0.7392636   0.16538066  1.2511833  -0.56231236\n",
      " -0.8473473   0.8901711   0.85334146  0.15333468  0.01367634  1.6462953\n",
      " -1.8391421   1.6453007  -0.03707594  1.4914061   0.01019159 -1.7487568\n",
      " -1.2723532  -0.43149284  1.576228   -0.40694848 -1.1763693  -0.3806484\n",
      "  1.1787139  -0.4708149  -0.07039511 -0.01709476  0.524276   -0.5619075\n",
      " -0.4159774  -2.5826     -1.2894838   1.035625    0.43591073  1.3130459\n",
      "  0.6107892   0.34280396  0.46182424 -0.88802016 -0.5899548   0.73784673\n",
      " -1.1019043   1.4177017   1.2537174   0.32281068 -1.9921987  -0.5383769\n",
      " -1.428919   -0.26400307  1.4474117  -0.41765726 -0.6270804   0.01755109\n",
      " -2.2889943   1.4356549   0.11197027  0.5964854  -1.0123782  -1.2102777\n",
      "  3.0947406  -0.16810106  3.0164177  -1.0548759   1.5497544  -0.8809046\n",
      " -1.7575543   1.659952    0.36301422 -0.09276488 -0.5686752  -1.1587968\n",
      "  1.1669868  -0.98547846 -0.52221155  0.5009829  -0.5522045  -0.4739889\n",
      "  0.57606584  1.1907935  -0.8745511  -0.53782254  0.94870514  3.2224357\n",
      "  0.22216435  0.5338558  -1.5919014  -0.9295038   0.07845271  0.03309979\n",
      "  0.03003245 -1.10305    -1.308584   -2.2701879   0.5748332   3.1908865\n",
      " -0.00538556 -0.44751903  0.9107842   0.5029317  -0.76540935 -0.9850846\n",
      " -1.5235392   0.40896666 -1.1308696  -2.863101   -1.0381156   1.168314\n",
      "  0.28885773 -1.1749489  -0.6715087   2.9577925   1.3598467  -0.31462023\n",
      "  1.6452141   0.6352597  -1.1288236   1.4467572  -0.9685238   0.3629485\n",
      " -1.8114274  -0.11873799  1.948497    1.8373961  -0.6270303  -0.05218105\n",
      "  2.9670238   0.12888177  0.7119386  -0.8263679   0.93900335 -0.6239759\n",
      " -0.2436305   0.37601107 -1.6361123   0.7317687   0.39012566 -1.3152434\n",
      " -0.98942333  0.20678352 -2.5773287   0.3232097  -2.3521018  -0.00813917]\n"
     ]
    }
   ],
   "source": [
    "print(w2v.wv[\"camera\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872d3a58-4796-4409-bdb5-0cdf08e89d2c",
   "metadata": {},
   "source": [
    "We can also ask the model to give us the words that are most similar to a word we're interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69428f0f-759e-4b39-94d3-d6eb5e37a853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simiularity('camera', 'dslr') = 0.7348\n",
      "simiularity('camera', 'slr') = 0.6980\n",
      "simiularity('camera', 'nikon') = 0.6917\n",
      "simiularity('camera', 'canon') = 0.6808\n",
      "simiularity('camera', 'camer') = 0.6659\n",
      "simiularity('camera', 'camcord') = 0.6652\n",
      "simiularity('camera', 'shoot') = 0.6399\n",
      "simiularity('camera', 'len') = 0.6348\n",
      "simiularity('camera', 'dlsr') = 0.6328\n",
      "simiularity('camera', 'olympu') = 0.6226\n"
     ]
    }
   ],
   "source": [
    "for word, similarity in w2v.wv.most_similar(\"camera\"):\n",
    "    print(f\"simiularity('camera', {repr(word)}) = {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b48eaf-fd8f-4641-b7fb-ce040a59b4d4",
   "metadata": {},
   "source": [
    "And, we can vectorize each document by just taking the element-wise sum of the vectors for each word within it.  This is the representation we'll use for building this final version of our score predictor.\n",
    "\n",
    "Note: there are a few things that might go wrong here.  Word2Vec, by default, filters out words below a certain frequency threshold (anything appearing in <5 documents).  This can be changed, but I've opted not to change it for this notebook.  So, when we're vectorizing a document, we need to make sure we:\n",
    "1. Preprocess it the same way we preprocessed the texts for Word2Vec.\n",
    "2. Do something with words that aren't in the word2vec model.  Since we'll be summing up word vectors, we can just treat these as being all-zero, or skip them.\n",
    "3. We probably want to filter out any documents that have no words that show up in the word2vec model.\n",
    "\n",
    "Since the vectorization step migth take some time, we'll do all our filtering before we get there.  This might mean we end up with a slight imbalance in our classes, but it shouldn't be big enough to be a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d5a848b-bcfc-45f5-a42c-253e8f22d5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85466bb08ef44c758416b32112448dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_document(text, w2v):\n",
    "    text = np.array([\n",
    "        # w2v.wv stores the word-vector mapping\n",
    "        w2v.wv[i]\n",
    "        if i in w2v.wv\n",
    "        else np.zeros(300)\n",
    "        for i in text\n",
    "    ])\n",
    "    text = np.sum(text, axis=0)\n",
    "    return text\n",
    "\n",
    "# Filter short documents and 3-star reviews\n",
    "reviews = reviews[~pd.isnull(reviews[\"overall\"])]\n",
    "reviews = reviews[[len(i) >= 20 for i in reviews[\"Preprocessed\"]]]\n",
    "\n",
    "# Resample\n",
    "reviews = reviews.groupby(\"overall\").sample(10_000, replace=False)\n",
    "\n",
    "# Vectorize\n",
    "vectors = np.array([\n",
    "    vectorize_document(i, w2v) \n",
    "    for i in tqdm(reviews[\"Preprocessed\"])\n",
    "])\n",
    "targets = reviews[\"overall\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2ade9f2-9d4f-4d24-b89f-e98c884fde11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 18.6 s\n",
      "Wall time: 18.6 s\n",
      "0.858422493157502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andersonh\\Miniconda3\\envs\\PFDA_Month_9\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "    vectors,\n",
    "    targets,\n",
    "    train_size=0.8,\n",
    "    stratify=targets,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"classifier\", LinearSVC(random_state=0, max_iter=2000))\n",
    "])\n",
    "\n",
    "# Just to show how long this can take, I'll use a\n",
    "# %time \"magic command\" in Jupyter.  LinearSVC models\n",
    "# are usually very fast, but this is a *lot* of data.\n",
    "%time clf.fit(train_x, train_y)\n",
    "preds = clf.predict(test_x)\n",
    "print(metrics.f1_score(test_y, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
