{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aae3444-b122-438c-8bd3-debccaeb62a0",
   "metadata": {},
   "source": [
    "# Basic document classification with scikit-learn\n",
    "\n",
    "`scikit-learn` has a few useful tools for doing basic document classification, all contained in the `sklern.feature_extraction.text` module.\n",
    "\n",
    "We'll use a dataset of Amazon reviews, from [https://nijianmo.github.io/amazon/index.html](https://nijianmo.github.io/amazon/index.html), as our demo.  We'll do something a bit tricky: we'll use reviews from one subject area as our training data, and then mix reviews from other product categories for our testing dataset.  We'll do a simple binary classification task: predict whether the number of stars a reviewer gives a product is greater than 3, or less than 3, based purely on the text of the review.  We'll ignore reviews with exactly 3 stars.  This is meant to capture the difference between generally positive and negative reviews, while excluding neutral ones.\n",
    "\n",
    "We could very easily do this as a regression task, but we're going to do it as a classification task for these demos.  To make it into a regression task, just swap the models in these notebooks for regression models, and change the scoring metrics to a regression metric like $R^2$ or mean squared error.  We could also treat this as a multi-class classification problem by leaving in all star values--the code in the cells below actually sets everything up to do this, before filtering out 3-star reviews and collapsing the rest into the two categories.\n",
    "\n",
    "We're going to do a basic _bag-of-words_ analysis, meaning that the features we extract from our text are just word counts.  This has a few advantages:\n",
    "- It's easy.  You can code up a pretty robust, well-tested, production-ready word-counting program, from scratch, in a few days.  Less if you just need something quick and dirty.\n",
    "- It's fairly interpretable.  You can train a model and look at the resulting weights, which will directly map back to a particular word.  So you can see what words are most indicative of the thing you're predicting or analyzing--which can be a good way to audit your model for problems, and can lead to very productive cycles of model/preprocessing revision.\n",
    "- It has a _lot_ of knobs and dials you can tweak to change the behavior.  What you define as a \"word,\" how you filter out words, how you clean up the text, etc.  The effects of a lot of these changes can often be reasoned about before running too much of your model pipeline.\n",
    "\n",
    "There are a few important caveats about this kind of approach, though:\n",
    "- Sparsity.  You'll end up with an _enormously_ sparse matrix (rarely more than 1%--on a good day--of the matric entries will be non-zero).  And you'll rarely have a small subset of features that contain most of your information: it's very common for most features to have a very small contribution, so you end up needing to keep a lot of them in.\n",
    "- Model runtime can be slow.  The matrix will have an enormous number of columns--regularly tens, or hundreds, of thousands--which just takes a long time to crunch through.\n",
    "- Bag-of-words models are extremely crude, blunt instruments.  They only measure a rough approximation of _the meaning of the text,_ and ignore _all_ inforation about structure (both grammatical structures like sentences and clauses, and larger organizational structures like paragraphs and sections and chapters).\n",
    "\n",
    "In spite of those caveats, bag-of-words models are always good to try, since they're easy to throw together and epxeriment with.  Their simplicity also makes them a great baseline to compare other models against: you generally want a model to be better than bag-of-words on some major metric.  That could be speed, or generalization to other linguistic domains, or accuracy, or something else.\n",
    "\n",
    "Let's download the data.  (Note: this code uses Parquet to store the intermediate/locally cached versions of the files; make sure you have `fastparquet` or `pyarrow` installed so Pandas can read and write this file type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93163f40-4fc6-4660-b57f-db031a7ad5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def undersample_majority_classes(df):\n",
    "    \"\"\"Under-sample classes in the dataset so that there's\n",
    "    an equal number of all target classes.\"\"\"\n",
    "    resample_n = min(5_000, df[\"overall\"].value_counts().min())\n",
    "    df = (\n",
    "        df\n",
    "        .groupby(\"overall\")\n",
    "        .sample(resample_n, random_state=0)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "if not all([\n",
    "    os.path.isfile(\"electronics.parquet\"),\n",
    "    os.path.isfile(\"video_games.parquet\"),\n",
    "    os.path.isfile(\"clothes.parquet\"),\n",
    "]):\n",
    "    # the \"reviewText\" field contains the text of the review.\n",
    "    # The \"overall\" field contains the number of stars.\n",
    "    electronics = pd.read_json(\n",
    "        \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz\",\n",
    "        lines=True,\n",
    "    )[[\"reviewText\", \"overall\"]]\n",
    "    electronics = undersample_majority_classes(electronics).assign(productCategory=\"Electronics\")\n",
    "    electronics.to_parquet(\"electronics.parquet\")\n",
    "    \n",
    "    video_games = pd.read_json(\n",
    "        \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Video_Games_5.json.gz\",\n",
    "        lines=True,\n",
    "    )[[\"reviewText\", \"overall\"]]\n",
    "    video_games = undersample_majority_classes(video_games).assign(productCategory=\"Video Games\")\n",
    "    video_games.to_parquet(\"video_games.parquet\")\n",
    "    \n",
    "    clothes = pd.read_json(\n",
    "        \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Clothing_Shoes_and_Jewelry_5.json.gz\",\n",
    "        lines=True,\n",
    "    )[[\"reviewText\", \"overall\"]]\n",
    "    clothes = undersample_majority_classes(clothes).assign(productCategory=\"Clothing and Jewelry\")\n",
    "    clothes.to_parquet(\"clothes.parquet\")\n",
    "else:\n",
    "    electronics = pd.read_parquet(\"electronics.parquet\")\n",
    "    video_games = pd.read_parquet(\"video_games.parquet\")\n",
    "    clothes = pd.read_parquet(\"clothes.parquet\")\n",
    "    \n",
    "# Remove 3-star reviews.\n",
    "electronics = electronics[electronics[\"overall\"] != 3]\n",
    "video_games = video_games[video_games[\"overall\"] != 3]\n",
    "clothes = clothes[clothes[\"overall\"] != 3]\n",
    "\n",
    "# Set the \"overall\" column to be the binary classes of \"positive\"\n",
    "# (for >3) and \"negative\" (<3).\n",
    "electronics[\"overall\"] = [\"Positive\" if i > 3 else \"Negative\" for i in electronics[\"overall\"]]\n",
    "video_games[\"overall\"] = [\"Positive\" if i > 3 else \"Negative\" for i in video_games[\"overall\"]]\n",
    "clothes[\"overall\"] = [\"Positive\" if i > 3 else \"Negative\" for i in clothes[\"overall\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c05f0-ac7a-45f5-af1f-5abc768478ef",
   "metadata": {},
   "source": [
    "Let's create our train-test splits..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d2d272a-1bb6-4e39-bbd1-c3568a5d52c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              reviewText   overall  \\\n",
      "22268  Outstanding. I have the 16mb version of this. ...  Positive   \n",
      "20118  I decided to give this product a try after hav...  Positive   \n",
      "20581  Holds a good amount of weight and works great....  Positive   \n",
      "23548  I bought this to replace a larger switch from ...  Positive   \n",
      "23512  Unlike other players I have bought, this unit ...  Positive   \n",
      "...                                                  ...       ...   \n",
      "4220   I've used a number of different wireless adapt...  Negative   \n",
      "20533  I guess I have a hard time with the uber expen...  Positive   \n",
      "55     I never got it to connect properly. There is a...  Negative   \n",
      "6169   Thought I had already written a review for thi...  Negative   \n",
      "19691  Let me preface this review by saying I'm not m...  Positive   \n",
      "\n",
      "      productCategory  \n",
      "22268     Electronics  \n",
      "20118     Electronics  \n",
      "20581     Electronics  \n",
      "23548     Electronics  \n",
      "23512     Electronics  \n",
      "...               ...  \n",
      "4220      Electronics  \n",
      "20533     Electronics  \n",
      "55        Electronics  \n",
      "6169      Electronics  \n",
      "19691     Electronics  \n",
      "\n",
      "[18000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(\n",
    "    electronics,\n",
    "    train_size=0.9,\n",
    "    stratify=electronics[\"overall\"],\n",
    "    random_state=0,\n",
    ")\n",
    "test = pd.concat((test, video_games, clothes))\n",
    "\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b265d4-36bc-43ff-be34-7d948c724839",
   "metadata": {},
   "source": [
    "Now let's use some of the tools from scikit-learn to convert our text columns into numeric vector format.  We'll just explore what this looks like before we build any models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2edce5d-c7bb-44fd-ab8d-6653abb1977c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "(18000, 38273)\n",
      "  (0, 24353)\t1\n",
      "  (0, 16774)\t1\n",
      "  (0, 33699)\t1\n",
      "  (0, 731)\t1\n",
      "  (0, 36232)\t1\n",
      "  (0, 23792)\t1\n",
      "  (0, 33925)\t1\n",
      "  (0, 19089)\t1\n",
      "  (0, 6134)\t1\n",
      "  (0, 23928)\t2\n",
      "  (0, 22725)\t3\n",
      "  (0, 19557)\t1\n",
      "  (0, 28780)\t1\n",
      "  (0, 37402)\t2\n",
      "  (0, 19586)\t1\n",
      "  (0, 18100)\t1\n",
      "  (0, 25759)\t2\n",
      "  (0, 19716)\t1\n",
      "  (0, 4565)\t3\n",
      "  (0, 8143)\t1\n",
      "  (0, 35898)\t1\n",
      "  (0, 13509)\t1\n",
      "  (0, 10611)\t1\n",
      "  (0, 14921)\t1\n",
      "  (0, 35013)\t1\n",
      "  (0, 38008)\t1\n",
      "  (0, 19046)\t1\n",
      "  (0, 20688)\t1\n",
      "  (0, 23150)\t1\n",
      "  (0, 19524)\t1\n",
      "  (0, 34075)\t1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train_x = CountVectorizer().fit_transform(train[\"reviewText\"])\n",
    "print(type(train_x))\n",
    "print(train_x.shape)\n",
    "print(train_x[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1fe89e-4a12-4498-829e-aa72c87aad1c",
   "metadata": {},
   "source": [
    "The `CountVectorizer()` transformer transforms texts into a spare matrix fromat.  We saw this kind of data structure briefly when we covered the `scipy` library a few months back, but here's the quick refresher.\n",
    "\n",
    "A spare matrix (or sparse array) is a specialized data structure for storing data with _lot_ of zeros.  It takes a lot of shortcuts to essentially not store zero-values at all.  This is critical for data like language, where for anything you measure, you'll end up with mostly zeros for each document.\n",
    "\n",
    "Language is _extremely_ sparse.  Consider: if we're using words as our features, how many words are there in a given corpus of data?  Easily a few tens of thousands of unique words in most non-trivial corpora, but empirically, most documents only use a _very_ small subset of those words.  If most of your documents are a few hundred words, then they can at most use a few hundred words out of those tens of thousands.  In other words: _most documents don't use most words._  And relatedly: _most words only appear in a few documents._\n",
    "\n",
    "Sparsity like this is a _big_ problem for any machine learning or statistical analysis.  The reasons for why are a bit beyond what we're covering today, but just remember: _sparsity is bad._  We want to get rid of it however we can.  Fortunately, the `CountVectorizer()` has a few tools for doing this (though it lacks a few important ones like stemming/lemmatization).  First let's look at our sparsity as-is (which we'll measure as \"the percent of entries in the matrix that are zero\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66b143c7-ef1b-4f5e-a2c3-984c4e525d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 99.803%\n"
     ]
    }
   ],
   "source": [
    "sparsity = 1 - (train_x.nnz / (train_x.shape[0] * train_x.shape[1]))\n",
    "print(f\"Sparsity: {sparsity:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c696141-0596-4db4-9e8b-cc1dcf47e8c4",
   "metadata": {},
   "source": [
    "...or, in other words, only 0.2% of our cells contain non-zero values.  Eek.  Let's use a few easy tools to decrease our sparsity: we'll just filter out words by frequency.  We can rely on two important insights:\n",
    "\n",
    "1. Extremely high-frequency words (e.g. stopwords) tend to contribute very little information.  Often these are \"structure\" or \"function\" words in English.  Their counts tend to have very low variance across documents.\n",
    "2. Extremely low frequency words also contribute very little information.  If a word only apears in a few documents out of hundreds of thousands, it _might_ be able to tell us someting, but probably not a whole lot.\n",
    "\n",
    "So, we can just remove words that are super common and super rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "023860a3-7512-450e-bb92-1ec217f6854c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "(18000, 7055)\n",
      "Sparsity: 99.118%\n"
     ]
    }
   ],
   "source": [
    "train_x = CountVectorizer(\n",
    "    # No words that appear in >50% of documents.\n",
    "    max_df=0.5,\n",
    "    \n",
    "    # No words that appear in <10 documents.\n",
    "    min_df=10,\n",
    ").fit_transform(train[\"reviewText\"])\n",
    "print(type(train_x))\n",
    "print(train_x.shape)\n",
    "sparsity = 1 - (train_x.nnz / (train_x.shape[0] * train_x.shape[1]))\n",
    "print(f\"Sparsity: {sparsity:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c986b62e-1268-42fc-b1b1-0cea297ea603",
   "metadata": {},
   "source": [
    "Note: the `min_df` and `max_df` parameters behave differently depending on whether they're a float or an int.  If they're a float, they must be strictly between 0 and 1, and they will be interpreted as a _percent of documents in the corpus_.  So `max_df=0.5` means \"nothing that appears in more than half of the documents.\"  If the arguments are integers, they're interpreted as the number of documents.  So `min_df=10` means \"nothing that appears in less than 10 documents.\"\n",
    "\n",
    "Note that we still have a _lot_ of sparsity, but we decreased our sparsity (and number of features) by almost a factor of 10!  We can be a bit more aggressive and try to get this down even lower by using some other options in the `CountVectorizer()` tranformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba99379-c090-44c0-b7cf-a6e43978746a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "(18000, 6786)\n",
      "Sparsity: 99.379%\n"
     ]
    }
   ],
   "source": [
    "train_x = CountVectorizer(\n",
    "    # No words that appear in >50% of documents.\n",
    "    max_df=0.5,\n",
    "    \n",
    "    # No words that appear in <10 documents.\n",
    "    min_df=10,\n",
    "    \n",
    "    # Keep only the 10,000 most frequent features.\n",
    "    max_features=10_000,\n",
    "    \n",
    "    # Use a pre-defined list of English stopwords.\n",
    "    # Anything on the list gets removed, regardless of\n",
    "    # the other filtering options.  We could provide our\n",
    "    # own stoplist as a list of words here, but passing\n",
    "    # \"english\" uses a built-in stoplist from scikit-learn.\n",
    "    stop_words=\"english\",\n",
    ").fit_transform(train[\"reviewText\"])\n",
    "print(type(train_x))\n",
    "print(train_x.shape)\n",
    "sparsity = 1 - (train_x.nnz / (train_x.shape[0] * train_x.shape[1]))\n",
    "print(f\"Sparsity: {sparsity:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96f8c1a-921e-4a61-8e3a-7aef059f7da1",
   "metadata": {},
   "source": [
    "Before we come back to way to decreae this sparsity even further, let's throw a quick classifier together.  There isn't much special here compared to other kinds of data, but there are a few considerations that are more important for language data:\n",
    "\n",
    "1. When doing this sort of word-counting analysis, you want to make sure you pick a model that deals well with huge numbers of features.\n",
    "2. You also usually want a model that's fast, because your dataset will often be huge.\n",
    "3. You want a model that can natively deal with sparse datasets without converting them to a dense format.  (This is only a consideration when doing word count analyses like we're doing now).\n",
    "\n",
    "In practice, you can throw pretty much any scikit-learn model at the data, but I find the following tend to consistently work pretty well:\n",
    "- Linear-kernel support vector machines, and `SGDClassifier()`, for binary classification.\n",
    "- Random forests and other ensembles of trees (they tend to be good at everything, though, so this isn't too surprising).\n",
    "- Naive Bayes models tend to excel in bag of words settings: they are extremely fast, scale well to large numbers of features, and tend to be reasonably accurate.  (though they can occasionally be _very_ inaccurate).\n",
    "\n",
    "We'll use a random forest model, just for kicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bdcd230-93d8-4b11-9dce-ff37b271fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.56 s\n",
      "Wall time: 3.56 s\n",
      "Overall F1 score: 0.763788898259823\n",
      "Clothing and Jewelry-specific F1 score: 0.7759023256824613\n",
      "Electronics-specific F1 score: 0.7919700436862909\n",
      "Video Games-specific F1 score: 0.7487776953058092\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\n",
    "        \"feature extraction\",\n",
    "        CountVectorizer(\n",
    "            min_df=10,\n",
    "            max_df=0.5,\n",
    "            stop_words=\"english\",\n",
    "            # max_features=10_000,\n",
    "        )\n",
    "    ),\n",
    "    (\"classifier\", LinearSVC(random_state=0, max_iter=2000))\n",
    "])\n",
    "\n",
    "# Just to show how long this can take, I'll use a\n",
    "# %time \"magic command\" in Jupyter.  LinearSVC models\n",
    "# are usually very fast, but this is a *lot* of data.\n",
    "%time clf.fit(train[\"reviewText\"], train[\"overall\"])\n",
    "test[\"Predictions\"] = clf.predict(test[\"reviewText\"])\n",
    "\n",
    "# Calculate a few different F1 scores.\n",
    "overall_f1 = metrics.f1_score(test[\"overall\"], test[\"Predictions\"], average=\"macro\")\n",
    "print(f\"Overall F1 score: {overall_f1}\")\n",
    "\n",
    "for product_category, results in test.groupby(\"productCategory\"):\n",
    "    f1 = metrics.f1_score(results[\"overall\"], results[\"Predictions\"], average=\"macro\")\n",
    "    print(f\"{product_category}-specific F1 score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8adb84-6bcd-4782-acff-3ffe3925b403",
   "metadata": {},
   "source": [
    "# Improving accuracy with TF-IDF weighting\n",
    "\n",
    "Often, we can gain a boost in accuracy by applying _term frequency-inverse document frequency_ weighting to our word counts.  This combines two different weighting schemes: the \"term frequency\" (TF) weighting an the \"inverse document frequency\" (IDF) weighting.\n",
    "\n",
    "Term frequency is just an $\\ell_1$ normalization applied row-wise to each document.  The word counts in a document are divided by the total number of words, so that all the re-weighted counts sum to 1.  This measurew what proportion of the document each word represents--this is often more desirable than the raw count, since raw counts are highly correlated with overall document length.  (Document length might be important for some analyses, but in general, for bag-of-words, we want to remove or isolate the effects of document length).\n",
    "\n",
    "$$\n",
    "\\operatorname{tf}(word, document) = \\frac{\\text{Number of times }word\\text{ appears in }document}{\\text{Number of words in }document}\n",
    "$$\n",
    "\n",
    "Sometimes you'll see term frequency calculated with $\\ell_2$ norm; this just means the denominator in the above example changes to the sum of the _squares_ of word counts, so that the _squared_ term frequencies sum to 1.  (This has some other mathematical advantages, mostly that the dot product between two vectors is now equivalent to the cosine similarity, but is much faster to compute; cosine similarity is used a lot in NLP tasks).\n",
    "\n",
    "Inverse document frequency is the negative logarithm of the proportion of documents in the corpus that contain the target word.  I.e.:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\operatorname{idf}(word) &= -\\log\\left(\\frac{\\text{Number of documents containing } term}{\\text{Total number of documents}}\\right) \\\\\n",
    "&= \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing } term}\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "TF-IDF is just the product of these two measures for each word.  We can apply TF-IDF transforms two ways in scikit-learn: either by running the `sklearn.feature_extraction.text.TfidfTransformer()` on the results of the `CountVectorizer()`, or by using the `sklearn.feature_extraction.text.TfidfVectorizer()` transformer, which combines these two steps for us.  We might want to keep them separate if we're applying custom filtering/feature selection steps to the raw word counts, but usually, you can just use the `TfidfVectorizer()` from the get-go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a3a9eaa-f8c0-40b3-986e-c2ddea2c0e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.47 s\n",
      "Wall time: 1.48 s\n",
      "Overall F1 score: 0.7939126991347976\n",
      "Clothing and Jewelry-specific F1 score: 0.807468314110313\n",
      "Electronics-specific F1 score: 0.8294812003023333\n",
      "Video Games-specific F1 score: 0.7766162021081828\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\n",
    "        \"feature extraction\",\n",
    "        # TfidfVectorizer supports all the same options as CountVectorizer,\n",
    "        # plus a few others that we're not setting\n",
    "        TfidfVectorizer(\n",
    "            min_df=10,\n",
    "            max_df=0.5,\n",
    "            stop_words=\"english\",\n",
    "            # max_features=10_000,\n",
    "        )\n",
    "    ),\n",
    "    (\"classifier\", LinearSVC(random_state=0, max_iter=2000))\n",
    "])\n",
    "\n",
    "# Just to show how long this can take, I'll use a\n",
    "# %time \"magic command\" in Jupyter.  LinearSVC models\n",
    "# are usually very fast, but this is a *lot* of data.\n",
    "%time clf.fit(train[\"reviewText\"], train[\"overall\"])\n",
    "test[\"Predictions\"] = clf.predict(test[\"reviewText\"])\n",
    "\n",
    "# Calculate a few different F1 scores.\n",
    "overall_f1 = metrics.f1_score(test[\"overall\"], test[\"Predictions\"], average=\"macro\")\n",
    "print(f\"Overall F1 score: {overall_f1}\")\n",
    "\n",
    "for product_category, results in test.groupby(\"productCategory\"):\n",
    "    f1 = metrics.f1_score(results[\"overall\"], results[\"Predictions\"], average=\"macro\")\n",
    "    print(f\"{product_category}-specific F1 score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37974b60-3a63-4283-8645-4b26af273b32",
   "metadata": {},
   "source": [
    "# Some other tricks\n",
    "\n",
    "Since `CountVectorizer()`/`TfidfVectorizer()` just convert text into sparse matrices, we can do our own feature selection or dimensionality reduction or whatever we want.  E.g., it's pretty common to use SVD to do dimensionality reduction, which increases the speed of fitting models by _a lot,_ at the cost of only a little accuracy.  (though the SVD step can take a long time on some datasets).  We'll revisit SVD a bit later; bag of words + SVD is actually a common topic modeling algorithm called Latent Semantic Analysis (LSA; sometimes called Latent Semantic Indexing, LSI)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
