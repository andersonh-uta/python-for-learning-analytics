{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d4757a-dbbc-465e-afb3-707da78a9b7a",
   "metadata": {},
   "source": [
    "# Document annotations with spaCy\n",
    "\n",
    "This will be a pretty quick notebook: we'll use spaCy to generate dense, 300-dimensional word vectors for each document in our dataset, and directly run predictions on those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "771a2d81-1d88-43a5-8fdc-ead66e436df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same data loading code as before\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def undersample_majority_classes(df):\n",
    "    \"\"\"Under-sample classes in the dataset so that there's\n",
    "    an equal number of all target classes.\"\"\"\n",
    "    resample_n = min(5_000, df[\"overall\"].value_counts().min())\n",
    "    df = (\n",
    "        df\n",
    "        .groupby(\"overall\")\n",
    "        .sample(resample_n, random_state=0)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "if not all([\n",
    "    os.path.isfile(\"electronics.parquet\"),\n",
    "    os.path.isfile(\"video_games.parquet\"),\n",
    "    os.path.isfile(\"clothes.parquet\"),\n",
    "]):\n",
    "    # the \"reviewText\" field contains the text of the review.\n",
    "    # The \"overall\" field contains the number of stars.\n",
    "    electronics = pd.read_json(\n",
    "        \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz\",\n",
    "        lines=True,\n",
    "    )[[\"reviewText\", \"overall\"]]\n",
    "    electronics = undersample_majority_classes(electronics).assign(productCategory=\"Electronics\")\n",
    "    electronics.to_parquet(\"electronics.parquet\")\n",
    "    \n",
    "    video_games = pd.read_json(\n",
    "        \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Video_Games_5.json.gz\",\n",
    "        lines=True,\n",
    "    )[[\"reviewText\", \"overall\"]]\n",
    "    video_games = undersample_majority_classes(video_games).assign(productCategory=\"Video Games\")\n",
    "    video_games.to_parquet(\"video_games.parquet\")\n",
    "    \n",
    "    clothes = pd.read_json(\n",
    "        \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Clothing_Shoes_and_Jewelry_5.json.gz\",\n",
    "        lines=True,\n",
    "    )[[\"reviewText\", \"overall\"]]\n",
    "    clothes = undersample_majority_classes(clothes).assign(productCategory=\"Clothing and Jewelry\")\n",
    "    clothes.to_parquet(\"clothes.parquet\")\n",
    "else:\n",
    "    electronics = pd.read_parquet(\"electronics.parquet\")\n",
    "    video_games = pd.read_parquet(\"video_games.parquet\")\n",
    "    clothes = pd.read_parquet(\"clothes.parquet\")\n",
    "    \n",
    "# Remove 3-star reviews.\n",
    "electronics = electronics[electronics[\"overall\"] != 3]\n",
    "video_games = video_games[video_games[\"overall\"] != 3]\n",
    "clothes = clothes[clothes[\"overall\"] != 3]\n",
    "\n",
    "# Set the \"overall\" column to be the binary classes of \"positive\"\n",
    "# (for >3) and \"negative\" (<3).\n",
    "electronics[\"overall\"] = [\"Positive\" if i > 3 else \"Negative\" for i in electronics[\"overall\"]]\n",
    "video_games[\"overall\"] = [\"Positive\" if i > 3 else \"Negative\" for i in video_games[\"overall\"]]\n",
    "clothes[\"overall\"] = [\"Positive\" if i > 3 else \"Negative\" for i in clothes[\"overall\"]]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(\n",
    "    electronics,\n",
    "    train_size=0.9,\n",
    "    stratify=electronics[\"overall\"],\n",
    "    random_state=0,\n",
    ")\n",
    "test = pd.concat((test, video_games, clothes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27594378-f765-4fe6-9d28-79d4bf359be0",
   "metadata": {},
   "source": [
    "Only the `en_core_web_lg` model has word vectors, so we need to load that. Fortunately, we can disable basically all the components to make it faster.  _Note:_ I've often found that, for reasons I don't entirely understand yet, using `n_process` with `nlp.pipe()` for vectorization seems to slow things down a lot.  So, we'll go single-threaded here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eca42ed0-56fb-44b6-8e2b-5cbc4b6c4671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94f913ae45642e1a0660ee0681b5c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42f6c2890f9462bb262209013186081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "train_x = np.array([\n",
    "    # make_doc() will apply vectors, so we'll use it for speed\n",
    "    nlp.make_doc(i).vector\n",
    "    for i in tqdm(train[\"reviewText\"])\n",
    "])\n",
    "test_x = np.array([\n",
    "    nlp.make_doc(i).vector\n",
    "    for i in tqdm(test[\"reviewText\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45d06b70-7aab-486f-8d42-dbc7a511d542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andersonh\\Miniconda3\\envs\\PFDA_Month_9\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 22.2 s\n",
      "Wall time: 22.2 s\n",
      "Overall F1 score: 0.787759197608813\n",
      "Clothing and Jewelry-specific F1 score: 0.7893347345330886\n",
      "Electronics-specific F1 score: 0.8023999649822723\n",
      "Video Games-specific F1 score: 0.783445178481015\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"classifier\", LinearSVC(random_state=0, max_iter=2_000)),\n",
    "])\n",
    "%time clf.fit(train_x, train[\"overall\"])\n",
    "test[\"Predictions\"] = clf.predict(test_x)\n",
    "\n",
    "# Calculate a few different F1 scores.\n",
    "overall_f1 = metrics.f1_score(test[\"overall\"], test[\"Predictions\"], average=\"macro\")\n",
    "print(f\"Overall F1 score: {overall_f1}\")\n",
    "\n",
    "for product_category, results in test.groupby(\"productCategory\"):\n",
    "    f1 = metrics.f1_score(results[\"overall\"], results[\"Predictions\"], average=\"macro\")\n",
    "    print(f\"{product_category}-specific F1 score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30669e36-288d-417b-845d-db140572dee0",
   "metadata": {},
   "source": [
    "Even though the accuracy may seem to have gone down a bit compared to the bag of words approach, there are actually a _lot_ of benefits to this sort of dense vector representation.  Mostly, it allows much better generalizeability.  Bag-of-words has a problem in that every word is orthogonal to every other word: the difference between \"cat\" and \"dog\" is the same as the distance between \"cat\" and \"nuclear.\"  In a dense representation, this is not necessarily true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10ac9704-4c5d-470d-ba29-b6f3a9ae5e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8016854705531046\n",
      "0.1296658079441922\n"
     ]
    }
   ],
   "source": [
    "cat = nlp(\"cat\")\n",
    "dog = nlp(\"dog\")\n",
    "nuclear = nlp(\"nuclear\")\n",
    "print(cat.similarity(dog))\n",
    "print(cat.similarity(nuclear))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90db926b-270e-4f00-a35e-dd63877a2012",
   "metadata": {},
   "source": [
    "(This results not from the denseness itself, but rather, from the way that these dense representations have been trained).  So, by using these kinds of representations, our model can better generalize to words that weren't seen during training, but which have vectors in the vector model.  This lets us provide a bit of \"prior knowledge\" to the models, which can be extremely useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
