{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "305cd51b-7e68-4e25-a551-6fde3ec606de",
   "metadata": {},
   "source": [
    "# Document clasification with spaCy\n",
    "\n",
    "spaCy is an awesome library.  It provides a number of large, pre-trained language model that do all sorts of cool stuff:\n",
    "- Smart tokenization: much smarter than most of the character-pattern-based splitting tools, like scikit-learn's `CountVectorizer()` uses.\n",
    "- Part of speech annotation, syntactic dependency parsing, named entity recognition, stopword identification, lemmatization, and a few other word-level annotations: it runs these automatically for you.\n",
    "- (with the right models) Dense word vectors: we'll come back to the details of this later, but each word is mapped to a dense, 300-dimensional vector that encodes something about its meaning.  Documents, words, and spans of words are assigned a single vector.\n",
    "- Document classification with pretty big, accurate models--we won't be covering this.\n",
    "- Lots of modularity: you can swap out different parts of the model with your own, e.g., swapping out the tokenizer for something custom, or using a different stopword list, or changing how lemmatization works.  We don't be doing any of this.\n",
    "- GPU acceleration for some models--we don't be doing this.\n",
    "\n",
    "spaCy is a very large, deep library, but we're only going to do a few basic thing with it.  Namely, we're going to use it to do smarter tokenization, remove stopwords, and lemmatize our text before feeding it back into scikit-learn's `CountVectorizer()`.  (Basically: using spaCy for smarter preprocessing). We'll also see a few ways to pick-and-choose what things we run, which might be desirable for speed.\n",
    "\n",
    "Install spaCy with:\n",
    "```bash\n",
    "conda install spacy\n",
    "```\n",
    "\n",
    "Or follow the [installation instructions](https://spacy.io/usage) if you want, e.g., to install with GPU support.  Download the `en_core_web_sm` model to run this notebook yourself.\n",
    "\n",
    "Then, we'll use spaCy's built-in vectorization and compare that to a bag-of-words approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "393d5b2b-741e-4bd9-96a1-56d68b8c67b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# make sure to download the models with: `python -m spacy download [model name]`.\n",
    "# Then load the models with `model = spacy.load(\"model name\")`.\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57680c6-1954-40fa-b659-8f7df55b4c06",
   "metadata": {},
   "source": [
    "The `nlp` object is callable, like a function.  It takes in a string, and returns a spaCy `Document`, which contains a list of `Tokens`, each of which has a bunch of different annotations set.  The example below does not show all of the annotations that get set, only the ones that I use most often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dde8b4b-629c-4c37-bdfc-e804d6250676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy (/speɪˈsiː/ spay-SEE) is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython.\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"spaCy (/speɪˈsiː/ spay-SEE) is an open-source software library for \"\n",
    "    \"advanced natural language processing, written in the programming \"\n",
    "    \"languages Python and Cython.\"\n",
    ")\n",
    "\n",
    "doc = nlp(text)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "888c8362-3b26-4f98-ae76-0e5f60ff9ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN          LEMMA          COARSE POS FINE POS  STOPWORD? SYNTACTIC ROLE\n",
      "spaCy          spacy          NUM        CD        0         nsubj\n",
      "(              (              PUNCT      -LRB-     0         punct\n",
      "/speɪˈsiː/     /speɪˈsiː/     PUNCT      NFP       0         compound\n",
      "spay           spay           PROPN      NNP       0         compound\n",
      "-              -              PUNCT      HYPH      0         punct\n",
      "SEE            SEE            PROPN      NNP       1         nsubj\n",
      ")              )              PUNCT      -RRB-     0         punct\n",
      "is             be             AUX        VBZ       1         ROOT\n",
      "an             an             DET        DT        1         det\n",
      "open           open           ADJ        JJ        0         amod\n",
      "-              -              PUNCT      HYPH      0         punct\n",
      "source         source         NOUN       NN        0         compound\n",
      "software       software       NOUN       NN        0         compound\n",
      "library        library        NOUN       NN        0         attr\n",
      "for            for            ADP        IN        1         prep\n",
      "advanced       advanced       ADJ        JJ        0         amod\n",
      "natural        natural        ADJ        JJ        0         amod\n",
      "language       language       NOUN       NN        0         compound\n",
      "processing     processing     NOUN       NN        0         pobj\n",
      ",              ,              PUNCT      ,         0         punct\n",
      "written        write          VERB       VBN       0         acl\n",
      "in             in             ADP        IN        1         prep\n",
      "the            the            DET        DT        1         det\n",
      "programming    programming    NOUN       NN        0         compound\n",
      "languages      language       NOUN       NNS       0         pobj\n",
      "Python         Python         PROPN      NNP       0         dobj\n",
      "and            and            CCONJ      CC        1         cc\n",
      "Cython         Cython         PROPN      NNP       0         conj\n",
      ".              .              PUNCT      .         0         punct\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Token':<15}{'Lemma':<15}{'Coarse POS':<11}{'Fine POS':<10}{'Stopword?':<10}{'Syntactic Role'}\".upper())\n",
    "\n",
    "for tok in doc:\n",
    "    text = tok.text\n",
    "    lemma = tok.lemma_\n",
    "    pos = tok.pos_\n",
    "    fine_grained_pos = tok.tag_\n",
    "    stopword = tok.is_stop\n",
    "    syntactic_role = tok.dep_\n",
    "    print(f\"{text:<15}{lemma:<15}{pos:<11}{fine_grained_pos:<10}{stopword:<10}{syntactic_role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b98146e-1c1c-48e3-bcbd-0b6d6e104527",
   "metadata": {},
   "source": [
    "Let's use this to tokenize our text data from last time.  First, we need to re-load the training data (this is the same loading code from the last notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "172a2c22-c404-435b-8714-4213824bce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "electronics = pd.read_parquet(\"electronics.parquet\")\n",
    "video_games = pd.read_parquet(\"video_games.parquet\")\n",
    "clothes = pd.read_parquet(\"clothes.parquet\")\n",
    "    \n",
    "# Remove 3-star reviews.\n",
    "electronics = electronics[electronics[\"overall\"] != 3]\n",
    "video_games = video_games[video_games[\"overall\"] != 3]\n",
    "clothes = clothes[clothes[\"overall\"] != 3]\n",
    "\n",
    "# Set the \"overall\" column to be the binary classes of \"positive\"\n",
    "# (for >3) and \"negative\" (<3).\n",
    "electronics[\"overall\"] = [\"Positive\" if i > 3 else \"Negative\" for i in electronics[\"overall\"]]\n",
    "video_games[\"overall\"] = [\"Positive\" if i > 3 else \"Negative\" for i in video_games[\"overall\"]]\n",
    "clothes[\"overall\"] = [\"Positive\" if i > 3 else \"Negative\" for i in clothes[\"overall\"]]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(\n",
    "    electronics,\n",
    "    train_size=0.9,\n",
    "    stratify=electronics[\"overall\"],\n",
    "    random_state=0,\n",
    ")\n",
    "test = pd.concat((test, video_games, clothes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb67ab3e-f26f-42e2-a229-8af489bb8cb4",
   "metadata": {},
   "source": [
    "# Simple use: apply the full processing pipeline\n",
    "\n",
    "We'll write a quick function that applies a spaCy model to a bunch of texts, and returns the lemmas for all non-stopword, non-punctuation, non-whitespace tokens.  We can use `nlp.pipe()` to pass an iterable of texts, and process them a bit more efficiently than explicitly looping over them. (this mostly only matters if we're running on the GPU, which we aren't, so we won't see much of a speedup).\n",
    "\n",
    "We _can_ tell spaCy to parse our text using multiple threads.  I would recommend being _very_ careful about doing this with the `*_lg` models.  spaCy will create a full copy of the model for each processing thread, which can quickly eat into your RAM.  It also adds a lot of startup time.  If you're doing to be using multiple threads, you should probably use one of the `*_sm` models, unless you have a lot of RAM and you need the extra accuracy from the `*_lg` models.\n",
    "\n",
    "Since we have a lot of data, I'm going to use the English small model (you'll need to install it to run this cell), run it in 4 parallel threads, and set a moderate batch size so data can be more efficiently shuttled back and forth between worker processes.  Note: the number of processes might not always be a \"more is faster\" setting--there's a lot of overhead involved in sending data back and forth between the worker processes.\n",
    "\n",
    "Note: `nlp.pipe` can accept any iterator--not just lists--and it returns an iterator that only processes documents on-demand, as you iterate through the results.  This makes it extremely easy to use spaCy for processing _enormous_ amounts of text.  Just create a generator/lazy iterator that reads through lines in a file, point `nlp.pipe` at that generator, and save the results out to another file as you get them and do whatever processing you need to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd3ccf0b-aaaf-4192-8256-e9bff30bc976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def spacy_clean(nlp, texts):\n",
    "    docs = list(tqdm(\n",
    "        nlp.pipe(texts, n_process=4, batch_size=1000),\n",
    "        total=len(texts),\n",
    "        desc=\"spaCy parsing\",\n",
    "        smoothing=0.01,\n",
    "    ))\n",
    "    docs = [\n",
    "        \" \".join(\n",
    "            tok.lemma_\n",
    "            for tok in doc\n",
    "            if not (\n",
    "                tok.is_stop\n",
    "                or tok.is_space\n",
    "                or tok.is_punct\n",
    "            )\n",
    "        )\n",
    "        for doc in tqdm(docs, desc=\"Filtering tokens (after spaCy parsing)\")\n",
    "    ]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6e7d079-6157-44c6-9211-1e80b9ce43b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219578f7a3024996957389eaa01c6008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spaCy parsing:   0%|          | 0/18000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cd8f9217344e14ae3f946f29332427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering tokens (after spaCy parsing):   0%|          | 0/18000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-cleaning:\n",
      "Outstanding. I have the 16mb version of this. Its been on my key ring with my keys in my pocket with pocket knife and change and used every day for two years. It looks new and keeps on ticking\n",
      "\n",
      "Post-cleaning:\n",
      "outstanding 16 mb version key ring key pocket pocket knife change day year look new keep tick\n"
     ]
    }
   ],
   "source": [
    "# This is going to take a long time--we're about to speed it up a lot,\n",
    "# so we're only going to run on a subset of the texts.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "cleaned = spacy_clean(nlp, train[\"reviewText\"])\n",
    "\n",
    "print(f\"Pre-cleaning:\\n{train['reviewText'].iloc[0]}\")\n",
    "print()\n",
    "print(f\"Post-cleaning:\\n{cleaned[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b310718-423f-4f49-9137-12c6c4a601ca",
   "metadata": {},
   "source": [
    "Eesh.  That took a long time.  There's got to be a way to speed that up.\n",
    "\n",
    "# Disabling processing steps for speed\n",
    "\n",
    "Fortunately there is!  When loading a spaCy model, or doing anything with it, we can disable some of the processing steps that we aren't using.  First, here's a list of the processing steps that our small NLP model is using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29d335ba-a415-4733-b042-5b7baf8b1820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok2vec\n",
      "tagger\n",
      "parser\n",
      "attribute_ruler\n",
      "lemmatizer\n",
      "ner\n"
     ]
    }
   ],
   "source": [
    "for step_name, step in nlp.pipeline:\n",
    "    print(step_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1c017e-cbcb-4d3d-b1e4-06d8c8ccf17a",
   "metadata": {},
   "source": [
    "The `tagger` step does part of speech tagging, which we're not using right now.  The `parser` step does syntactic parsing, which we also aren't using.  The `ner` step does named entity recognition, which we also aren't using.  So let's disable all of those steps, and run the above code again, and see how much speed we gain.  (the spaCy documentation has more information on available steps, including ones you can add to a model after the fact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "536f1988-d3d7-42a8-a14a-76828d2c4729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efed97a718964814846f698c92db0a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spaCy parsing:   0%|          | 0/18000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72dae8abcfeb40a1bdb0bcaf2ceddaa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering tokens (after spaCy parsing):   0%|          | 0/18000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938d7750ce3b402a8c6f27a79b9e1bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spaCy parsing:   0%|          | 0/42000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3181954b2741a6be5213a00fa15ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering tokens (after spaCy parsing):   0%|          | 0/42000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "train[\"Cleaned Text\"] = spacy_clean(nlp, train[\"reviewText\"])\n",
    "test[\"Cleaned Text\"] = spacy_clean(nlp, test[\"reviewText\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839216f6-376f-4aa2-804c-0af999c11f6b",
   "metadata": {},
   "source": [
    "Much faster!  But this is still a lot slower than using a `CountVectorizer()`, because the spaCy model is still doing a lot of processing to figure out where the different tokens are and all that.  That's the cost for higher accuracy: the model spend more time figuring stuff out and crunching numbers on our behalf.  In the real world, you'll often have to make decisions about how fast is \"fast enough\" and how accurate is \"accurate enough.\"  \n",
    "\n",
    "# Using `nlp.make_doc` for even more speed--but even fewer features\n",
    "\n",
    "Fortunately, we have one last trick up our sleeves.  If you only need tokenization--not lemmatization--you can use `nlp.make_doc`, which disables even more of the processing.  It effectively runs the bare minimum processing: it does tokenization and some extremely basic token tagging like identifying stopwords.  Let's see how much faster this is.\n",
    "\n",
    "We'll use the same function as before, but we'll replace `nlp.pipe(docs)` with some code to run `nlp.make_doc` over all the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ede33354-a1ff-42c4-a9ee-4d3994d6260b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adada1edc9bd4c5ca7fed4ce1116caf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spaCy parsing:   0%|          | 0/18000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b26ec65af3f4100b630593c41954fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering tokens (after spaCy parsing):   0%|          | 0/18000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead43761bfa64c9990a3ef6ce759618c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spaCy parsing:   0%|          | 0/42000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed42468a91954694966e59d8eb32c50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering tokens (after spaCy parsing):   0%|          | 0/42000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def spacy_clean(nlp, texts):\n",
    "    docs = [\n",
    "        nlp.make_doc(i)\n",
    "        for i in tqdm(\n",
    "            texts,\n",
    "            desc=\"spaCy parsing\",\n",
    "            smoothing=0.01,\n",
    "        )\n",
    "    ]\n",
    "    docs = [\n",
    "        \" \".join(\n",
    "            # get the lowercase version--not lemma--since make_doc()\n",
    "            # doesn't apply lemmatization.\n",
    "            tok.lower_\n",
    "            for tok in doc\n",
    "            if not (\n",
    "                tok.is_stop\n",
    "                or tok.is_space\n",
    "                or tok.is_punct\n",
    "            )\n",
    "        )\n",
    "        for doc in tqdm(docs, desc=\"Filtering tokens (after spaCy parsing)\")\n",
    "    ]\n",
    "    return docs\n",
    "\n",
    "_ = spacy_clean(nlp, train[\"reviewText\"])\n",
    "_ = spacy_clean(nlp, test[\"reviewText\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381ece15-eff2-422e-909e-5d768b9633ad",
   "metadata": {},
   "source": [
    "Dang.  That's fast--and we're only using a single processing thread!  You can use this with multiprocessing, but you have to write the logic yourself.  It isn't too complicated, though.\n",
    "\n",
    "Note: In one of the next notebook, we'll see how Gensim provides tools similar to spaCy for doing this kind of preprocessing, but the tools in Gensim are much faster (though a bit less accurate).  Gensim has a very different approach compared to spaCy.\n",
    "\n",
    "Now, having done all our own preprocessing, let's throw the results through our classifier from the other notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ebd114a-9bcd-47c3-8f9d-558f31e4255e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 906 ms\n",
      "Wall time: 921 ms\n",
      "Overall F1 score: 0.7896350083888972\n",
      "Clothing and Jewelry-specific F1 score: 0.8026907232785583\n",
      "Electronics-specific F1 score: 0.8274530240858073\n",
      "Video Games-specific F1 score: 0.7726024660828436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\n",
    "        \"feature extraction\",\n",
    "        # TfidfVectorizer supports all the same options as CountVectorizer,\n",
    "        # plus a few others that we're not setting\n",
    "        TfidfVectorizer(\n",
    "            min_df=10,\n",
    "            max_df=0.5,\n",
    "            stop_words=\"english\",\n",
    "            # max_features=10_000,\n",
    "        )\n",
    "    ),\n",
    "    (\"classifier\", LinearSVC(random_state=0, max_iter=2000))\n",
    "])\n",
    "\n",
    "# Just to show how long this can take, I'll use a\n",
    "# %time \"magic command\" in Jupyter.  LinearSVC models\n",
    "# are usually very fast, but this is a *lot* of data.\n",
    "%time clf.fit(train[\"Cleaned Text\"], train[\"overall\"])\n",
    "test[\"Predictions\"] = clf.predict(test[\"Cleaned Text\"])\n",
    "\n",
    "# Calculate a few different F1 scores.\n",
    "overall_f1 = metrics.f1_score(test[\"overall\"], test[\"Predictions\"], average=\"macro\")\n",
    "print(f\"Overall F1 score: {overall_f1}\")\n",
    "\n",
    "for product_category, results in test.groupby(\"productCategory\"):\n",
    "    f1 = metrics.f1_score(results[\"overall\"], results[\"Predictions\"], average=\"macro\")\n",
    "    print(f\"{product_category}-specific F1 score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb941e8-4e22-4cc4-a37d-559085413fd0",
   "metadata": {},
   "source": [
    "# Some things we didn't cover\n",
    "\n",
    "All of the steps that spaCy applies--tokenization, lemmatization, POS tagging, syntactic dependencies, etc--can be used independently, rather than through the language models you get from `spacy.load()`.  In my experience, the most common reason to do this is either to seriously customize the behavior of these elements (which is much more advanced than we'd be covering here anyways), or for speed (in which case, Gensim or other tools might be better anyways).\n",
    "\n",
    "spaCy's models can also be run on a GPU for extra speed if your computer has one.  As with all GPU-bound computations, though, it's not necessarily an automatic speedup.  You can't/shouldn't use multiprocessing with GPU models (unless you have multiple discrete GPUs in your system), and you need to balance the length of your texts and the batch sizes going to the GPUs to minimize data serialization overhead.  Running on a GPU is extremely useful if you have a lot of data.\n",
    "\n",
    "spaCy also has ways to save `Doc` objects to file and reload them later--look into `DocBin` if you're interested in this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d38b106-8852-49fe-a7a4-b41502c58e27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
