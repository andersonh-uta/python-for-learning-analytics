{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "910d837e-ab0b-4ea1-905a-a788889f6f57",
   "metadata": {},
   "source": [
    "This notebook contains no code, only a very brief description of some of the foundational concepts in NLP.\n",
    "\n",
    "# Foundations of Natural Language Processing (NLP)\n",
    "\n",
    "NLP, fundamentally, only tries to answer a singe question: _how do we convert language into numbers?_\n",
    "\n",
    "Importantly, NLP is _not_ a field of Linguistics, or cognitive science, or anything like that; it's an engineering field.  NLP is employed when we have a bunch of language data and we need to use it to get something done.  \"Getting something done\" is the end goal--not \"analyzing language data for deeper insights about language\" (which we would probably call _computational methods in Linguistics_ or just _computational Linguistics_), or \"finding ways to make a computer process language _like humans do_.\"  This means we're going to take some shortcuts here and there, and use models and code that may seem like they fly in the face of how language actually works.\n",
    "\n",
    "NLP has a strong \"the ends justify he means\" mindset.  However, it should still be guided by some level of prior knowledge.  When choosing what features to measure or what kind of model to create, we can reason about the validity of some decisions.  E.g.: measuring the number of times the letter \"e\" is used probably won't be a valid decision.  It might lead to high accuracy scores on a held-out test set, but we can pretty easily identify this as almost certainly being spurious and invalid.  But beyond this, pretty much anything is fair game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bbd31a-14ec-46b0-8b8f-9819703f871b",
   "metadata": {},
   "source": [
    "# Feature extraction: What do we measure about language?\n",
    "\n",
    "Short and unhelpful answer: anything that helps us accomplish the task we've set for ourselves.\n",
    "\n",
    "More detailed and more helpful anwer: usually, we're ultimately _counting some sort of linguistically meaningful unit_.  Words are the most common basis: one way to extract features is to count words.  Each word in our corpus becomes a feature, and for each document, we measure how often that word is used.  For some languages, you might want to count _morphemes_ instead of words (e.g., a language like Turkish, where single words can contain large numbers of morphemes).  Or, you may want to go in the other direction, and count _n-grams_ (consecutive sequences of _n_ words).  All of these approaches can be lumped together under the broader notion of \"counting substrings.\"\n",
    "\n",
    "But you might also count more abstract things, like parts of speech, or syntactic relationships, or some intersection of all of these.  E.g. \"first-person singular subjects of active, transitive sentences\" might be  valid thing to measure, though this would probably require some very specific circumstances.\n",
    "\n",
    "Many modern methods (almost universally based on neural networks) actually pass this question off to the model, letting the model figure out what substrings to extract.  Some methods based on recurrent neural networks even use individual letters as features (though this is somewhat uncommon nowadays).\n",
    "\n",
    "We will focus on _word_-based features for most of this group of notebooks.\n",
    "\n",
    "NLP also encompasses, in principle, all forms of human language: written, signed, spoken, etc.  However, there are _massive_ technological difficulties with processing, e.g., audio data.  So usually, we focus on text: it's the easiest to collect and process in large quantities.  These notebooks focus exclusivey on text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5458fbb-3743-44d0-a29f-de05409818f0",
   "metadata": {},
   "source": [
    "## So what is a word?\n",
    "\n",
    "This is a surprisingly difficult question, and one we're going to be very, very lazy about.  Linguists will debate what exactly a \"word\" is, and whether a \"word\" is even a meaningful concept (interestingly: a \"word\" might not be a general, cross-linguistic idea.  It might only be relevant in a subset of the world's languages).\n",
    "\n",
    "But we're not linguists, and we don't need exact anwers.  We just need answers that are good enough, most of the time, to get our job done.  So we're going to generally use the following sort of definition for a \"word\": _a word is a thing you can look up in a dictionary._  This usually means it's a sequence of letters, maybe with apostrophes (at least in English), usually separated by some sort of punctuation or a whitespace character.\n",
    "\n",
    "Here's the wonderful thing, though: because we're going to be dealing with large amounts of data, an inexact/imprecise definition of a \"word\" or whatever we're measuring isn't automatically a huge problem.  There will _always_ be edge cases we can't account for, and they'll always be surprisingly common.  We don't worry about getting everything exactly right.  (It's not even clear if \"exactly right\" is a well-defined concept for this sort of thing).  So we can play a little fast and loose with definitions like this, and if they don't end up working for our project, we can just swap them out for another one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974fbc9f-37c4-415d-9458-726a40774665",
   "metadata": {},
   "source": [
    "# Kinds of NLP tasks\n",
    "\n",
    "# Document classification\n",
    "\n",
    "__Definition: _Document.___  NLP uses the word _document_ to refer to any self-contained units of text.  Usually, this is at least a whole sentence, but often this is a single piece of text containing multiple sentences.  E.g.: a student essay, or an Amazon product review, or a tweet.  In principle a document could be smaller than a sentence, but usually the term refers to a piece of text that is \"at least a sentence long.\"\n",
    "\n",
    "This title is a bit of a misnomer.  It's probably better called _string annotation._  E.g., we may be performing some sort of regression on individual words.  Most often, though, we're running classification models on whole documents.\n",
    "\n",
    "In general, document classification is a fairly straightforward kind of task.  We have some texts, and we want to automatically apply some sort of label to them.  E.g.: \"this essay shows a high/moderate/low degree of subject matter comprehension,\" or \"the author of this document is probably in middle school/high school/college.\"\n",
    "\n",
    "This kind of work generally involves two step:\n",
    "1. Feature extraction: converting our documents into fixed-length vectors.\n",
    "2. Feeding those vectors through any statistical or machine learning model we want.  (There's not a lot that's specific to NLP in this step).\n",
    "\n",
    "Of course, with the rise of neural networks over the past 10-15 years, there are lots of neural network architectures and techniques that are fairly specific to NLP.  Mostly, this comes in the form of _recurrent_ network (mostly the Long Short-Term Memory architecture) and, nowadays, _transformer_ networks.  We will not be covering these--they are extremely deep, dense topics, and we don't have time for them.  Plus, for the kinds of tasks you'll usually encounter in LA, the value proposition of these models isn't as great as it may seem.  The accuracy benefits are often surprisingly small compared to the simpler and more traditional methods we'll cover here; neural models are black boxes, meaning we can't undertand what exactly they've learned (and thus can't audit them for, e.g., algorithmic bias); neural models are much, _much_ slower than non-neural one; and neural models require significantly more specialized expertise and hardware to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d72d64a-edbb-412d-a109-3b9b96ea77e2",
   "metadata": {},
   "source": [
    "## Preprocessing/text cleaning\n",
    "\n",
    "We will often do a lot of preprocessing of text data.  This jut means we \"clean it up\" a bit before running it through a model, to reduce the amount of noise present in the data.\n",
    "\n",
    "Preprocessing usually consists of a few common steps, which are usually (but not always) done in roughly this order:\n",
    "1. Case normalization.  Strings are case-sensitive in Python, so `\"The\"` and `\"the\"` are considered different words.  Case normalization just means \"convert everything to lowercase/uppercase\" (conventionally lowercase, but it rarely matters which you pick).\n",
    "2. Stemming/lemmatization: removing the inflectional content from words.  This is often done with smaller datasets and with \"bag of words\" analyses (we'll talk about those later).\n",
    "3. Tokenization: identifying word boundaries, and splitting our documents at these boundaries, so we get a list of words for each one.  There are a few different approaches to tokenization:\n",
    "    a. Single-word tokenization.  Extract a list of all individual words.\n",
    "    b. _n-gram_ tokenization.  Extract a list of all _sequences of n consecutive words._\n",
    "    c. Character ngrams: rather than words, extract fixed-length sequences of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f21fa-52c1-4189-966f-d795a8194fd0",
   "metadata": {},
   "source": [
    "## Topic modeling\n",
    "\n",
    "Topic modeling anwers the question: _what topics are being discussed here?_  Topic modeling generally takes a slightly roundabout way to get to an answer, by defining a very abstract notion of what a \"topic\" is.  In general: we can define a topic in any way we choose, as long as it has the following properties:\n",
    "1. We can quantify some measure of relationship/association beetween topics and documents.\n",
    "2. We can quantify some measure of relationship/association between words and topics.\n",
    "3. These relationships are all many-to-many: one document can be associated with multiple topics; one topic may show up across multiple documents; one word may have different levels of association with different topics; each topic is associated with multiple words.\n",
    "\n",
    "In practice, there are two very natural mathematical structures that fit the bill:\n",
    "1. _Basis vectors_ in a high-dimensional vector space.  Documents and words are points in this space, and their \"association\" with each topic is the magnitude of their projection along the corresponding basis vector.\n",
    "2. _Statistical distributions._  A topic is a particular distribution over individual words.  A document is a mixture of these distributions (or roughly equivalently: a document is a distribution over these distributions).\n",
    "\n",
    "Inreasingly, there is interest in _neural topic modeling,_ where we allow a neural network to figure out a good mathematical structure.  We won't be covering neural topic models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7764d84f-c4c8-4fff-8043-1e4bebb5dbe2",
   "metadata": {},
   "source": [
    "## Text embeddings\n",
    "\n",
    "A lot of work has been done on good ways to represent text before running your classification or regression models.  Word embeddings are just one such tool: they focus on using a lot of computational resources \"up front\" to generate smaller, more compact, information-rich representations of words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
