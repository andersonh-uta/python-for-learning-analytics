{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b50387-4ce8-4b8a-b5ef-8ab8906266bd",
   "metadata": {},
   "source": [
    "# Text preprocessing with Gensim\n",
    "\n",
    "Gensim is a very nice counterpart to spaCy.  It also has a very different focus.  spaCy focuses on general-purpose NLP, with lots of language annotation tools.  Gensim is focused more directly on analyzing _enormous_ datasets.  It makes a lot of choices that trade accuracy for speed when it comes to preprocessing steps.\n",
    "\n",
    "Let's do the same thing we just did: a bag of words analysis with some basic preprocessing: tokenization, stopword removal, lemmatization.  Then feed it back through our scikit-learn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0699731b-ac27-473a-80ff-566b8186dd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same data loading code as before\n",
    "import pandas as pd\n",
    "\n",
    "electronics = pd.read_parquet(\"electronics.parquet\")\n",
    "video_games = pd.read_parquet(\"video_games.parquet\")\n",
    "clothes = pd.read_parquet(\"clothes.parquet\")\n",
    "    \n",
    "# Remove 3-star reviews.\n",
    "electronics = electronics[electronics[\"overall\"] != 3]\n",
    "video_games = video_games[video_games[\"overall\"] != 3]\n",
    "clothes = clothes[clothes[\"overall\"] != 3]\n",
    "\n",
    "# Set the \"overall\" column to be the binary classes of \"positive\"\n",
    "# (for >3) and \"negative\" (<3).\n",
    "electronics[\"overall\"] = [\"Positive\" if i > 3 else \"Negative\" for i in electronics[\"overall\"]]\n",
    "video_games[\"overall\"] = [\"Positive\" if i > 3 else \"Negative\" for i in video_games[\"overall\"]]\n",
    "clothes[\"overall\"] = [\"Positive\" if i > 3 else \"Negative\" for i in clothes[\"overall\"]]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(\n",
    "    electronics,\n",
    "    train_size=0.9,\n",
    "    stratify=electronics[\"overall\"],\n",
    "    random_state=0,\n",
    ")\n",
    "test = pd.concat((test, video_games, clothes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2afea611-84ef-4ca6-9f72-af41d24e5b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792a873ee7884ddf8bf664da901fb4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1e175e140b4af3aa6749ab01cf2a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gensim.parsing import preprocessing\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def preprocess(s):\n",
    "    \"\"\"Apply some of gensim's preprocessing tools.\"\"\"\n",
    "    # remove HTML tags\n",
    "    s = preprocessing.strip_tags(s)\n",
    "    \n",
    "    # Remove non-alphanumeric characters\n",
    "    s = preprocessing.strip_punctuation(s)\n",
    "    s = preprocessing.strip_numeric(s)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    s = preprocessing.remove_stopwords(s)\n",
    "    \n",
    "    # Remove short tokens. This turns out to sometimes\n",
    "    # be useful when you're focusing on meaning; in English\n",
    "    # and many Indo-European languages, veyr short words\n",
    "    # are often structural or otherwise non-meaning-carrying.\n",
    "    # Or at least, they're *less* meaning-carrying than longer\n",
    "    # words.\n",
    "    s = preprocessing.strip_short(s)\n",
    "    \n",
    "    # Stem the text using the Porter stemmer, which looks\n",
    "    # at the last few characters in a word and applies some\n",
    "    # patterns to remove inflection.\n",
    "    s = preprocessing.stem_text(s)\n",
    "    \n",
    "    return s\n",
    "\n",
    "train[\"Cleaned Text\"] = train[\"reviewText\"].progress_apply(preprocess)\n",
    "test[\"Cleaned Text\"] = test[\"reviewText\"].progress_apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b42a7-383c-4987-85d3-c70ac9fa62ac",
   "metadata": {},
   "source": [
    "That's fast!  We're applying most of the same steps as we did with spaCy, but multiple times faster! However: the results we get are not as easily human-readable.\n",
    "\n",
    "(Note: the above processing steps, plus a few others, are available in the `gensim.parsing.preprocessing.preprocess_string()` function; I've broken them out to show how easy it is to mix-and-match different processing steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a52f153-4908-4287-ac6c-609a2e5ee32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outstanding. I have the 16mb version of this. Its been on my key ring with my keys in my pocket with pocket knife and change and used every day for two years. It looks new and keeps on ticking\n",
      "outstand version it kei ring kei pocket pocket knife chang dai year look new keep tick\n"
     ]
    }
   ],
   "source": [
    "print(train[\"reviewText\"].iloc[0])\n",
    "print(train[\"Cleaned Text\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a563b10-3f5e-4811-8d48-05695ab265cf",
   "metadata": {},
   "source": [
    "Notice a few weird words:\n",
    "- \"Outstanding\" --> \"outstand\"\n",
    "- \"Key\" --> \"kei\"\n",
    "- \"Day\" --> \"dai\"\n",
    "- \"Change\" --> \"chang\"\n",
    "\n",
    "This is pretty emblematic of the Porter stemmer.  The Porter stemer is designed with a very narrow purpose in mind.  It is designed to make sure that every inflected form of a word is replaced with the same string, but it does not require that string to be the \"dictionary form\" of a word.  (This contrasts with spaCy, which does return the \"dictionary form\" of a word).  The Porter stemmer does this by looking only at the patterns of letters in the word, and using a set of replacement rules.  This allows it to be fast, but at the cost of human readability.  In practice, using a human-readable lemmatizer versus the Porter stemmer (or any of the many other stemmers out there) has a negligible impact on the accuracy of your downstream models, at least every time I've tested it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51cbcfda-8055-42ef-9a96-467ff541acbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 828 ms\n",
      "Wall time: 821 ms\n",
      "Overall F1 score: 0.7892704955134862\n",
      "Clothing and Jewelry-specific F1 score: 0.8046542231105482\n",
      "Electronics-specific F1 score: 0.836921069797782\n",
      "Video Games-specific F1 score: 0.7689399612259036\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\n",
    "        \"feature extraction\",\n",
    "        # TfidfVectorizer supports all the same options as CountVectorizer,\n",
    "        # plus a few others that we're not setting\n",
    "        TfidfVectorizer(\n",
    "            min_df=10,\n",
    "            max_df=0.5,\n",
    "            # stop_words=\"english\",\n",
    "            # max_features=10_000,\n",
    "        )\n",
    "    ),\n",
    "    (\"classifier\", LinearSVC(random_state=0, max_iter=2000))\n",
    "])\n",
    "\n",
    "# Just to show how long this can take, I'll use a\n",
    "# %time \"magic command\" in Jupyter.  LinearSVC models\n",
    "# are usually very fast, but this is a *lot* of data.\n",
    "%time clf.fit(train[\"Cleaned Text\"], train[\"overall\"])\n",
    "test[\"Predictions\"] = clf.predict(test[\"Cleaned Text\"])\n",
    "\n",
    "# Calculate a few different F1 scores.\n",
    "overall_f1 = metrics.f1_score(test[\"overall\"], test[\"Predictions\"], average=\"macro\")\n",
    "print(f\"Overall F1 score: {overall_f1}\")\n",
    "\n",
    "for product_category, results in test.groupby(\"productCategory\"):\n",
    "    f1 = metrics.f1_score(results[\"overall\"], results[\"Predictions\"], average=\"macro\")\n",
    "    print(f\"{product_category}-specific F1 score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a6e723-85b4-45ec-9220-0ed042047459",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
